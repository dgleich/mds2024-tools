- poster: 139738
  presenter: "Abas, Asaf"
  email: "e56a8baeb7747be"
  title: "MS80: Bias-Variance Trade-off in Multiscale Interpolation"
  abstract: |-
    We introduce a multiscale scheme for scattered data approximation. Our
    approach leverages a given interpolation scheme and constructs a hierarchical
    approximation over levels where a more significant portion of the data set is
    considered at each level. In particular, at the first level, one gets a coarse
    approximation from a small portion of data, while, in the following levels, we
    gradually correct the approximation and refine it to improve the approximation
    quality. This work investigates the statistical properties of error, mainly through
    the lens of the bias-variance trade-off. We thoroughly explore these properties
    numerically to approximate real and manifold-valued functions.
- poster: 139878
  presenter: "Abdullah, Abdullah"
  email: "d10bc098253cfaf4"
  title: |-
    Novel Numerical Reconstruction and Isolation of Different Nonlinear Dynamics
    in Video Via Latent Space Disentanglement of An Untrained Generator Network and
    Applications to Dynamic Mri
  abstract: |-
    Processing different types of dynamics in video data is a highly relevant
    problem in video analysis particularly in dynamic medical imaging where contrast
    enhancement, respiratory motion and patient moments poses a great challenge due
    to its effects on the image reconstruction as well as for its subsequent interpretation.
    The analysis and further processing of the dynamics of interest is often complicated
    by additional unwanted dynamics. This work proposes a novel nonlinear approach
    for the reconstruction and subsequent separation of different types of nonlinear
    dynamics in a video data via deep learning. The dynamic images are represented
    as the forward mapping of a sequence of time dependent latent space variables
    via an untrained generator neural network with no supervision. Different kinds
    of dynamics are characterized independently from each other via latent space disentanglement
    using one dimensional prior information, called triggers. Leveraging the triggers,
    the method successfully reconstruct a video containing different dynamics from
    highly undersampled data with parallel imaging. The model also detect the unknown
    dynamic and subsequently freeze any selection of dynamics and obtain accurate
    independent representations of the other dynamics of interest at any phase of
    the frozen dynamic. The method is tested on both synthetic data and real MRI datasets
    where contrast intensity, breathing, respiratory and body motion are separated.
- poster: 139720
  presenter: "Achour, El Mehdi"
  email: "9220b8baf2d0eb30"
  title: |-
    MS78: The Loss Landscape of Deep Linear Neural Networks: a Second-Order
    Analysis
  abstract: |-
    We study the optimization landscape of deep linear neural networks with
    the square loss. It is known that, under weak assumptions, there are no spurious
    local minima and no local maxima. However, the existence and diversity of non-strict
    saddle points, which can play a role in first-order algorithms' dynamics, have
    only been lightly studied. We go a step further with a full analysis of the optimization
    landscape at order 2. We characterize, among all critical points, which are global
    minimizers, strict saddle points, and non-strict saddle points. We enumerate all
    the associated critical values. The characterization is simple, involves conditions
    on the ranks of partial matrix products, and sheds some light on global convergence
    or implicit regularization that have been proved or observed when optimizing linear
    neural networks. In passing, we provide an explicit parameterization of the set
    of all global minimizers and exhibit large sets of strict and non-strict saddle
    points.
- poster: 139823
  presenter: "Ackermann, Michael"
  email: "510155492e465a45"
  title: "MS7: Learning Mechanical Systems From Data Via Structured Barycentric Forms"
  abstract: |-
    High-fidelity modeling of complex physical phenomena often leads to large
    scale models containing too many degrees of freedom for efficient simulation.
    In the case one has access to the realization of such a model, projection-based
    methods allow for the construction of accurate surrogates that preserve the physical
    and differential structures from the original model. However, this structure preservation
    is typically lost in the cases when one does not have access to the state-space
    realization of the model but rather input-output data. In this work, we consider
    data resulting from frequency response measurements. Based on our previous work
    on structured barycentric forms, we present extensions to data-driven modeling
    approaches such as the Adaptive Anderson-Antoulas (AAA) algorithm, which now enable
    us to not only learn accurate models from data but also to preserve second-order
    differential structures in addition. These approaches are well suited for the
    structured data-driven modeling of mechanical systems. This poster accompanies
    the talk "Learning Mechanical Systems via a Structured AAA Algorithm".
- poster: 138453
  presenter: "Acquah, Theophilus"
  email: "fd7b923792511eeb"
  title: |-
    Temporal Dynamics in Spatial Random Field Theory: A Methodological Advance
    in Fmri Data Analysis
  abstract: |-
    This research enhances fMRI data analysis by integrating temporal dynamics
    into spatial random field theory. We developed a new test statistic,, within the
    time-adaptive Scale Space Gaussian Random Field Model, focusing on signal detection
    in fMRI data. It captures the global maximum across spatial and temporal dimensions.  Our
    methodology, employing the Functional Autoregressive (FAR (1)) model, focuses
    on temporal dependencies and spatial arrangements in data, significantly contributing
    to neuroimaging studies. We used a simulation approach to estimate the p-value
    for testing the signal using X_max and understand its advantages in analyzing
    spatial-temporal patterns in fMRI data,
- poster: 139831
  presenter: "Actor, Jonas"
  email: "876fa3f177149c66"
  title: "MS30: Operator Learning for Exterior Calculus Surrogate Models"
  abstract: |-
    Data-driven surrogate models have proven to be an effective tool for a
    wide range of physical applications, but their use is limited in instances where
    structure-preserving guarantees are necessary to match the underlying physics.
    Additionally, many of such methods must often be retrained for each instance of
    a problem, particularly if right-hand sides, forcing terms, or spatially-varying
    coefficients change. We pose a framework for building learning mechanisms for
    operator learning into a structure-preserving surrogate model framework using
    exterior calculus, where bespoke Whitney forms are adapted to fit from data and
    where a nonlinear operator that acts on the modal coefficients of the mimetic
    surrogate system is learned simultaneously alongside the Whitney forms. We pose
    examples of such problems for a handful of applications, demonstrating the dimension
    reduction and structure-preserving capabilities of our approach.
- poster: 139917
  presenter: "Adams, Henry"
  email: "92ab68870441595f"
  title: "MS12: Evasion Paths in Mobile Sensor Networks"
  abstract: |-
    Suppose ball-shaped sensors are scattered in a bounded domain. Unfortunately
    the sensors don't know their locations (they're not equipped with GPS), and instead
    only measure which sensors overlap each other. Can you use this connectivity data
    to determine if the sensors cover the entire domain? I will explain how tools
    from topology allow you to address this coverage problem. Suppose now that the
    sensors are moving; an evasion path exists if a moving intruder can avoid overlapping
    with any sensor. Can you use the time-varying connectivity data of the sensor
    network to decide if an evasion path exists? Interestingly, there is no method
    that gives an if-and-only-if condition for the existence of an evasion path, but
    I will advertise follow-up questions that remain open!
- poster: 140272
  presenter: "Adara, Emmanuel"
  email: "bcdefb3263cb2bfb"
  title: |-
    Probability Reconstruction for Chemical Kinetics Using a Maximum Entropy
    Method with Moments
  abstract: |-
    The challenge known as the "curse of dimensionality" has posed significant
    difficulties for conventional techniques employed in addressing the chemical master
    equation (CME). This predicament  arises when the state space of the Markov chain
    expands exponentially with the number of species, hindering the  computation of
    the full probability distribution of the CME due to the large data generated.
    The method of moment provides an efficient alternative to circumvent the challenge,
    in comparison to other well known approaches such as the stochastic simulation
    algorithm (SSA) and finite state projection (FSP). Unfortunately, in circumstances
    where the full marginal probabilities are needed it is necessary to have a process
    by which to reconstruct them from the moments.  In this study, we applied the
    maximum entropy principle to reconstruct the distribution of certain models. This
    is accomplished using a finite set of moment constraints, enabling us to gain
    valuable insights into the underlying probability distribution with increased
    computational efficiency.
- poster: 139601
  presenter: "Adcock, Ben "
  email: "767712cda733544"
  title: |-
    MS39: CS4ML: A General Framework for Active Learning with Arbitrary Data
    Based on Christoffel Functions
  abstract: |-
    Active learning is an important concept in machine learning (ML), in which
    the learning algorithm can choose where to query the ground truth to improve the
    accuracy of the learned model. As ML tools come to be more commonly used in scientific
    computing, where data is often expensive to obtain, the use of active learning
    is expected to be particularly important. In this work, we introduce a general
    framework for active learning in regression tasks. Our framework extends the standard
    setup by allowing for general types of data, rather than just pointwise samples
    of the target function. This generalization covers many cases of practical interest,
    such as data acquired in transform domains (e.g., Fourier data), vector-valued
    data (e.g., gradient-augmented data), data acquired along continuous curves, and
    multimodal data (i.e., combinations of different types of measurements). Our framework
    considers random sampling according to a finite number of sampling measures and
    arbitrary nonlinear approximation spaces (model classes). We introduce the concept
    of generalized Christoffel functions and show how these can be used to optimize
    the sampling measures. We prove that this leads to near-optimal sample complexity
    in various important cases. Finally, we demonstrate the efficacy of our framework
    for gradient-augmented learning with polynomials, Magnetic Resonance Imaging (MRI)
    using generative models and adaptive sampling for solving PDEs using Physics-Informed
    Neural Networks (PINNs).
- poster: 140108
  presenter: "Aeron, Shuchin"
  email: "f715a6ce8d96e202"
  title: |-
    MS67: Hard Negative Sampling Via Regularized Optimal Transport for Contrastive
    Representation Learning
  abstract: |-
    We study the problem of designing hard negative sampling distributions
    for unsupervised contrastive representation learning. We propose and analyze a
    novel min-max framework that seeks a representation which minimizes the maximum
    (worst-case) generalized contrastive learning loss over all couplings (joint distributions
    between positive and negative samples subject to marginal constraints) and prove
    that the resulting min-max optimum representation will be degenerate. This provides
    the first theoretical justification for incorporating additional regularization
    constraints on the couplings. We re-interpret the min-max problem through the
    lens of Optimal Transport (OT) theory and utilize regularized transport couplings
    to control the degree of hardness of negative examples. Through experiments we
    demonstrate that the negative samples generated from our designed negative distribution
    are more similar to the  anchor than those generated from the baseline negative
    distribution.
- poster: 139784
  presenter: "Aeron, Shuchin"
  email: "f715a6ce8d96e202"
  title: |-
    MS86: Entropy-Regularized Wasserstein Barycenter Functionals: Theory and
    Applications
  abstract: |-
    We investigate functional-analytic, statistical, and algorithmic aspects
    related to analysis and synthesis of measures using entropy-regularized Wasserstein-2
    barycenters. We highlight  and contrast our results with the unregularized case,
    pointing specifically to dimension-independent rates of statistical estimation
    when using entropy-regularization under mild assumptions. Furthermore, we give
    theoretical justification for the use of a popular method for synthesis of free-support
    entropy-regularized Wasserstein-2 barycenters. Finally, we demonstrate promising
    results suggesting the applicability of analysis coefficients to point-cloud classification
    under possible corruptions and occlusions.
- poster: 139851
  presenter: "Agarwal, Medha"
  email: "6515169397cf60af"
  title: "Transformer Learning As a Heat Flow"
  abstract: "Transformers are one of the most versatile learning architectures across domains in machine learning and data science. Self-attention matrices lie at the heart of transformer-based architecture. The transformation of input data under consecutive self-attention layers can be interpreted as the evolution of a set of particles over time. Building off the seminal work of Sander et al. (2022), we present a theoretical analysis of this evolution process, its dynamics, and its convergence to a heat flow, within a framework of gradient flows in measure spaces. We also propose a general discretization scheme for gradient flows that involves successfully computing Schrödinger bridges with equal marginals. We prove the uniform convergence of our proposed method to the gradient flow in 2-Wasserstein metric. We present several numerical illustrations of our theoretical results. This is joint work with Garrett Mulcahy, Soumik Pal, and Zaid Harchaoui."
- poster: 139942
  presenter: "Aghaieebeiklavasani, Rouzbeh"
  email: "80d5d151779299ad"
  title: |-
    A Data-Driven Analysis of Combat Curves in Dynamic Interactions of Rivals
    in Conflicts
  abstract: |-
    To understand conflicts and their dimensions, many variables, including
    players' political and military goals, combat capabilities like ground, naval,
    and air power, and military strategy play an undeniable role. It is worth noting
    that the traditional classification of conflicts has implications for the overall
    assessment of success or failure. One can raise a question about the proper methodology
    required to lay the groundwork for depicting a broader image of a conflict. Data-driven
    approaches provide valuable insight into understanding how interactions between
    rivals mean through the lens of dynamical systems. Furthermore, an attempt to
    decipher such dynamics enables us to bring our findings with different categories
    according to which players behave into comparison and extract notable features
    related to complexity. In other words, the comparison between data-driven dynamical
    systems and our observations sheds light on differences and parallels between
    our model and the real-world interpretation. Our findings show that quantifying
    rivals' objectives and reconstructing a dynamical system governing their strategic
    interaction and its comparison with the category obtained by categorizing regional
    and global powers' combat curves gives us an appropriate framework to use the
    mathematical models in this real-world problem.
- poster: 140553
  presenter: "Alcala, James"
  email: "f556c339045e0f82"
  title: |-
    MS37: Deterministic and Stochastic Moving Anchor Extragradient Algorithms
    for Structured Saddlepoint Problems (Poster)
  abstract: |-
    Our work introduces a moving anchor technique to extragradient algorithms
    for smooth structured minimax problems. First, our moving anchor technique is
    introduced into the original algorithmic anchoring framework known as EAG. We
    match the optimal order of convergence in terms of worst-case complexity on the
    squared gradient norm, $O(1/k^2)$. As many problems of practical interest are
    nonconvex-nonconcave, the recently developed FEG class of algorithms brings order-optimal
    anchoring methods developed within EAG to certain nonconvex-nonconcave problem
    settings. We introduce the moving anchor methods to the FEG class of algorithms
    and again obtain order-optimal complexity results. Extensions include a preconditioned
    version of our algorithms, as well as newly developed stochastic moving anchor
    methods for convex-concave problems. In both convex-concave and nonconvex-nonconcave
    settings, a variety of numerical examples demonstrate the efficacy and flexibility
    of the moving anchor framework over its fixed-anchor counterparts. Future directions
    of the moving anchor framework and other applications are also discussed.
- poster: 140096
  presenter: "Alcalde, Albert"
  email: "4c494dad1b3a0240"
  title: "Clustering in Pure-Attention Hardmax Transformers"
  abstract: |-
    We study the behaviour in the infinite-depth limit of a transformer model
    with hardmax self-attention and normalization sublayers, by viewing it as a discrete-time
    dynamical system acting on a collection of points. Leveraging a simple geometric
    interpretation of our transformer connected with ideas of hyperplane separation,
    we establish convergence to a clustered equilibrium and prove that clusters are
    completely determined by special points called leaders . We apply our theoretical
    understanding to design a model based on our transformer to solve the sentiment
    analysis task in an interpretable way: the transformer filters out meaningless
    words by clustering them towards the leaders, identified with words carrying the
    sentiment of the text such as `amazing' or `terrible'.
- poster: 139785
  presenter: "Alfatemi, Ali"
  email: "99c643e412bf6321"
  title: "A Deep Learning Framework for Ekg Diagnosis"
  abstract: |-
    This project introduces a deep-learning framework for multi-label classification
    of EKG data. The framework converts EKG wavelet data into images to be used as
    inputs into a specially-tuned CNN model. To improve model performance, a data
    augmentation method is used to increase the data set size before training the
    network, which is done through the implementation of a generative adversarial
    network (GAN). The framework is implemented in PyTorch and tested on the MIT-BIH
    Arrythmia database. Classification performance using our augmentation framework
    shows is comparable to that of previous state-of-the-art (SOTA) image classification
    models. As a persistent and pronounced issue in health-related datasets arises
    from both the quality and quantity of existing data, we have evidence that deep-learning
    frameworks such as these may be effective solutions to complement the available
    technical tools accessible to medical professionals.
- poster: 139607
  presenter: "Aljabea, Ibrahem"
  email: "229a66249b5170b7"
  title: "MS47: Topological Neural Network and Deep Learning"
  abstract: |-
    Deep neural networks have revolutionized machine learning by leveraging
    vast amounts of data across various domains. While they excel in processing standard
    data like images and text, they face challenges when applied to structured scientific
    data in non-Euclidean domains.  Geometric Deep Learning (GDL) extends the capabilities
    of deep learning to non-Euclidean domains by incorporating geometric principles.
    However, capturing non-local properties inherent in topological data remains a
    challenge.  In this poster, I will introduce higher-order structures beyond traditional
    graph-based approaches, known as Topological Neural Networks (TNNs). TNNs offer
    a deeper understanding of complex data relationships.  Moreover, I will discuss
    potential applications of TNNs, including drug discovery and social media analysis,
    and propose a novel approach (TopoX) inspired by classical topological concepts
    and recent advancements in higher-order networks. This framework aims to provide
    a more sophisticated representation of data in topological domains, pushing TNNs
    towards new frontiers. Relevant citations include seminal works in machine learning,
    geometric deep learning, and topological data analysis.
- poster: 139660
  presenter: "Al-Jarrah, Mohammad"
  email: "1b48a41f8c204af6"
  title: |-
    MS72: Data-Driven Approximation of Stationary Nonlinear Filters with Optimal
    Transport Maps
  abstract: |-
    The nonlinear filtering problem aims to determine the conditional probability
    distribution (posterior) of the state of a stochastic dynamical system given a
    history of partial and noisy observations. This poster introduces a data-driven
    nonlinear filtering algorithm when the state and observation processes are stationary.
    We approximate the posterior using an optimal transport (OT) map, which is the
    push-forward from a given distribution that is easy to sample from to the posterior,
    conditioned on a truncated observation window. The OT map is obtained as the solution
    to a stochastic optimization problem that is solved offline using recorded trajectory
    data from the state and observations. We present an error analysis of the algorithm
    under the stationarity and filter stability assumptions, which decomposes the
    error into two parts related to the truncation window during training and the
    error due to the optimization procedure. The proposed optimal transport data-driven
    filter (OT-DDF) demonstrates considerable computational efficiency during the
    online stage and maintains the flexibility and accuracy of OT in nonlinear filtering
    through various numerical examples.
- poster: 140053
  presenter: "Alkhouri, Ismail"
  email: "458205601578246b"
  title: "MS10: Diffusion-Based Robust Hybrid Deep Mri Reconstruction"
  abstract: |-
    Deep learning (DL) techniques have been extensively employed in magnetic
    resonance imaging (MRI) reconstruction, delivering notable performance enhancements
    over traditional non-DL methods. Nonetheless, these models have vulnerabilities
    during testing such as their susceptibility to worst-case or noise-based measurement
    perturbations, variations in training/testing settings like acceleration factors,
    contrast, k-space sampling locations, and distribution shifts stemming from unseen
    lesions and different anatomies. This paper addresses these robustness challenges
    by leveraging diffusion models. In particular, we present a robustification strategy
    that improves the resilience of DL-based MRI reconstruction methods by utilizing
    pre-trained diffusion models as purifiers. In contrast to conventional robustification
    methods for DL-based MRI reconstruction, such as adversarial training (AT), our
    proposed approach eliminates the need to tackle a minimax optimization problem.
    It only necessitates efficient finetuning on purified examples. Our experimental
    findings underscore the effectiveness of our approach in addressing the mentioned
    instabilities, outperforming standalone diffusion-based MRI reconstructors and
    leading robustification methods for deep MRI reconstruction, including AT and
    randomized smoothing. Our experiments show the adaptability of our approach to
    multiple DL-based MRI reconstruction models and show its robustness for data with
    unseen lesions.
- poster: 139640
  presenter: "Allard, Thomas"
  email: "78269a9f73b328a8"
  title: "Ellipsoids Methods for Metric Entropy Computations"
  abstract: |-
    Historically, metric entropy has played a significant role in various
    domains of mathematics such as non-linear approximation theory, statistical learning
    theory, and empirical process theory. Recent advances in data science, and more
    specifically in deep learning theory, have led to renewed interest in the concept
    of metric entropy. However, computing the precise value of the metric entropy
    of a given function class turns out to be notoriously difficult in general; exact
    expressions are available only in very few simple cases. For this reason, it has
    become common practice to resort to characterizations of the asymptotic behavior
    of metric entropy only. Even this more modest endeavor has turned out daunting
    in most cases and standard results in the literature typically only characterize
    the leading term in the asymptotic behavior up to a multiplicative constant. We
    fill this gap by presenting a new general method for the precise characterization
    of the leading term in the asymptotic expansion of metric entropy for infinite-dimensional
    ellipsoids. We further argue that our results provide a unified framework for
    the derivation of the metric entropy of a wide variety of function classes, such
    as unit balls in Besov spaces, Modulation spaces, Sobolev spaces, and various
    classes of analytic functions, thereby retrieving and improving, sometimes significantly
    so, on standard results.
- poster: 139563
  presenter: "Alqahtani, Aisha"
  email: "2078abc66cbac456"
  title: |-
    Heat and Mass Transfer Through Mhd Darcy Forchheimer Casson Hybrid Nanofluid
    Flow Across An Exponential Stretching Sheet
  abstract: "This work aims to study the energy and mass transition caused by Casson hybrid nanofluid flow across an extended stretching sheet. Fluid flow is subjected to an inclined magnetic field to control the flow stream. Cu and $Al_2 O_3$ NPs are added to the Casson fluid to generate a hybrid nanoliquid (Blood). This model of flow dynamics is an evolving nonlinear system of PDEs, which is then reduced to a system of dimensionless ODEs using similarity proxies. The ODEs is solved using the analytical program “HAM” for further processing."
- poster: 139818
  presenter: "Amezquita, Erik"
  email: "f48fbccf7e871c62"
  title: |-
    MS79: Characterizing Single-Cell Transcriptomic Signatures with Persistent
    Homology and Molecular Cartography
  abstract: |-
    The central dogma of molecular biology follows a simple path: DNA is
    transcribed into RNA transcripts in the cell nucleus, and transcripts are then
    translated into proteins in the cell cytosol. However, protein production is not
    solely impacted by the level of expression of genes, but by many other regulatory
    processes that can be specific to the gene type and even specific to the cell
    spatial location. Different cells of different shapes and sizes present different
    RNA transcript distribution for different genes. To mathematically model and analyze
    this highly variable distributions, we turn to Topological Data Analysis (TDA)
    for a robust and comprehensive pipeline. Here, through the use of single-cell
    sequencing and molecular cartography technologies, we first produce detailed maps
    of the spatial location of individual transcripts for different genes, cell types,
    and organs of the soybean root and nodule. We then use persistent homology to
    characterize the distribution of these transcripts for each cell. Comparing these
    topological shape signatures reveals a new perspective on the role of the nuclear
    and cytoplasmic localization of transcripts as a central mechanism to control
    protein translation and the biology of plant cells. This work reveals the influence
    of the compartmentalization of transcripts as another regulatory mechanism of
    protein translation and a new understanding of the central dogma of molecular
    biology.
- poster: 139567
  presenter: "Amona, Elizabeth"
  email: "9bcef55bfefa11b2"
  title: |-
    Studying Disease Reinfection Rates, Vaccine Efficacy and the Timing of Vaccine
    Rollout in the Context of Infectious Diseases
  abstract: |-
    This research uniquely explores the varied efficacy of existing vaccines
    and the pivotal role of vaccination timing in the context of COVID-19. We introduce
    two models that account for the impact of vaccines on infections, reinfections,
    and deaths. We estimate model parameters under the Bayesian framework, specifically
    utilizing the Metropolis-Hastings Sampler. The study conducts data-driven scenario
    analyses for the State of Qatar, quantifying the potential duration during which
    the healthcare system could have been overwhelmed by an influx of new COVID-19
    cases surpassing available hospital beds. Additionally, the research explores
    similarities in predictive probability distributions of cumulative infections,
    reinfections, and deaths, employing the Hellinger distance metric. Comparative
    analysis, utilizing the Bayes factor, underscores the plausibility of a model
    assuming a different susceptibility rate to reinfection, as opposed to assuming
    the same susceptibility rate for both infections and reinfections. Results highlight
    the adverse outcomes associated with delayed vaccination, emphasizing the efficacy
    of early vaccination in reducing infections, reinfections, and deaths. Our research
    advocates prioritizing early vaccination as a key strategy in effectively combating
    future pandemics. This study contributes vital insights for evidence-based public
    health interventions and reinforcing preparedness for challenges posed by infectious
    diseases.
- poster: 139967
  presenter: "An, Jing"
  email: "4a50e9470a1bd623"
  title: |-
    MS72: Critical Points and Convergence Analysis of Generative Deep Linear
    Networks Trained with Bures-Wasserstein Loss
  abstract: |-
    We consider a deep matrix factorization model of covariance matrices trained
    with the Bures-Wasserstein distance. While recent works have made advances in
    the study of the optimization problem for overparametrized low-rank matrix approximation,
    much emphasis has been placed on discriminative settings and the square loss.
    In contrast, our model considers another type of loss and connects with the generative
    setting. We characterize the critical points and minimizers of the Bures-Wasserstein
    distance over the space of rank-bounded matrices. The Hessian of this loss at
    low-rank matrices can theoretically blow up, which creates challenges to analyze
    convergence of gradient optimization methods. We establish convergence results
    for gradient flow using a smooth perturbative version of the loss as well as convergence
    results for finite step size gradient descent under certain assumptions on the
    initial weights.
- poster: 139599
  presenter: "Anguluri, Rajasekhar"
  email: "368683604593866c"
  title: |-
    Riccati Equations in Sparse Gaussian Graphical Models: Insights and Applications
    in Equilibrium Networks
  abstract: "In probabilistic graphical models, a crucial statistical inverse problem is learning edges that link nodes representing random variables. For the Gaussian model, this inverse problem concerns estimating the sparsity pattern—whether elements are zero or non-zero—of the inverse covariance or precision matrix. Recent research has concentrated on estimating the precision matrix under various constraints for high-dimensional data, where the number of samples surpasses the graph's size. Many fundamental statistical limits were discovered, and efficient algorithms were championed. We present a novel parameterization for the precision matrix, expressing it as LQL, where Q is known, but the symmetric matrix L remains unknown. Our framework extends the Cholesky-type precision matrix decomposition beyond restrictions to triangular or Toeplitz structures. We devise a maximum likelihood estimator (MLE) to learn the sparsity patterns of L and the precision matrix. Leveraging the theory of Matrix Riccati Equations, we solve the MLE and derive optimal sample complexity results. Furthermore, when L is an M-matrix (nonpositive off-diagonals), we establish conditions under which the underlying Gaussian distribution is total positivity of order two and investigate the associated Markov properties. Finally, we discuss the implications of our approach in learning equilibrium networks; for preliminary results, see (Rayas, Anguluri, and Dasarathy, vol. 35, pp. 14637-14650, NeurIPS 2022)."
- poster: 140248
  presenter: "Araujo Torres, Ricardo"
  email: "346486e487a05355"
  title: |-
    Fundamental Solution-Based RBF Neural Networks for Solving Inverse Boundary
    Value Problems
  abstract: |-
    The method of fundamental solutions (MFS) has been applied to solve boundary
    value problems and certain real-world challenges. However, its practical application,
    especially in solving inverse problems, is hindered by necessity of introducing
    source points (a fictitious boundary) outside the physical domain. This arises
    from the significant sensitivity of numerical solutions to placement of source
    points. To overcome this limitation, we propose a fundamental solution-based RBF
    neural network for solving inverse boundary value problems with homogeneous equations.
    In this framework, inspired by MFS, we use fundamental solutions of the homogeneous
    equations as activation functions and designate source points as centers of neurons.
    These source points are trained by minimizing the physics-informed loss function.
    We present numerical experiments in both 2D and 3D domains to demonstrate the
    effectiveness of the proposed approach.
- poster: 139955
  presenter: "Arora, Vinam"
  email: "678ac671c9f18fa4"
  title: |-
    MS77: Leveraging Perceiver Io and Relative Position Encodings for Enhanced
    Node Classification
  abstract: |-
    Graph transformer models have been limited in their application to large-scale
    graphs due to the quadratic computational complexity inherent to their attention
    mechanisms. This often results in a trade-off between scalability and model performance.
    In this work, we introduce a novel graph transformer architecture inspired by
    the PerceiverIO framework, which utilizes a combination of latent compression
    and relative position encoding to efficiently scale graph attention to larger
    graphs. Our approach mitigates the quadratic cost of pairwise communication between
    all nodes in a graph by learning a set of latent tokens through which nodes exchange
    messages. We evaluate our model on several node classification benchmarks and
    demonstrate that it not only surpasses existing graph transformer models in terms
    of performance but also maintains high efficiency and adaptability across different
    graph structures. Overall, our architecture offers a scalable solution for efficiently
    processing large graph datasets while significantly improving model performance
    and generalization.
- poster: 139071
  presenter: "Awoke, Temesgen "
  email: "97eea5642fadf6d3"
  title: |-
    Sex-Structured Disease Transmission Model and Control Mechanisms for Visceral
    Leishmaniasis (vl)
  abstract: |-
    Leishmaniasis are a group of diseases caused by more than 20 species of
    protozoan that are transmitted through the bite of a female sand fly. Several
    research has been conducted to propose disease control strategies. The mathematical
    models for the transmission dynamics of the disease studied so far did not consider
    the sex-biased burden of the disease into consideration. This study introduces
    a new deterministic sex-structured model for understanding the transmission dynamics
    of visceral leishmaniasis. Numerical simulations were performed using baseline
    parameter values, and scenario analysis was performed by changing some of these
    parameters as appropriate.  Our numerical result shows that the implementation
    of disease-preventive strategies, as well as effectively treating the affected
    ones can significantly reduce the disease prevalence if applied to more proportion
    of the male population. The numerical simulation infers that a maximum of 60%
    of extra preventative measures targeted to only the male population considerably
    reduces the total prevalence of VL by 80%. It is also possible to decrease the
    total prevalence of VL by 69.51% when up to 50% of additional infected males receive
    treatment with full efficacy. Therefore, to reduce the disease burden of visceral
    leishmaniasis, public health officials and concerned stakeholders need to give
    more emphasis to the proportion of male humans in their intervention strategies.
- poster: 140284
  presenter: "Bai, Xiangqi"
  email: "660f149477abc197"
  title: |-
    MS81: Joint Inference of Clonal Structure Using Single-Cell Genome and Transcriptome
    Sequencing Data
  abstract: |-
    Latest advancements in the high-throughput single-cell genome (scDNA)
    and transcriptome (scRNA) sequencing technologies enabled cell-resolved investigation
    of tissue clones. However, it remains challenging to cluster and couple single
    cells for heterogeneous scRNA and scDNA data generated from the same specimen.
    In this study, we present a computational framework called CCNMF, which employs
    a novel Coupled-Clone Non-negative Matrix Factorization technique to jointly infer
    clonal structure for matched scDNA and scRNA data. CCNMF couples multi-omics single
    cells by linking copy number and gene expression profiles through their general
    concordance. It successfully resolved the underlying coexisting clones with high
    correlations between the clonal genome and transcriptome from the same specimen.
    We validated that CCNMF can achieve high accuracy and robustness using both simulated
    benchmarks and real-world applications, including an ovarian cancer cell lines
    mixture, a gastric cancer cell line, and a primary gastric cancer. In summary,
    CCNMF provides a powerful tool for integrating multi-omics single-cell data, enabling
    simultaneous resolution of genomic and transcriptomic clonal architecture. This
    computational framework facilitates the understanding of how cellular gene expression
    changes in conjunction with clonal genome alternations, shedding light on the
    cellular genomic difference of subclones that contributes to tumor evolution.
- poster: 139896
  presenter: "Bai, Yikun"
  email: "b4d903916da8676c"
  title: "MS50: Stereographic Spherical Sliced Wasserstein Distance"
  abstract: |-
    Comparing spherical probability distributions is of great interest in
    various fields, including geology, medical domains, computer vision, signal processing,
    and deep representation learning. The utility of optimal transport-based distances,
    such as the Wasserstein distance, for comparing probability measures has spurred
    active research in developing computationally efficient variations of these distances
    for spherical probability measures. This paper introduces a high-speed and highly
    parallelizable distance for comparing spherical measures using the stereographic
    projection and the generalized Radon transform, which we refer to as the Stereographic
    Spherical Sliced Wasserstein (S3W) distance. We carefully address the distance
    distortion caused by the stereographic projection and provide an extensive theoretical
    analysis of our proposed metric and its rotationally invariant variation. Finally,
    we evaluate the performance of the proposed metrics and compare them with recent
    baselines in terms of both speed and accuracy through a wide range of numerical
    studies, including gradient flows and self-supervised learning.
- poster: 140301
  presenter: "Balasubramanian, Krishnakumar"
  email: "1e8bac74f7add2e8"
  title: "MS76: Understanding Gaussian-Stein Variational Gradient Descent"
  abstract: |-
    For sampling from a Gaussian target, the SVGD dynamics with a bilinear
    kernel will remain Gaussian as long as the initializer is Gaussian. Inspired by
    this fact, we undertake a detailed theoretical study of the Gaussian-SVGD, ie,
    SVGD projected to the family of Gaussian distributions via the bilinear kernel,
    or equivalently Gaussian variational inference (GVI) with SVGD. We present a complete
    picture by considering both the mean-field PDE and discrete particle systems.
    When the target is strongly log-concave, the mean-field Gaussian-SVGD dynamics
    is proven to converge linearly to the Gaussian distribution closest to the target
    in KL divergence. In the finite-particle setting, there is both uniform in time
    convergence to the mean-field limit and linear convergence in time to the equilibrium
    if the target is Gaussian. In the general case, we propose a density-based and
    a particle-based implementation of the Gaussian-SVGD, and show that several recent
    algorithms for GVI, proposed from different perspectives, emerge as special cases
    of our unified framework. Interestingly, one of the new particle-based instance
    from this framework empirically outperforms existing approaches. Our results make
    concrete contributions towards obtaining a deeper understanding of both SVGD and
    GVI.
- poster: 139690
  presenter: "Balcer, Matthew"
  email: "b1efd079e287282b"
  title: |-
    Surrogate-Assisted Parallel Global Optimization Using Sensitivity Analysis
    Applied to Shaped Charges
  abstract: |-
    Optimization algorithms that use batch sampling, known as parallel optimization,
    can speed up convergence when the objective function involves evaluating an expensive-to-evaluate
    black-box simulation. Additionally, sensitivity methods have been successful in
    speeding up convergence by reducing the dimension of the optimization problem.
    However, sensitivity analysis coupled with parallel optimization has not been
    well studied. This work developed several methods to effectively couple sensitivity
    analysis with parallel optimization. The methods focus on determining the best
    batch sample selection for a specified surrogate model. To show the effectiveness
    of the developed methods, an application to maximize the penetration depth of
    a shaped charge was conducted by optimizing the contours and thicknesses of a
    multi-material liner as well as shell geometry using a Gaussian process surrogate
    model. The accuracy and efficiency of the method was compared to other parallel
    approaches and surrogate models including an artificial neural network and a genetic
    algorithm.
- poster: 139724
  presenter: "Banerjee, Amartya"
  email: "7dd524ee74ef2ab7"
  title: "MS26: Efficient Wasserstein Splines Through Consecutive Averaging"
  abstract: |-
    Capturing data from dynamic processes through cross-sectional measurements
    is seen in fields such as cell trajectory inference. This inherently involves
    the challenge of understanding and reconstructing the continuous trajectory of
    these processes from discrete data points, for which interpolation and approximation
    play a crucial role. In this work, we propose methods for B-splines and interpolation
    in the Wasserstein space through consecutive intrinsic averaging. Our methods
    have the ability to carry out interpolation and approximation with high precision
    and at a chosen level of refinement, including the capability to accurately infer
    trajectories in scenarios where particles undergo splitting (division) over time.
    We rigorously evaluate our method using simulated cell data characterized by bifurcations
    and merges, comparing its performance against both state-of-the-art trajectory
    inference techniques and other spline and interpolation methods. The results of
    our work not only underscore the effectiveness of our method in addressing the
    complexities of trajectory inference but also highlight its proficiency in performing
    interpolation and approximation that respects the inherent geometric properties
    of the data.
- poster: 139661
  presenter: "Baptista, Ricardo"
  email: "97cb6add1328bb9"
  title: "MS23: Toward Consistent Nonlinear Filtering and Smoothing Via Measure Transport"
  abstract: |-
    Solving filtering and smoothing problems for geophysical applications
    involve estimating the hidden states of complex systems and accurately characterizing
    their uncertainty. Popular algorithms for tackling these problems include ensemble
    Kalman methods such as the EnKF, EnKS and RTS smoother. While these algorithms
    yield robust state estimates for high-dimensional models with non-Gaussian statistics,
    ensemble Kalman methods are limited by linear transformations and are generally
    inconsistent with the true Bayesian solution. In this presentation, I will discuss
    how measure transport can be used to consistently transform a prior ensemble into
    samples from a filtering or smoothing distribution. This approach provides a natural
    generalization of Kalman methods to nonlinear transformations, thereby reducing
    the intrinsic bias of classic algorithms with a marginal increase in computational
    cost. In small-sample settings, I will show how to estimate transport maps for
    high-dimensional inference problems by exploiting low-dimensional structure in
    the target distribution. Finally, I will demonstrate the benefit of this framework
    for filtering and smoothing on chaotic dynamical systems and aerodynamic flows.
- poster: 139514
  presenter: "Baptista, Ricardo"
  email: "97cb6add1328bb9"
  title: |-
    MS17: Scalable Estimation of Conditional Brenier Maps Via Entropic Optimal
    Transport
  abstract: |-
    The task of conditional simulation asks to generate samples from the conditional
    of a joint distribution described by a collection of samples. Conditional Brenier
    maps, $T_{\text{CB}}$ allow for conditional simulation in a single evaluation,
    though the estimation of such maps is notoriously difficult. [Carlier et al.,
    2010] show that optimal transport maps under a rescaled quadratic cost, written
    $T_{\text{B},t}$, asymptotically converge to $T_{\text{CB}}$ as the scaling parameter
    $t \rightarrow 0$. While recent works have proposed methods for conditional simulation
    based on this framework, they fall short of providing quantitative convergence
    guarantees of their procedure, nor are they necessarily scalable to large sample
    sizes. We propose to instead use computational methods based on entropic optimal
    transport to estimate $T_{\text{CB}}$, which, unlike existing approaches, has
    the advantage of being both principled and scalable to many training points. We
    demonstrate the performance on various conditional simulation tasks arising from
    Bayesian inference problems with intractable likelihood functions.
- poster: 139791
  presenter: "Bari, Sajjadul"
  email: "651db700fdfad825"
  title: |-
    Advancements in Parametrization of Sub-Grid Fluxes Utilizing Homogenization
    Techniques and Machine Learning Approaches
  abstract: "In this work, we use a combination of machine learning and homogenization techniques to derive effective equations discretized in a coarse mesh. Our starting point is a fine-mesh discretization of a PDE expressed in flux form. We use homogenization combined with machine learning to estimate effective fluxes on a much coarser mesh. We demonstrate that homogenization terms considerably simplify the machine learning model and its training. Moreover, we also use flux limiters to ensure that effective fluxes do not violate physical constraints. We demonstrate our approach using the 1D and 2D forced Burgers’ equations."
- poster: 139958
  presenter: "Bassi, Hardeep"
  email: "dd7ad9589dce7d52"
  title: "MS14: Machine Learning Applications to Quantum Many-Body Systems"
  abstract: |-
    Developing new approaches to electronic structure calculations and quantum
    many-body dynamics is of extreme relevance for chemical, physical, and materials
    applications. Although there exists a variety of approximation methods, in conjunction
    with formally exact theory, quantum many-body systems often exhibit emergent phenomena
    that defy traditional analytical techniques. This necessitates innovative computational
    strategies to capture their nuanced dynamics and complex interactions. In this
    presentation, we explore how to leverage machine learning (ML) methods to capture
    the dynamics of highly correlated electronic systems.
- poster: 139998
  presenter: "Basu, Sabyasachi"
  email: "efffc905bb019111"
  title: "Spectral Triadic Decompositions of Real-World Networks"
  abstract: |-
    A fundamental problem in mathematics and network analysis is to find conditions
    under which a graph can be partitioned into smaller pieces. A ubiquitous tool
    for this partitioning is the Fiedler vector or discrete Cheeger inequality. These
    results relate the graph spectrum (eigenvalues of the normalized adjacency matrix)
    to the ability to break a graph into two pieces, with few edge deletions. An entire
    subfield of mathematics, called spectral graph theory, has emerged from these
    results. Yet these results do not say anything about the rich community structure
    exhibited by real-world networks, which typically have a significant fraction
    of edges contained in numerous densely clustered blocks. Inspired by the properties
    of real-world networks, we discover a new spectral condition that relates eigenvalue
    powers to a network decomposition into densely clustered blocks. We call this
    the spectral triadic decomposition . Our relationship exactly predicts the existence
    of community structure, as commonly seen in real networked data. Our proof provides
    an efficient algorithm to produce the spectral triadic decomposition. We observe
    on numerous social, coauthorship, and citation network datasets that these decompositions
    have significant correlation with semantically meaningful communities.
- poster: 140290
  presenter: "Batlle, Pau"
  email: "d388a31385b4dc46"
  title: |-
    MS87: Frequentist Confidence Intervals Via Optimization: Resolving the Burrus
    Conjecture
  abstract: |-
    We introduce an optimization-based framework to construct confidence intervals
    for functionals in constrained inverse problems, ensuring valid one-at-a-time
    frequentist coverage guarantees. Our method builds upon the now-called strict
    bounds intervals which offer ways to directly incorporate any side information
    about parameters during inference without introducing external biases. By tying
    these intervals to an inversion of a constrained likelihood ratio test, we translate
    interval coverage guarantees into type-I error control, and characterize the resulting
    interval via solutions of optimization problems. Along the way, we refute the
    Burrus conjecture. Our framework provides a novel approach to analyze the conjecture
    and construct a counterexample by employing a stochastic dominance argument, which
    we also use to disprove a general form of the conjecture. We illustrate our framework
    with several numerical examples and provide directions for extensions beyond the
    Rust-Burrus method for non-linear, non-Gaussian settings with general constraints.
- poster: 139822
  presenter: "Belton, Robin"
  email: "8070115b7e798a97"
  title: "MS91: Learning Parameters for Mapper Graphs"
  abstract: |-
    The Mapper algorithm is a popular visualization technique in topological
    data analysis (TDA) that outputs a graph reflecting the structure of a given dataset.
    We illustrate Mapper's high sensitivity to its input parameters by answering
    the inverse problem: Given a graph $G$ and data $X$, does there exist a set of
    Mapper parameters such that the Mapper graph of $X$ is isomorphic to $G$? Additionally,
    we introduce $G$-Mapper, an iterative and statistical algorithm for optimizing
    parameters.
- poster: 139083
  presenter: "Ben-Yair, Ido"
  email: "7e68397168b04951"
  title: |-
    MS18: Optimizing Non-Galerkin Algebraic Multigrid for Asymmetric Finite
    Element Problems
  abstract: |-
    This study investigates the numerical optimization of sparse operator
    matrices in the non-Galerkin algebraic multigrid (AMG) smoothed aggregation family
    of algorithms.  AMG is a popular method for solving linear systems, such as those
    arising from the finite element discretization of partial differential equations
    (PDEs).  However, numerical methods such as AMG and Krylov methods often struggle
    with asymmetric systems, such as advection-diffusion (AD).  In this work, we consider
    PDEs such as advection-dominated AD and anisotropic diffusion as model problems
    to be solved by the flexible generalized minimal residual (FGMRES) algorithm.  These
    problems are often solved many times in succession in inverse problems, and therefore
    reducing the number of iterations to convergence of a single forward problem can
    be beneficial.  Thus, optimizing the graph neural network-like V-cycle for a specific
    problem may yield an improvement many times over.  To this end, we take advantage
    of deep learning framework PyTorch to optimize AMG cycles for these problems in
    our numerical experiments, leveraging its automatic differentiation capabilities
    and its ability to execute computations efficiently on graphical processing units
    (GPUs).  We show that significant improvements in convergence rates can be achieved
    by improving the action of the coarse-level operator matrices, while maintaining
    fixed sparsity patterns to avoid expensive sparse matrix operations.
- poster: 139626
  presenter: "Berman, Jules"
  email: "fdda456c587de4e5"
  title: |-
    MS1: Continuous Low-Rank Adaptation (CoLoRa) for Reduced Implicit Neural
    Modeling of Parameterized Partial Differential Equations
  abstract: |-
    This work introduces reduced models based on Continuous Low Rank Adaptation
    (CoLoRA) that pre-train neural networks for a given partial differential equation
    and then continuously adapt low-rank weights in time to rapidly predict the evolution
    of solution fields at new physics parameters and new initial conditions. The adaptation
    can be either purely data-driven or via an equation-driven variational approach
    that provides Galerkin-optimal approximations. Because CoLoRA approximates solution
    fields locally in time, the rank of the weights can be kept small, which means
    that only few training trajectories are required offline so that CoLoRA is well
    suited for data-scarce regimes. Predictions with CoLoRA are orders of magnitude
    faster than with classical methods and their accuracy and parameter efficiency
    is higher compared to other neural network approaches.
- poster: 140314
  presenter: "Berner, Julius"
  email: "445830d019de701a"
  title: "MS17: Sampling with Stochastic Bridges"
  abstract: "We introduce a family of methods to sample from unnormalized densities via learned stochastic bridges between an auxiliary prior and the target distribution. This includes recently developed methods based on Schrödinger half-bridges and diffusion models. We show how such methods can be improved using sequential Monte Carlo methods as well as tools from reinforcement learning and stochastic optimal control. Finally, we demonstrate that the resulting deep learning algorithms achieve state-of-the-art performance on several challenging benchmarks."
- poster: 140400
  presenter: "Bhandari, Dila Ram"
  email: "c20c9ea7ccbcd492"
  title: "Touching with Time Series Models to Predict Tourist Traffic"
  abstract: "Planning for tourism at all levels of the sector, from the government to a single travel agency, depends on forecasting travel demand. The potential of forecasting to lessen losses resulting from differences in supply and demand is what makes it valuable. To meet the needs of visitors, tourist sites must have accurate projections of the amount of money that will be required in the future for services such as lodging, transportation, and retail establishments. The forecasting methodologies utilized in the study incorporate both trend and seasonal components in the data series, as seen by the trend and seasonal patterns seen in the quarterly visitor arrivals from India and the USA to Nepal. This study considers seasonality-aware four-time series forecasting models. They include the Naive Trend and Seasonal, Time Series Decomposition, Winter’s Exponential, and SARIMA models."
- poster: 140546
  presenter: "Bharti, Beepul"
  email: "13d80b5ba4722a71"
  title: "MS15: Certifying Fairness with Incomplete Sensitive Information"
  abstract: |-
    As the use of machine learning models in real world high-stakes decision
    settings continues to  grow, it is highly important that we are able to audit
    and control for any potential fairness  violations these models may exhibit towards
    certain groups. To do so, one naturally requires  access to sensitive attributes,
    such as demographics, biological sex, or other potentially  sensitive features
    that determine group membership. Unfortunately, in many settings, this  information
    is often unavailable. In this work we study the well known equalized odds (EOD)  definition
    of fairness. In a setting without sensitive attributes, we first provide tight
    and  computable upper bounds for the EOD violation of a predictor. These bounds
    precisely reflect  the worst possible EOD violation. Second, we demonstrate how
    one can provably control the  worst-case EOD by a new post-processing correction
    method. Our results characterize when  directly controlling for EOD with respect
    to the predicted sensitive attributes is--and when is  not--optimal when it comes
    to controlling worst-case EOD. Our results hold under assumptions  that are milder
    than previous works, and we illustrate these results with experiments on  synthetic
    and real datasets.
- poster: 140236
  presenter: "Bhaskar, Dhananjay"
  email: "ab6e22b0782463d9"
  title: "MS29: Learning Graph Geometry and Topology Via Continuous-Time Message Passing"
  abstract: |-
    Graph Neural Networks (GNNs), while transformative, face challenges in
    capturing the full essence of graph structures due to their reliance on discrete-time
    message passing. This conventional approach, despite ensuring permutation equivariance,
    struggles with oversmoothing, underreaching, and computational bottlenecks, thus
    limiting its effectiveness in capturing graph structure. In this talk, I will
    introduce Continuous-time Message PASSing Network (COMPASS), a method designed
    to overcome these limitations by incorporating continuous-time dynamics derived
    from the heat and wave equations. Based on a solid theoretical foundation, COMPASS
    distinguishes itself in complex tasks such as the prediction of geometrical and
    topological features such as Ricci curvature, persistent homology, and the generating
    parameters of random graphs. Additionally, COMPASS can accurately predict topological
    attributes of molecular graphs, such as total polar surface area and ring counts,
    marking a significant improvement over existing discrete-time message passing
    networks.
- poster: 139595
  presenter: "Bhaumik, Jnanajyoti"
  email: "82258482a0be2871"
  title: "Bayesian Calibration of the Rosenthal Model"
  abstract: |-
    Hot cracking prevents the additive manufacturing of a range of high-strength
    and temperature alloys. In [Kitt, L., Alexander, et. al: Additive Manufacturing,
    73, 103675 (2023)], a hierarchical model that links process parameters, thermal
    features, and crack formation was proposed. The model uses a multi-source Rosenthal  model
    calibrated against process monitoring to provide thermal features as an input
    to a simplified crack formation criterion based on the Rappaz-Drezet-Gremaud model.
    In this work, we apply the Bayesian calibration techniques originally proposed
    in [Kennedy, Marc C and O'Hagan, Anthony: Journal of the Royal Statistical Society
    B, 63, 425--464 (2001)]   to build a surrogate model for the multi-source Rosenthal
    model, and also a model deficiency function that accounts for the discrepancy
    between the model output and the experimentally observed data.  We use the Kennedy-O'Hagan
    model, to perform a probabilistic calibration of the thermal models proposed in
    Kitt2023. In the original work the analytic thermal models are calibrated using
    a grid search method which is computationally expensive, and does not quantify
    calibration parameter uncertainty. The surrogate model improves upon this, requiring
    only a few minutes to generate a posterior distribution for the calibration parameters
    and quantifying uncertainty, and achieving superior accuracy.
- poster: 139602
  presenter: "Blount, Alyssa"
  email: "845b0f02ef2aa9ce"
  title: |-
    MS44: Predicting Appendicular Lean Mass, Bone Density and Body Fat Using
    P-Laplacian Based Semi-Supervised Learning
  abstract: |-
    Nutritionists require accurate data on critical body characteristics such
    as Appendicular Lean Mass (ALM), Bone Density (BD), and Body Fat Percentage (BF)
    to conduct their research. Current methods to predict ALM, BD, and BF such as
    DEXA (Dual X-ray Absorptiometry) scans are not cost-effective. Previous studies
    have shown biomarkers such as waist circumference, arm length, and head circumference
    have correlations with ALM, BD, and BF. We applied semi-supervised learning algorithms
    to predict these three quantities using data on 40+ biomarkers of 700 patients
    provided by Pennington Biomedical Research Center or PBRC. Semi-supervised learning
    algorithms have been extensively used for classification and regression, especially
    when limited training data is available. An effective technique for exploiting
    unlabeled data in semi-supervised learning is to utilize a graph structure, which
    may be intrinsic to the data, or constructed based on similarities between data
    points. p- Laplacian is one of many graph-based semi-supervised learning techniques.
    This technique has only been used for classification problems, but a recent paper
    has shown it can also be used for regression. In this poster, we will elucidate
    the mathematical foundations behind p-Laplacian and demonstrate how we used it
    to predict ALM, BD, and BF by exploiting the similarity between the patients.
    We will also compare the results with those of supervised learning algorithms.
- poster: 140262
  presenter: "Boahen, Edem"
  email: "94cf187c9309d6ae"
  title: "MS67: An Efficient Streaming Algorithm for Sparse Eigenvector"
  abstract: |-
    In the era of big data, handling large datasets efficiently presents formidable
    challenges. Our research introduces a novel algorithm tailored to approximate
    sparse eigenvectors of large matrices. Given a positive semi-definite $A \in \mathbb{C}^{N\times
    N}$, we show that $MAM^*$ where $M$ is a carefully chosen matrix, provides an
    effective way to approximate the eigenvectors of $A$. This drastically reduces
    the need for frequent memory accesses to the original matrix $A$. Furthermore,
    our algorithm is accompanied by theoretical guarantees, ensuring reliability and
    robustness.
- poster: 139569
  presenter: "Boettcher, Lucas"
  email: "fbd92cadfd720d81"
  title: "MS29: Complex Networks with Complex Weights"
  abstract: "In many studies, it is common to use binary (i.e., unweighted) edges to examine networks of entities that are either adjacent or not adjacent. Researchers have generalized such binary networks to incorporate edge weights, which allow one to encode node–node interactions with heterogeneous intensities or frequencies (e.g., in transportation networks, supply chains, and social networks). Most such studies have considered real-valued weights, despite the fact that networks with complex weights arise in fields as diverse as quantum information, quantum chemistry, electrodynamics, rheology, and machine learning. Many of the standard network-science approaches in the study of classical systems rely on the real-valued nature of edge weights, so it is necessary to generalize them if one seeks to use them to analyze networks with complex edge weights. In this paper, we examine how standard network-analysis methods fail to capture structural features of networks with complex edge weights. We then generalize several network measures to the complex domain and show that random-walk centralities provide a useful approach to examine node importances in networks with complex weights."
- poster: 140083
  presenter: "Botvinick-Greenhouse, Jonah"
  email: "d2fdc3114a988ae1"
  title: "MS26: Comparing Dynamical Systems Using Time-Delayed Invariant Measures"
  abstract: |-
    In recent years, several works have compared dynamical systems by studying
    the discrepancy between their invariant measures under a suitable metric or divergence
    on the space of probability measures. While the robustness of invariant measures
    to noisy measurements, uncertain initial conditions, and slow sampling makes them
    appealing tools for comparing dynamical systems in applications, the approach
    is also limited by the existence of infinitely many distinct systems all admitting
    the same invariant measure. To overcome this difficulty, we instead propose studying
    invariant measures in time-delayed coordinate systems. We present theoretical
    results which show that, up to a topological conjugacy, the time-delayed invariant
    measure can distinguish between dynamical systems, and we provide numerical examples
    which demonstrate the utility of our proposed method.
- poster: 139747
  presenter: "Brarda, Francesco"
  email: "9f3e811700d3d6ea"
  title: "MS28: Efficient Hybrid Learning of Spatial-Temporal Operator Networks"
  abstract: |-
    Recent developments in operator-type neural networks, including Fourier
    Neural Operator (FNO) and Deep Operator Network (DeepONet), have shown encouraging
    potential for simulating solutions to spatial-temporal Partial Differential Equations
    (PDEs). Nevertheless, these networks often require significant training resources
    and may not consistently reach the accuracy needed in various scientific and engineering
    fields. In this presentation, we introduce a new framework for operator learning
    that aims to mitigate these challenges. This new approach integrates classical
    insights from numerical PDE theory and methodologies to enhance the functionality
    of existing operator neural networks. Our innovative framework specifically targets
    low-frequency errors through its design, while incorporating a linear layer to
    manage high-frequency errors. Through numerical tests on a widely recognized 2D
    Navier-Stokes equations benchmark, we demonstrate notable gains in both computational
    efficiency and accuracy over current FNO models and conventional numerical methods.
- poster: 139715
  presenter: "Bryutkin, Andrey"
  email: "f7a40d98c47fec41"
  title: |-
    MS71: Techniques for Reducing Dimensions and Quantifying Uncertainty in
    Dynamical Systems
  abstract: |-
    Understanding and predicting the behavior of dynamical systems through
    partial observations is a critical challenge across various disciplines. In this
    research, we work towards a robust framework that integrates analytical approaches
    for dimensional reduction with a Bayesian framework for uncertainty quantification,
    tailored to the unique complexities of partially observed dynamical systems with
    inherent noise and symmetries. The work centers on a system observed over time,
    subject to additional noise and incomplete information retrieval through an observation
    function. This setup commonly results in data that are either sparse or redundant,
    complicating the process of making accurate inferences. By leveraging dimensional
    reduction, our approach effectively simplifies the complexity of the data, while
    preserving essential system dynamics. Besides this, we employ a Bayesian framework
    specifically designed to address uncertainties in parameter inference. We will
    show our results for linear cases and some preliminary results in non-linear cases.
    The flexibility of our approach, agnostic to specific machine learning models,
    enables its application across a broad spectrum of disciplines, paving the way
    for more precise and reliable predictions in scenarios where traditional models
    might falter due to incomplete or noisy data.
- poster: 140525
  presenter: "Buchanan, Sam"
  email: "d6b0c8b10680ea83"
  title: "MS89: White-Box Transformers Via Sparse Rate Reduction"
  abstract: |-
    In this work, we contend that a natural objective of representation learning
    is to compress and transform the distribution of the data, say sets of tokens,
    towards a low-dimensional Gaussian mixture supported on incoherent subspaces.
    The goodness of such a representation can be evaluated by a principled measure,
    called sparse rate reduction, that simultaneously maximizes the intrinsic information
    gain and extrinsic sparsity of the learned representation. From this perspective,
    popular deep network architectures, including transformers, can be viewed as realizing
    iterative schemes to optimize this measure. Particularly, we derive a transformer
    block from alternating optimization on parts of this objective: the multi-head
    self-attention operator compresses the representation by implementing an approximate
    gradient descent step on the coding rate of the features, and the subsequent multi-layer
    perceptron sparsifies the features. This leads to a family of white-box transformer-like
    deep network architectures, which we call CRATE, which are mathematically fully
    interpretable. Experiments show that these networks, despite their simplicity,
    indeed learn to compress and sparsify representations of large-scale real-world
    image and text datasets, and achieve performance very close to highly engineered
    transformer-based models, including ViT and GPT2.
- poster: 140277
  presenter: "Budzinski, Roberto"
  email: "51e25484bcef9afa"
  title: |-
    MS36: An Exact Mathematical Description of Computation with Transient Spatiotemporal
    Dynamics in a Complex-Valued Neural Network
  abstract: "We study a complex-valued neural network (cv-NN) with linear, delayed interactions. We report the cv-NN displays sophisticated spatiotemporal dynamics, including partially synchronized “chimera” states. We then use these spatiotemporal dynamics, in combination with a nonlinear readout, for computation. The cv-NN can instantiate dynamics-based logic gates, encode short-term memories, and mediate secure message passing through a combination of interactions and time delays. The computations in this system can be fully described in an exact, closed-form mathematical expression. Finally, using direct intracellular recordings of neurons in slices from neocortex, we demonstrate that computations in the cv-NN are decodable by living biological neurons as the nonlinear readout. These results demonstrate that complex-valued linear systems can perform sophisticated computations, while also being exactly solvable. Taken together, these results open future avenues for design of highly adaptable, bio-hybrid computing systems that can interface seamlessly with other neural networks."
- poster: 139032
  presenter: "Bugalia, Sarita"
  email: "ae6e77326d89cf99"
  title: |-
    A Mathematical Model at the Wildlife-Livestock-Human Interface to Predict
    the Implications of Measures to Control Bovine Tuberculosis
  abstract: |-
    The complex and dynamic interactions among wildlife, livestock, and humans
    create environments conducive to the emergence of new diseases or the reemergence
    of existing ones. Such outbreaks pose a significant threat to multiple host-species
    and, in case of cattle infections, farmers face large economic losses. Our study
    focuses on Mycobacterium bovis that infects domestic and wild animals, as well
    as humans. In particular, the presence of M. bovis in a wildlife reservoir compromises
    disease control efforts in the livestock sector and increases the risk of infection
    in humans involved in wildlife activities (e.g. hunting). To comprehend the transmission
    dynamics of M. bovis among wildlife, livestock, and human populations, we propose
    a mathematical model that incorporates cross-species transmission between livestock,
    wildlife, and humans, as well as contaminated environment transmission routes.
    The novelty of our model lies in integrating multiple host-species (three distinct
    populations) plus an external heterogeneous environmental transmission source.
    The state of Michigan, USA is selected as a case study to estimate the model parameters
    and disease burden of bovine tuberculosis. Vaccination of livestock and wildlife
    scenario analysis are performed to quantify the effectiveness of these control
    strategies to reduce the transmission of bovine tuberculosis among wildlife, livestock,
    and human populations.
- poster: 139679
  presenter: "Burns, Benjamin"
  email: "879a7fa19bbe09e9"
  title: "MS3: A Mean-Field Games Approach to Score-Based Generative Modeling"
  abstract: |-
    Advancements in diffusion-based generative modeling have made high quality
    generative models easily accessible. Recently it has been shown that score-based
    generative models can be formulated in terms of a mean-field game (MFG). This
    MFG formulation admits alternate characterizations of the SGM based on its optimality
    conditions, which are a set of coupled nonlinear partial differential equations
    (PDE). The first PDE is a controlled Fokker-Planck equation which is equivalent
    to the denoising process in SGM, while the second PDE is a HJB equation that characterizes
    the optimal controller, whose solution is related to the score function. Based
    on this mathematical connection, we propose a new objective function for learning
    the score function. In addition to the implicit score-matching objective, we introduce
    two regularizers, the first based on measuring the discrepancy in the HJB equation,
    the second based on measuring discrepancy in satisfying the terminal condition.
    By including these regularizers, we are informing the structure of the score function,
    thereby constraining the search space. This strategy is well-grounded through
    the theory of mean-field games and HJB equations, and develops a PDE-based theory
    for explaining and understanding the role of latent space in SGM. Through experiments
    we will show that the HJB regularizer helps learn the score function in a more
    stable way, while the terminal condition regularizer is associated with higher
    quality samples.
- poster: 140044
  presenter: "Buzzy, Michael"
  email: "677fb6ab107b5d7f"
  title: |-
    MS9: Active Learning for the Design of Polycrystalline Materials Using Conditional
    Normalizing Flows
  abstract: |-
    Generative modeling has opened new avenues for solving previously intractable
    materials design problems. However, these new opportunities are accompanied by
    a drastic increase in the required amount of training data. This is in stark juxtaposition
    to the high expense and scarcity of such large materials datasets. In this work,
    we propose a novel framework for integrating generative models into an active
    learning loop. This enables the training of generative models with datasets far
    smaller than what has traditionally been demonstrated. The framework is demonstrated
    to the problem of designing polycrystalline textures associated with target anisotropic
    mechanical properties. Our active learning strategy outperforms random sampling
    with an $\sim$18x reduction in the number of simulations required for training.
- poster: 139578
  presenter: "C G, Krishnanunni"
  email: "b626e151d883a006"
  title: "MS85: Approaches for Deep Neural Network Architecture Adaptation"
  abstract: |-
    This poster presents two novel strategies for progressively adapting
    neural network architecture along the depth. In particular, we attempt to address
    the following questions in a mathematically principled way: i) Where to add new
    capacity (layer) during the training process? ii) How to initialize the new capacity?.
    The first approach is based on a layerwise training strategy where a new layer
    is added each time and trained independently by freezing parameters in the previous
    layers. The algorithm uses a data-dependent regularizer that force each hidden
    layer to learn meaningful representation of the data.  The second approach is
    based on defining the sensitivity for a neural network which is conceptually the
    derivative of a lost function with respect to infinitesimal changes in the neural
    network. Using an optimal control viewpoint, we show that the network derivative
    exists under certain conditions and a closed-form expression for the same is derived.  The
    algorithm we derived simply determines the most sensitive location along the depth
    where a new layer needs to be inserted during the training process and the associated
    parametric initialization for the newly added layer. Numerical investigations
    with feed-forward networks (fully connected network and convolutional neural network)
    on prototype regression and classification problems demonstrate that both our
    proposed approaches can outperform an ad-hoc baseline network and other neural
    architecture adaptation strategies.
- poster: 139617
  presenter: "Cai, Difeng"
  email: "1bcbf15e730b6a43"
  title: |-
    MS82: Autm Flow: Atomic Unrestricted Time Machine for Monotonic Normalizing
    Flows
  abstract: |-
    Normalizing flows constitute an important class of generative models.
    It tries to provide a generally nonlinear bijective mapping between the base distribution
    and the target distribution. How to design an architecture that is computationally
    efficient and expressive is a central topic. We present a novel normalizing flow
    called AUTM that allows triangular Jacobian, tractable inverse calculation, and
    provable universality. We compare AUTM to popular flow architectures such as RealNVP,
    GLOW, NSF, FFJORD, UMNN, BNAF, etc. to demonstrate the efficiency and expressive
    power over machine learning and large scale image data sets.
- poster: 139609
  presenter: "Cai, HanQin"
  email: "96fa239f149073cb"
  title: |-
    MS20: Associated Poster: A Zeroth-Order Block Coordinate Descent Algorithm
    for Huge-Scale Black-Box Optimization
  abstract: |-
    We consider the zeroth-order optimization problem in the huge-scale setting,
    where the dimension of the problem is so large that performing even basic vector
    operations on the decision variables is infeasible. In this paper, we propose
    a novel algorithm, coined ZO-BCD, that exhibits favorable overall query complexity
    and has a much smaller per-iteration computational complexity. In addition, we
    discuss how the memory footprint of ZO-BCD can be reduced even further by the
    clever use of circulant measurement matrices. As an application of our new method,
    we propose the idea of crafting adversarial attacks on neural network based classifiers
    in a wavelet domain, which can result in problem dimensions of over one million.
    In particular, we show that crafting adversarial examples to audio classifiers
    in a wavelet domain can achieve the state-of-the-art attack success rate of 97.9%
    with significantly less distortion.
- poster: 140064
  presenter: "Cairns, Darby"
  email: "948e3c22a7e0e81b"
  title: |-
    Particle Swarm Optimization for Generating Virtual Populations of Cardiac
    Patients
  abstract: |-
    There has recently been increased interest in virtual cadres of patients
    in the study of cardiac electrophysiology. Rather than examining a model's behavior
    using a single representative parameter set, populations of many virtual patients
    configured to reproduce the variability of dynamics seen in real world data allow
    deeper insight into potential effects of interventions, drug-based or otherwise.
    However, finding parameter sets for models to generate these virtual populations
    is a difficult and time-consuming process. Derivative-free optimization methods
    such as particle swarm optimization (PSO) have been used for fitting models to
    specific datasets, yielding individual, low-error parameter sets. We demonstrate
    a novel approach which exploits the power of PSO as a tool to explore the search
    space efficiently, rather than yielding a single solution. Our modified PSO, referred
    to as population PSO (PPSO), discourages convergence to a local minimum, and instead
    focuses on guiding the search to explore low-error areas of the search space,
    yielding many parameter sets which reproduce the variability of biomarkers seen
    in real tissue data. Our browser-based implementation uses WebGL, taking advantage
    of GPU parallel processing capability to provide a fast, flexible tool allowing
    researchers to quickly obtain customized cadres of virtual patients.
- poster: 140036
  presenter: "Calle-Saldarriaga, Alejandro"
  email: "91b078eb54565dda"
  title: |-
    MS9: Generative Multi-Fidelity Modeling and Downscaling Via Spatial Autoregressive
    Gaussian Processes
  abstract: |-
    Computer models are often run at different fidelities or resolutions due
    to trade-offs between computational cost and accuracy. For example, global circulation
    models can simulate climate on a global scale, but they are too expensive to be
    run at a fine spatial resolution. Hence, regional climate models (RCMs) forced
    by GCM output are used to simulate fine-scale climate behavior in regions of interest.
    We propose a highly scalable generative approach for learning high-fidelity or
    high-resolution spatial distributions conditional on low-fidelity fields from
    training data consisting of both high- and low-fidelity output. Our method learns
    the relevant high-dimensional conditional distribution from a small number of
    training samples via spatial autoregressive Gaussian processes with suitably chosen
    regularization-inducing priors. We demonstrate our method on simulated examples
    and for emulating the RCM distribution corresponding to GCM forcing using past
    data, which is then applied to future GCM forecasts.
- poster: 140258
  presenter: "Cao, Yan"
  email: "4e44c114ea7210bb"
  title: "Ordinal Pattern Preserving Surrogates for Non-Stationary Time Series"
  abstract: "Most real-world time series exhibit non-stationary behavior, where the statistical properties of the underlying process change over time. Standard surrogate methods may not adequately capture the dynamics of the data because of the stationarity assumption. Inspired by the success of ordinal pattern based analysis of non-stationary time series, we developed a new surrogate method which preserves the ordinal patterns up to a predefined length. This new surrogate method, named “Order Preserving surrogates' is computationally efficient and works well for both stationary and non-stationary time series. We evaluate our method on both synthetic and real world time series. Compared to other existing surrogate methodologies, our Order Preserving surrogates do a better job of capturing the dynamics of various signals. This new surrogate method can be used for hypothesis testing in various applications."
- poster: 140012
  presenter: "Cao, Yiyi"
  email: "86f0cb99733c3803"
  title: "User Analysis for Instagram Operational Improvement"
  abstract: |-
    Instagram is a popular social media site around the world. Maintaining
    the social media environment has become a severe problem because of the fake users'
    intervention leading to some malicious activities in recent years. Due to the
    growth of users and how to enhance user satisfaction, it's essential to identify
    the types of fake users to set specific regulatory measures, while the types of
    authenticated users facilitate business cooperation. Combining identification
    and cluster analysis into algorithms will correlate high accuracy and differentiate
    the user types. What's more, an item-based ideal value provided will strengthen
    the users' account build-up. Instagram can take advantage of the functions to
    detect both fake users and authenticate users to improve the efficiency of the
    user detection process. Moreover, the adoption of item-based recommendation analysis
    will also enable the company to develop a user improvement service, helping the
    users improve their Instagram account operation.
- poster: 140164
  presenter: "Catlett, Christina"
  email: "69281353839ae439"
  title: |-
    MS52: Model-Guided Experiment Design to Characterize Spatial Organization
    of Bacterial Metabolism
  abstract: |-
    Bacteria are known to organize metabolism spatially through the encapsulation
    of enzymes, substrates, and cofactors in protein-based microcompartments. Microcompartments
    are of great interest in bioengineering for their ability to control local reaction
    environments and concentrate toxic intermediates, but key physical properties
    are unable to be measured directly. To probe key properties including permeability,
    we built a kinetic model and developed an experimental design method based on
    principal component analysis to inform sampling times of an experimental assay.
    Our method successfully constrains the parameter-of-interest, and parameter estimates
    match previous theoretical predictions. We additionally present preliminary results
    using experimental design to propose experiments which discriminate between equally
    parsimonious models in a sparse model selection framework.
- poster: 140221
  presenter: "Chak, Matthew"
  email: "e1b61014ae950ff7"
  title: "Finding Multiple Optimal Solutions"
  abstract: |-
    Recent papers have shown that, after fixing a loss function and a dataset,
    models tend to converge to similar probability distributions regardless of initialization
    and dataset shuffling. It is thus tempting to conclude that multiple models trained
    on a given dataset with a given loss will converge to a fixed interpretation (that
    is, the relative importance of features will be roughly the same for each model).
    In our work, we show empirically that although randomly initialized models do
    tend to converge to the same interpretation, other interpretations with similar
    training and test accuracy metrics exist in at least some datasets. We also introduce
    a method to find models that take on these different interpretations. Exploring
    these alternate models is important, as some may have interpretations that more
    closely reflect the underlying patterns than those found through random initialization.
- poster: 140160
  presenter: "Chan, Ga-Ming (Angus)"
  email: "78be8890f9563c5d"
  title: "MS2: Inference on Interaction Hypergraphs"
  abstract: |-
    While there has been tremendous activity in the area of network inference,
    hypergraphs have not enjoyed the same attention, on account of their relative
    complexity and the lack of tractable statistical models. We introduce a hyper-edge-centric
    model for analyzing hypergraphs, called the interaction hypergraph, which models
    natural sampling methods for hypergraphs in neuroscience and communication networks,
    and accommodates interactions involving different numbers of entities. We define
    latent embeddings for the interactions in such a network, and analyze their estimators.
- poster: 139714
  presenter: "Chaudhry, Abraar"
  email: "f4cec83511dcd08d"
  title: "MS49: Learning Nonnegative Matrix Factorization from Compressed Data"
  abstract: |-
    We study how to find a nonnegative matrix factorization (NMF) from compressed
    measurements.  We consider methods of compression which can be adapted to the
    data, or can be oblivious.  We formulate optimization problems that only depend
    on the compressed data, but which can recover a nonnegative factorization which
    closely approximates the original matrix.  These optimization problems can be
    approached with a variety of algorithms, and in particular, we discuss variations
    of the popular multiplicative updates method for these compressed problems.  We
    test our approaches on examples to validate performance in real-world applications.
    For the section "Efficient and robust optimization techniques for structured data
    learning".
- poster: 140210
  presenter: "Chen, Ke"
  email: "c7cf2d11c57cbf9d"
  title: |-
    MS19: Variational Framework for Super-Resolution 3D Surface Reconstruction
    from Limited Inputs in Multimodal Imaging
  abstract: |-
    In many imaging applications, the obtained images are often in low resolution
    or even have missing data due to practical and hardware limitations, which can
    impact subsequent 3D reconstruction. For instance, X-ray imaging carries radiation
    risks, limiting data collection time. Terahertz (THz) imaging, while safe, is
    slow and affected by diffraction and noise. Magnetic Resonance Imaging (MRI) struggles
    with the small region of interest in a high-resolution image. To address these
    challenges, a new framework using the Euler-Elastica regulariser is presented
    to reconstruct high-resolution surfaces from a few low-resolution 2D slices, combining
    mathematical models with local edge features and global smoothness. Two algorithms
    are developed (a projected gradient descent method and the alternating direction
    method of multipliers), and quantitative comparisons based on discrete curvatures
    show superiority over other regularisers. Practical examples in X-ray, MRI, and
    THz imaging validate its effectiveness, offering potential applications in medical
    imaging and computer vision. Joint work with Prof Ke Chen (Strathclyde and Liverpool),
    and Prof Shang-Hua Yang (NTHU, Taiwan).
- poster: 139946
  presenter: "Chen, Ziyu"
  email: "4ea3a5f627f7ce53"
  title: "MS3: Sample Complexity of Probability Divergences under Group Symmetry"
  abstract: |-
    We rigorously quantify the improvement in the sample complexity of variational
    divergence estimations for group-invariant distributions. In the cases of the
    Wasserstein-1 metric and the Lipschitz-regularized a-divergences, the reduction
    of sample complexity is proportional to an ambient-dimension-dependent power of
    the group size. For the maximum mean discrepancy (MMD), the improvement of sample
    complexity is more nuanced, as it depends on not only the group size but also
    the choice of kernel. We provide numerical simulations to verify our theories.
- poster: 139474
  presenter: "Chen, Haoxuan"
  email: "1342cbee8b921175"
  title: |-
    MS72: When Can Regression-Adjusted Control Variate Help? Rare Events, Sobolev
    Embedding and Minimax Optimality
  abstract: |-
    This paper studies the use of a machine learning-based estimator as a
    control variate for mitigating the variance of Monte Carlo sampling. Specifically,
    we seek to uncover the key factors that influence the efficiency of control variates
    in reducing variance. We examine a prototype estimation problem that involves
    simulating the moments of a Sobolev function based on observations obtained from
    (random) quadrature nodes. Firstly, we establish an information-theoretic lower
    bound for the problem. We then study a specific quadrature rule that employs a
    nonparametric regression-adjusted control variate to reduce the variance of the
    Monte Carlo simulation. We demonstrate that this kind of quadrature rule can improve
    the Monte Carlo rate and achieve the minimax optimal rate under a sufficient smoothness
    assumption. Due to the Sobolev Embedding Theorem, the sufficient smoothness assumption
    eliminates the existence of rare and extreme events. Finally, we show that, in
    the presence of rare and extreme events, a truncated version of the Monte Carlo
    algorithm can achieve the minimax optimal rate while the control variate cannot
    improve the convergence rate.
- poster: 139610
  presenter: "Chen, Tianlong"
  email: "b83b8622194d6115"
  title: |-
    MS20: Associated Poster: Revisiting Zeroth-Order Optimization for Memory-Efficient
    Llm Fine-Tuning: A Benchmark
  abstract: |-
    In the evolving landscape of natural language processing (NLP), fine-tuning
    pre-trained Large Language Models (LLMs) with first-order (FO) optimizers like
    SGD and Adam has become standard. Yet, as LLMs grow {in size}, the substantial
    memory overhead from back-propagation (BP) for FO gradient computation presents
    a significant challenge. Addressing this issue is crucial, especially for applications
    like on-device training where memory efficiency is paramount. This paper proposes
    a shift towards BP-free, zeroth-order (ZO) optimization as a solution for reducing
    memory costs during LLM fine-tuning, building on the initial concept introduced
    by MeZO. Unlike traditional ZO-SGD methods, our work expands the exploration to
    a wider array of ZO optimization techniques, through a comprehensive, first-of-its-kind
    benchmarking study across five LLM families (Roberta, OPT, LLaMA, Vicuna, Mistral),
    three task complexities, and five fine-tuning schemes. Our study unveils previously
    overlooked optimization principles, highlighting the importance of task alignment,
    the role of the forward gradient method, and the balance between algorithm complexity
    and fine-tuning performance. We further introduce novel enhancements to ZO optimization,
    including block-wise descent, hybrid training, and gradient sparsity. Our study
    offers a promising direction for achieving further memory-efficient LLM fine-tuning.
- poster: 139871
  presenter: "Chen, Shaoxuan"
  email: "fde32c9d3b4b1f41"
  title: |-
    MS40: Neural Deflation for Data-Driven Discovery of Conservation Laws from
    Trajectories
  abstract: ""
- poster: 140558
  presenter: "Chen, Tianyi"
  email: "7f8377ece60878e7"
  title: "MS75: Euclidean Mirrors and First-Order Changepoints in Network Time Series"
  abstract: |-
    We describe a model for a network time series whose evolution is governed
    by an underlying stochastic process, known as the latent position process, in
    which network evolution can be represented in Euclidean space by a curve, called
    the Euclidean mirror. We define the notion of a first-order changepoint for a
    time series of networks, and construct a family of latent position process networks
    with underlying first-order changepoints. We prove that a spectral estimate of
    the associated Euclidean mirror localizes these changepoints, even when the graph
    distribution evolves continuously, but at a rate that changes. Simulated and real
    data examples on organoid networks show that this localization captures empirically
    significant shifts in network evolution.
- poster: 140189
  presenter: "Chen, Joshua"
  email: "9ce4da630caf0fd3"
  title: |-
    MS5: Scalable and Efficient Lazy Maps Enabled by Derivative-informed Neural
    Operators
  abstract: "We propose a fast and scalable method for solving high-dimensional PDE-constrained Bayesian inverse problems (BIPs). The method synthesizes a structure-exploiting lazy map, a measure transport technique for solving high-dimensional Bayesian inference, and a derivative-informed neural operator (DINO) surrogate of the forward operator. Training a parametrized lazy map to push prior samples to posterior samples involves minimizing the evidence lower bound. This online training procedure requires repeated evaluations of the log-likelihood function and its gradient (i.e., the score function), which can be prohibitive for PDE-based likelihood models. A DINO surrogate is trained offline via derivative-informed operator learning using samples of the forward operator and its Fr\\'echet derivative. The surrogate can achieve high accuracy in both log-likelihood and score evaluations at a much lower cost than conventional operator learning methods. Furthermore, a reduced basis DINO surrogate can be seamlessly integrated into the lazy map structure by sharing a set of reduced bases spanning a derivative-informed subspace found from samples. Numerical results on challenging BIPs suggest that highly accurate DINO-driven lazy maps can be trained as quickly as a few seconds up to a few minutes, depending on the difficulty of the BIPs."
- poster: 140021
  presenter: "Chen, Haowei(Alice)"
  email: "9990089206b9baf1"
  title: "MS16: Fractional-Order Dynamics Control using Kolmogorov–Arnold Networks(KAN)"
  abstract: "Fractional calculus and Kolmogorov–Arnold networks (KANs) are both adept at encapsulating historical dependencies within dynamic systems. Former utilize fractional calculus to inherently model memory effects, whereas KANs leverage their function approximation capabilities to learn and represent historical data from the system’s behavior. A Fractional dynamic system combined with KANs was introduced, and we proposed the solution uniquely existed with numerical experiments for validation.Finally, we apply our theory to the Path Tracking system where the slip effect is involved and learned through KANs."
- poster: 140562
  presenter: "Chen, Yuntian"
  email: "d9ab19a10a969d59"
  title: "MS31: Equation Discovery Via Symbolic Mathematics"
  abstract: |-
    Mathematical equations provide profound insights about complex dynamics
    across various scientific disciplines. However, discovering such insightful equations
    in real-world scenarios is faced with significant challenges due to the absence
    of prior knowledge and sparse and limited data. To address these challenges, we
    propose a robust framework to discover open-form equations directly from data
    without prior knowledge. First, symbolic mathematics are utilized to realize the
    flexible representation of any equations in a binary tree structure. Diverse equations
    are generated by a reinforcement learning (RL)-guided hybrid generator and their
    rewards are evaluated by a neural network-based predictive model. The structure
    of equations is iteratively optimized using a RL strategy and the best-performing
    equation is finally selected by a parameter-free stability metric. Second, to
    avoid the overfitting of noise, the initially identified equation is integrated
    as a physical constraint into the predictive model for robust evaluation and more
    accurate discovery. According to extensive experiments, our framework is capable
    of uncovering true equations with complex structures even from limited and highly
    noisy data. It also opens up new potential for exploring real-world systems with
    limited understanding.
- poster: 139787
  presenter: "Chen, Jiahui"
  email: "255788e4935bd760"
  title: "MS51: Clustering Phylogenetic Analysis of Influenza Mutation Data"
  abstract: |-
    The impact of virus mutations on public health is profound, leading to
    increased infectivity, vaccine resistance, and alterations in disease severity.
    Annually, the CDC characterizes approximately 2,000 virus strains to monitor their
    evolution and confirm vaccine effectiveness. Grasping the essence of viral mutagenesis
    and evolution is paramount. Through the integration of genomic analysis, clustering,
    and dimensionality reduction methods, our study specifically investigates the
    influence of COVID-19 on influenza virus propagation. Our findings reveal that
    a robust dimension reduction and clustering approach can yield promising results
    in deciphering the complex dynamics of virus mutation, providing valuable insights
    for future research and public health strategies.
- poster: 140287
  presenter: "Chen, Yifan"
  email: "cf2d215d9601595f"
  title: "MS66: Conditional Sampling with Stochastic Interpolants and Follmer Processes"
  abstract: |-
    In this work, we address probabilistic forecasting via generative modeling.
    To do so, we construct novel SDEs that map a point mass of the current state to
    a conditional distribution of future states, based on the framework of stochastic
    interplants. The SDEs can be learned from data via simulation-free training, and
    the drifts can be tuned a posteriori to optimize the estimation accuracy in KL
    divergence of path measures. We show that the optimal drift corresponds to Follmer
    processes. Experiments on stochastic Navier Stokes equations and video forecasting
    demonstrate the effectiveness and scope of this work.
- poster: 140255
  presenter: "Chen, Haoxuan"
  email: "1342cbee8b921175"
  title: "An Improved Analysis of Parallel Diffusion Generative Model Sampling"
  abstract: |-
    Recently, diffusion models have exerted huge impact on many tasks, including
    but not limited to generative artificial intelligence, scientific machine learning
    (AI4Science) and optimization. However, generation of the samples can be slow
    in many cases as diffusion models depend on an iterative sampling procedure. Among
    many studies focusing on how to accelerate the sampling of diffusion models, one
    approach is the ParaDiGMS algorithm, which leverages the Picard iteration to parallelize
    the sampling procedure. Despite the empirical success achieved by the ParaDiGMS
    algorithm, theoretical understanding of the parallelized sampling technique in
    the context of diffusion-based generative models is still lacking. Our theoretical
    analysis in this work reveals that the parallelized sampling technique would eventually
    bring about the first implementation of the diffusion model with O(polylog d)
    time complexity, where d denotes the dimension of the underlying data distribution.
- poster: 140520
  presenter: "Chen, Yudong"
  email: "b576e332cd8906d4"
  title: |-
    MS14: Stochastic Algorithms As Markov Chains in Min-Max and Variational
    Inequality Problems
  abstract: |-
    Many reinforcement/machine learning problems involve loss minimization,
    min-max optimization and fixed-point equations, all of which can be cast as Variational
    Inequality (VI) problems. Stochastic methods like SGD, SEG, and TD/Q Learning
    are prevalent, and their constant stepsize versions have gained popularity due
    to effectiveness and robustness. Viewing the algorithm's iterates as a Markov
    chain, we study their fine-grained probabilistic behavior. In particular, we establish
    finite-time geometric convergence of the iterate's distribution and relate the
    ergodicity properties of the Markov chain to the characteristics of the VI, algorithm
    and data. Using coupling and drift analysis, we characterize the limit distribution
    and how its bias depends on stepsize. For smooth problems, exemplified by TD learning
    and smooth min-max optimization, the bias is proportional to the stepsize. For
    nonsmooth problems, exemplified by Q-learning and ReLU regression, the bias has
    drastically different behavior and scales with the square root of the stepsize.
    This probabilistic characterization allows for variance reduction via tail-averaging
    and bias reduction via Richardson-Romberg extrapolation. The combination of constant
    stepsize, averaging, and extrapolation provides a favorable balance between fast
    mixing and low long-run error. We demonstrate its effectiveness in statistical
    inference compared to traditional diminishing stepsize schemes.
- poster: 139009
  presenter: "Chen, Peiyi"
  email: "f2fa6b26f9451924"
  title: "Reconstruction of Phonon Relaxation Time Using Boltzmann Transport Equation"
  abstract: "In this poster we present a recent completion in addressing an inverse problem associated with the phonon transport equation—a fundamental model in material science for heat conductance. Common laboratory practice involves employing temperature measurement data to deduce the relaxation coefficient $\\tau$ of the material under investigation. Our work approaches this problem from a mathematical perspective, questioning whether the temperature provides adequate information for a successful inversion and exploring methodologies for executing such an inverse problem.  To establish the well-posedness of the inversion process, we employ singular decomposition, elucidating the singular component of the solution to trace the information embedded in the equation. Subsequently, PDE-constrained optimization is utilized for the numerical reconstruction where Stochastic Gradient Descent is applied. Moreover, we compare the non-constant heat conductivity in non-diffusive regime compared with that in diffusive regime.  This is a joint work with Irene Gamba, Qin Li and Li Wang."
- poster: 139734
  presenter: "Chen, Haoxuan"
  email: "1342cbee8b921175"
  title: "MS33: Ensemble-Based Annealed Importance Sampling"
  abstract: |-
    Sampling from a multimodal distribution is a fundamental and challenging
    problem in computational science and statistics. Among various approaches proposed
    for this task, one popular method is Annealed Importance Sampling (AIS). In this
    paper, we propose an ensemble-based version of AIS by combining it with population-based
    Monte Carlo methods to improve its efficiency. By keeping track of an ensemble
    instead of a single particle along some continuation path between the starting
    distribution and the target distribution, we take advantage of the interaction
    within the ensemble to encourage the exploration of undiscovered modes. Specifically,
    our main idea is to utilize either the snooker algorithm or the genetic algorithm
    used in Evolutionary Monte Carlo. We discuss how the proposed algorithm can be
    implemented and derive a partial differential equation governing the evolution
    of the ensemble under the continuous time and mean-field limit. We also test the
    efficiency of the proposed algorithm on various continuous and discrete distributions.
- poster: 140084
  presenter: "Chen, Haoyu"
  email: "97b8022843fd09a4"
  title: "MS86: Largest Angle Path Distance for Multi-Manifold Clustering"
  abstract: |-
    We propose a novel, angle-based path metric for the multi-manifold clustering
    problem. This metric, which we call the largest-angle path distance (LAPD), is
    computed as a bottleneck path distance in a graph constructed on $d$-simplices
    of data points. Under reasonable assumptions, this distance is small between data
    points from the same manifold and is large for data points from different manifolds.
    We provide a theoretical guarantee for recovering such an LAPD gap, even when
    the data is polluted with noise and when the underlying manifolds are curved.
    When data is sampled from a collection of $d$-dimensional manifolds which may
    intersect, the method can cluster the manifolds with high accuracy and automatically
    detect how many manifolds are present. By leveraging fast approximation schemes
    for bottleneck distance, this method exhibits quasi-linear computational complexity
    in the number of data points. In addition to being highly scalable, the method
    outperforms existing algorithms in numerous numerical experiments on intersecting
    manifolds, and exhibits robustness with respect to noise and curvature in the
    data.
- poster: 139723
  presenter: "Chen, Jessie"
  email: "3f76db7b7dcef89c"
  title: "Optimal Sensor Placements in Gaussian Processes Using Column Subset Selection"
  abstract: "In this work, we focus on optimal sensor placement which is then used in a Gaussian Process (GP) to reconstruct a spatially dependent function from a limited set of measurements. We develop an algorithm to select k sensors out of a much larger candidate set of m sensors such that the Bayesian D-optimality of the selected points is maximized. Our approach is to view sensor placement as a column subset selection problem (CSSP) on the kernel matrix of spatial data. We propose an algorithm that uses the Golub-Klema-Stewart approach (GKS) to select sensors which are then combined with a GP regression to reconstruct the function. We also provide an analysis of lower bounds on the D-optimality of these sensor placements using this approach. To reduce the computational cost in the GKS step, we will investigate low rank kernel methods such as Nyström approximation and Randomly Pivoted Cholesky. We also compare our algorithm against a greedy algorithm in numerical experiments on interpolation problems and spatial temperature data."
- poster: 139988
  presenter: "Chen, Long"
  email: "56e86ce58475f5f5"
  title: |-
    MS73: Data-Driven Aerodynamic Shape Design with Distributionally Robust
    Optimization
  abstract: |-
    We formulate and solve data-driven aerodynamic shape design problems with
    distributionally robust optimization (DRO) approaches. DRO aims to minimize the
    worst-case expected performance in a set of distributions that is informed by
    observed data with uncertainties. Building on the findings of the work [Gotoh
    el at.], we study the connections between a class of DRO and robust design optimization,
    which is classically based on the mean-variance (standard deviation) optimization
    formulation pioneered by Taguchi. Our results provide a new perspective to the
    understanding and formulation of robust design optimization problems. It enables
    data-driven and statistically principled approaches to quantify the trade-offs
    between robustness and performance, in contrast to the classical robust design
    formulation that captures uncertainty only qualitatively.
- poster: 140259
  presenter: "Chen, Yatong"
  email: "7ce5c1a4e1e8819a"
  title: "MS88: Metric-Fair Classifier Derandomization"
  abstract: "In this chapter, we study the problem of classifier derandomization in machine learning: given a stochastic binary classifier $f: X \\to [0,1]$, sample a deterministic classifier $\\hat{f}: X \\to \\{0,1\\}$ that approximates the output of $f$ in aggregate over any data distribution. Recent work revealed how to efficiently derandomize a stochastic classifier with strong output approximation guarantees, but at the cost of individual fairness --- that is, if $f$ treated similar inputs similarly, $\\hat{f}$ did not. In this paper, we initiate a systematic study of classifier derandomization with metric fairness guarantees. We show that the prior derandomization approach is almost maximally metric-unfair, and that a simple “random threshold' derandomization achieves optimal fairness preservation but with weaker output approximation. We then devise a derandomization procedure that provides an appealing tradeoff between these two: if $f$ is $\\alpha$-metric fair according to a metric $d$ with a locality-sensitive hash (LSH) family, then our derandomized $\\hat{f}$ is, with high probability, $O(\\alpha)$-metric fair and a close approximation of $f$. We also prove generic results applicable to all (fair and unfair) classifier derandomization procedures, including a bias-variance decomposition and reductions between various notions of metric fairness."
- poster: 139527
  presenter: "Cheng, Xiaoou"
  email: "ba93b78caea7c1db"
  title: "MS39: Improved Active Learning Via Dependent Leverage Score Sampling"
  abstract: |-
    We show how to obtain improved active learning methods in the agnostic
    (adversarial noise) setting by combining marginal leverage score sampling with
    non-independent sampling strategies that promote spatial coverage. In particular,
    we propose an easily implemented method based on the pivotal sampling algorithm
    , which we test on problems motivated by learning-based methods for parametric
    PDEs and uncertainty quantification. In comparison to independent sampling, our
    method reduces the number of samples needed to reach a given target accuracy by
    up to $50%$. We support our findings with two theoretical results. First, we show
    that any non-independent leverage score sampling method that obeys a weak \emph{one-sided
    $\ell_{\infty}$ independence condition} (which includes pivotal sampling) can
    actively learn $d$ dimensional linear functions with $O(d\log d)$ samples, matching
    independent sampling. This result extends recent work on matrix Chernoff bounds
    under $\ell_{\infty}$ independence, and may be of interest for analyzing other
    sampling strategies beyond pivotal sampling. Second, we show that, for the important
    case of polynomial regression, our pivotal method obtains an improved bound of
    $O(d)$ samples.
- poster: 139694
  presenter: "Cheng, Jiahui"
  email: "d4472570475984c1"
  title: |-
    MS35: Identification of Differential Equations Via Weighted Weak Form with
    Voting
  abstract: |-
    Modern data-driven approaches combining machine learning and numerical
    methods have provided intriguing possibilities of identifying differential equations
    from a given data set. Numerical identification of differential equations remains
    challenging. We explore a weighted weak/integral form of differential equations
    with respect to a collection of weighted test functions, and  introduce a voting
    strategy to identify the active features for stable identification.
- poster: 139926
  presenter: "Cheung, Siu Wun"
  email: "a463510cc395e880"
  title: |-
    MS70: libROM: a Free, Lightweight, Scalable C++ Library for Data-Driven
    Physical Simulation Methods
  abstract: |-
    In decision-making applications where multiple forward simulations are
    needed, such as parameter study, design optimization, optimal control, uncertainty
    quantification, and inverse problems, we need to repeatedly solve forward problems.
    However, subject to the model complexity and the fineness of the discretization,
    the computational cost of forward simulations can be high. It may take a long
    time to complete a single forward simulation with the available computing resource.
    In this presentation, we will introduce various reduced order modeling techniques,
    which aim to lower the computational complexity and maintain a good accuracy,
    including projection-based intrusive nonlinear model reduction and non-intrusive
    model reduction approaches. We will demonstrate the implementation of these reduced
    order modeling techniques in libROM (www.librom.net) and its application to numerical
    solvers for solving various physics problems.
- poster: 139979
  presenter: "Chi, Eric"
  email: "91a8eaea87d4732c"
  title: "MS21:Mcmc Importance Sampling Via Moreau-Yosida Envelopes"
  abstract: |-
    Markov chain Monte Carlo (MCMC) is the workhorse computational algorithm
    employed for inference in Bayesian statistics. Existing approaches require the
    gradient of the log posterior. In modern parsimonious models, the use of non-differentiable
    priors is fairly standard, yielding non-differentiable posteriors.  Without differentiability,
    gradient-based MCMC algorithms cannot be employed effectively. Recently proposed
    proximal MCMC approaches, however, can partially remedy this limitation. These
    approaches employ the Moreau-Yosida (MY) envelope to smooth the nondifferentiable
    prior enabling sampling from an approximation to the target posterior. In this
    work, we leverage properties of the MY envelope to construct an importance sampling
    paradigm to correct for this approximation error. Specifically, we show that the
    importance weights are uniformly bounded irrespective of the choice of the smoothing
    parameter. Leveraging this result, we establish asymptotic normality of the importance
    sampling estimators of posterior expectations. Our conditions for this result
    only rely on finiteness of moments under the desired target distribution.
- poster: 140317
  presenter: "Choi, Bosu"
  email: "f93058727279d627"
  title: |-
    MS24: Sampling Error Mitigation Through Spectrum Smoothing in Ensemble Data
    Assimilation
  abstract: |-
    In data assimilation, an ensemble provides a nonintrusive way to evolve
    a probability density described by a nonlinear prediction model. Although a large
    ensemble size is required for statistical accuracy, the ensemble size is typically
    limited to a small number due to the computational cost of running the prediction
    model, which leads to a sampling error. Several methods, such as localization,
    exist to mitigate the sampling error, often requiring problem-dependent fine-tuning
    and design. This work introduces another sampling error mitigation method using
    a smoothness constraint in the Fourier space. In particular, this work smoothes
    out the spectrum of the system to increase the stability and accuracy even under
    a small ensemble size. The efficacy of the new idea is validated through a suite
    of stringent test problems, including Lorenz 96 and Kuramoto-Sivashinsky turbulence
    models. This is joint work with Yoonsang Lee at Dartmouth College.
- poster: 140209
  presenter: "Chowdhary, Abhijit"
  email: "f3ca4dee9ccb58da"
  title: |-
    Techniques for the Robust Experimental Design of Nonlinear Bayesian Inverse
    Problems
  abstract: |-
    The optimal design of experiments for Bayesian inverse problems is a field
    in the uncertainty quantification community that has attracted wide attention.
    An optimal design is one which maximizes a predefined metric of quality on the
    resulting inverse problem. However, such an optimal design is local to the selection
    of the other hyper-parameters in the inverse problem, such as measurement uncertainty.
    Robust optimal experimental design is a recently introduced reformulation of the
    classical problem which considers a worst-case scenario approach regarding the
    uncertain parameters. However, such formulations were presented for linear(-ized)
    Bayesian inverse problems, in order to obtain tractable utility criteria evaluations
    and gradients. In this work, we present a framework tailored to infinite-dimensional
    Bayesian inverse problems constrained by large-scale partial differential equations
    for the efficient and scalable evaluation of an approximation to the expected
    information gain. In addition, we provide methods for its scalable differentiation
    leveraging eigenvalue sensitivity techniques. Together, along with an adaptation
    of state-of-the-art stochastic optimization techniques to the robust setting,
    we enable non-linear robust optimal experimental design. Extensive numerical experiments
    to validate the proposed approach are carried out for sensor placement in a classical
    nonlinear parameter identification problem.
- poster: 140316
  presenter: "Churchill, Victor"
  email: "7793fbd62ee401ee"
  title: "MS24: Details On: Priors for Efficient Three-Dimensional Imaging"
  abstract: |-
    The poster will present finer details of the talk "Priors for Efficient
    Three-Dimensional Imaging", as part of the mini-symposium "Non-intrusive Computational
    Methods to Incorporate Prior Knowledge for Improved Statistical Accuracy". In
    particular, the study explores appropriate priors for the challenging problem
    of 3D image reconstruction from indirect measurements. We apply the proposed techniques
    for synthetic aperture radar (SAR) imaging and change detection.
- poster: 140024
  presenter: "Cole, Frank"
  email: "709d61db0afe9789"
  title: |-
    MS60: Analysis of Score-Based Generative Models in Learning Distributions
    with Low Complexity
  abstract: ""
- poster: 140085
  presenter: "Comstock, Maxfield"
  email: "d7f92efd819e6921"
  title: |-
    Quantifying Chaos in Cardiac Action Potential Dynamics by Computing the Spatiotemporal
    Lyapunov Exponent
  abstract: |-
    Detecting chaotic dynamics in data taken from biological systems is a
    challenging task. One common technique for identifying chaos is to compute an
    estimate of the Lyapunov exponent of the system, but typical methods require measurements
    over a long period of time. In typical spatiotemporal recordings of cardiac voltage
    data, however, usually only comparatively short time windows are recorded. To
    identify chaos in such systems, we utilize the spatiotemporal Lyapunov exponent,
    which has previously been demonstrated to identify chaos in coupled maps of discrete
    dynamical systems. This method compensates for a lack of temporal data by identifying
    similar patterns across locations in space and measuring the stability of the
    evolution of the system from these points. This presentation will focus on how
    to adapt the spatiotemporal Lyapunov exponent to continuous data taken from cardiac
    tissue and will demonstrate the results of the method on synthetic and experimental
    cardiac data sets.
- poster: 140170
  presenter: "Connerty, Erik"
  email: "a66c199af9647974"
  title: |-
    MS25: Inferring Network Connectivity Using Reservoir Networks: A Comparative
    Analysis of Brain Connectivity Metrics for Eeg
  abstract: |-
    Electroencephalography (EEG) functional (undirected) and effective (directed)
    brain connectivity metrics have been used to study communication between brain
    regions in healthy and diseased conditions, including neurological disorders such
    as Alzheimer's disease, major depressive disorders, and epilepsy. These metrics
    capture important aspects of brain connectivity such as linear and nonlinear information
    flow. However, these different metrics do not converge and provide a significantly
    different picture of brain connectivity raising the pressing need of resolving
    this inconsistency and guiding neuroimaging researchers on what connectivity metric
    to choose. To address this challenge, we systematically compared functional and
    effective connectivity metrics. We examined two functional connectivity metrics
    (coherence (coh) and the corrected imaginary part of phase lagged value (ciPLV))
    and three established effective connectivity metrics (generalized partial directed
    coherence (gPDC), direct directed transfer function (dDTF), and pairwise spectral
    granger prediction (pSGP)), and a new effective connectivity metric based on modeling
    EEG time-series as a dynamical system (DynSys). We evaluated these metrics using
    experimental data from an open-access dataset comprising 112 EEG recordings of
    healthy adults resting with their open (EO) or eyes closed (EC).
- poster: 139665
  presenter: "Constante, Luis"
  email: "870bd275c28354da"
  title: |-
    Physics-Informed Machine Learning Algorithms As Pde Solvers in Fluid Flow
    Through Porous Media
  abstract: |-
    The so-called Physics-Informed machine learning (ML) algorithms, which
    can incorporate the mathematical formulation of the modeled physics phenomena
    in their training process, have shown outstanding growth in the last years. Two
    interesting applications are possible for them: building data-free solvers for
    partial differential equations (PDE) and identifying the mathematical model behind
    a data set. This kind of ML techniques were tested in many problems, including
    the flow of fluids through porous media, generally for limited and very simplified
    scenarios. To evaluate their true capacity, more practical problems must be addressed.
    In this sense, one of the most useful and practical models in oil and gas reservoir
    evaluation is expressed as an advection-diffusion PDE called hydraulic diffusivity
    equation. This model was used to evaluate the limitations of the recently introduced
    physics-informed ML algorithms. Clear limitations were identified after testing
    different types of boundary conditions, which, from the physics side, are analogous
    to different types of reservoirs. The identification of failure scenarios allowed
    to successfully establish some Galerkin and regularization-based approaches to
    overcome the intrinsic limitations of such methods.
- poster: 139897
  presenter: "Coskunuzer, Baris"
  email: "46ab909eab158e36"
  title: "MS82: Topological Machine Learning for Drug Discovery"
  abstract: |-
    In this work, we present applications of latest topological machine learning
    methods in virtual screening and computer aided drug discovery.
- poster: 139041
  presenter: "Cuerno, Manuel"
  email: "6c3bb82b70d9c969"
  title: "Reassessing Relationality for Bipolar Data"
  abstract: "Methods to cluster people across relationality, a concept developed to describe how similarly individuals respond to survey items measuring the intensity of agreement or disagreement towards specific question statements, have recently gained traction. These methods cluster people into “social affinity” groups, sharing similarities in their outlooks on social life. Relational Class Analysis (RCA) is currently the most commonly used method for relationality clustering. RCA has been applied to identify affinity groups in social spheres as varied as politics, musical preferences, and attitudes towards science. In this study, we highlight limitations in RCA’s ability to accurately identify the number and underlying structure of these groups. These limitations stem from RCA’s mathematical underpinnings and its insensitivity to the orientational quality of the survey items they use: they demand them to place themselves in a support or rejection zone and then express how intensely their support or rejection is. We develop a method, called Bipolar Class Analysis (BCA), that aims to address this foundational limitation. BCA conceptualizes people’s attitudinal positions as moving along agree/disagree subspaces and assesses relationality by taking into account position switches across these subspaces. We run extensive simulation analyses of data organized around different relationality structures to show that BCA is more accurate than RCA and other available alternatives."
- poster: 139564
  presenter: "Curato, Imma Valentina"
  email: "8a442eb6170cca0d"
  title: "Mixed Moving Average Field Guided Learning for Spatio-Temporal Data"
  abstract: |-
    Influenced mixed moving average fields are a versatile modeling class
    for spatio-temporal data. However, their predictive distribution is not generally
    known. Under this modeling assumption, we define a novel spatio-temporal embedding
    and a theory-guided machine learning approach that employs a generalized Bayesian
    algorithm to make ensemble forecasts. We employ Lipschitz predictors and determine
    fixed-time and any-time PAC Bayesian bounds in the batch learning setting. Performing
    causal forecast is a highlight of our methodology as its potential application
    to data with spatial and temporal short and long-range dependence. We then test
    the performance of our learning methodology by using linear predictors and data
    sets simulated from a spatio-temporal Ornstein-Uhlenbeck process.
- poster: 139793
  presenter: "Curtu, Rodica"
  email: "8266e59f609cc3e"
  title: |-
    MS36: Extracting Koopman-Based Features from Eeg Recordings of Brain Activity
    During Time-Based Decision Making
  abstract: |-
    Discovering dynamical patterns from high fidelity, seconds-to-minutes
    timeseries is typically a challenging task. Furthermore, the problem increases
    in complexity when the timeseries themselves only represent partial measurements
    of some intrinsic dynamics of an unknown underlying dynamical system. In this
    poster presentation, the timeseries data consist of scalp electroencephalography
    (EEG) neural recordings obtained from the midfrontal Cz-electrode of healthy participants
    that performed an interval-timing cognitive task. In the experiment, the participants
    estimated a 7-second time-period by pressing a button. We implement a data-driven
    method, which leverages time-delayed coordinates, diffusion maps, and dynamic
    mode decomposition, to identify neural features in the EEG that correlated with
    the subjects' reports. The method involves extracting Koopman eigenfunctions from
    the data and relating them to certain Kolmogorov backward eigenfunctions and to
    the trajectories of a hypothesized underlying stochastic differential equations
    system.
- poster: 139646
  presenter: "Dagdelen, Ebru"
  email: "c748b14c03174573"
  title: "Understanding Fluid Flux: Topological Data Analysis of Porous Media"
  abstract: |-
    This research project utilizes topological data analysis (TDA) to quantify
    the material properties of porous media samples. The main question driving this
    study is whether there are correlations between the fluid flux and the topological
    properties of the considered porous materials; and if so, to identify and quantify
    such correlations. Our goals and objectives include analyzing the experimental
    images (CT scans of porous material samples), computing their topological properties,
    and establishing correlations with the experimental results. For this purpose,
    we utilize methods based on persistence homology that allow us to quantify the
    porous material properties in a clear and precise manner. Our poster presentation
    will discuss the methodology, preliminary results, and the potential impact of
    this research on understanding fluid behavior in porous media.
- poster: 139840
  presenter: "Dahal, Biraj"
  email: "b09b333b502d35a7"
  title: |-
    MS41: On Deep Generative Models for Approximation and Estimation of Distributions
    on Manifolds
  abstract: ""
- poster: 139780
  presenter: "Daley, Kevin"
  email: "f102f4f0ec21e823"
  title: "Mapping and Control of High-Dimensional Bistabilities from Noisy Data"
  abstract: |-
    Multistability in high-dimensional complex systems such as partial differential
    equations and large multi-agent swarms poses a unique challenge to modeling and
    control in experiments.  Frequently, such systems' state spaces contain significant
    regions of dynamical uncertainty where, in the presence of small-amplitude observation
    noise, robust control schemes and avoidance of undesirable dynamical regimes are
    intractable.    We consider an example high-dimensional swarming system with delay-induced
    bistability between two rotational modes and an embedded saddle and introduce
    a novel, equation-free procedure for determining the location of the saddle, which
    we describe phenomenologically. Notably, our approach assumes only the ability
    to generate a dense sample of trajectories and classify their asymptotic behavior,
    allows for precise control of complex systems even close to uncertainty, and yields
    an accurate estimate of the minimum embedding dimension of the stable manifold
    of the saddle which determines the reducibility of the corresponding control problem.
- poster: 140177
  presenter: "Damodaran, Swaroop"
  email: "1c04d500272bd2b2"
  title: |-
    Image-Based Goal Conditioned Reinforcement Learning with a Gflownet Planner
    and Transformer Policy
  abstract: |-
    Most tasks faced by autonomous agents in practical settings are long-horizon
    tasks. The complex nature of long-horizon tasks arises from the delay in obtaining
    a corresponding reward for a long sequence of actions. Goal-conditioned reinforcement
    learning is an approach to tackle long-horizon tasks, which uses a planner followed
    by a policy to achieve the final goal. The key challenges in this approach are
    two-fold: finding the optimal sequence of sub-goals and finding a policy with
    enough generalizability to attain the final goal. Previous approaches with conventional
    policies require pre-training for each distinct long-horizon task. However, it
    is known that pre-trained transformers generalize well across different tasks,
    which would imply a few-shot and zero-shot learning approach for the policy. Thus,
    the proposed model explores the use of such a transformer-based policy, which
    can successfully generalize to new tasks without pre-training for each distinct
    long-horizon task. Additionally, a novel planner is proposed based on Generative
    Flow Networks which samples objects from a distribution in a sequence of constructive
    steps. This planner would sample the optimal sequence of sub-goals not by minimizing
    a loss function, but rather by sampling such that the generated sample probabilities
    are proportional to a reward function. Further, the use of large language models
    to add text descriptions to the corresponding image at each state is explored.
- poster: 140180
  presenter: "Dang, Thanh"
  email: "fbf497fb3905c825"
  title: "MS61: Euler-Maruyama Schemes for Stochastic Differential Equations Driven by Stable Lévy Processes with I.I.D. Stable Components"
  abstract: "The study of the Euler-Maruyama scheme of SDE has a long history in the probability and numerical analysis literature. In this work, we study Euler-Maruyama numerical schemes of stochastic differential equations driven by stable Lévy processes with i.i.d. stable components. Our motivation is based on recent applications of Lévy-driven SDE in the machine learning community to solve optimization and sampling problems. We obtain a uniform-in-time approximation error in Wasserstein distance. Our approximation error has a linear dependence on the stepsize, which is expected to be tight, as can be seen from an explicit calculation for the case of an Ornstein-Uhlenbeck process. We also obtain a uniform-in-time approximation error when Pareto noises are used to simulate stable Lévy noise  in the numerical scheme."
- poster: 139919
  presenter: "Darcy, Matthieu"
  email: "1dbb22c9a8d10bc5"
  title: "MS63: Kernel Methods for Operator Learning"
  abstract: |-
    We introduce a kernel-based framework for learning operators between
    Banach spaces. We show that even with simple kernels, our approach is competitive
    in terms of cost-accuracy trade-off and either matches or beats the performance
    of Neural Network methods on a majority of PDE-based benchmarks. Additionally,
    our framework offers several advantages inherited from kernel methods: simplicity,
    convergence guarantees, a priori error estimates, and Bayesian UQ. It is, therefore,
    a natural benchmark for operator learning problems.
- poster: 139619
  presenter: "Dassanayaka, Sachith"
  email: "325a2358719f9710"
  title: |-
    A Text-Based Classification to Escalate the Predictability of Fake Tweet
    Actors in Troll Networks
  abstract: |-
    Russian internet trolls utilize fake identities to distribute false information
    across various social media channels. Drawing from previous research on social
    media influence networks, we present a novel approach to map these operations.
    Utilizing Twitter data connected to the Russian influence network, we developed
    a predictive model to outline these network activities. We categorize accounts
    based on their authenticity role within a subset. This categorization involves
    defining logical groupings and training a model to recognize similar behavioral
    patterns across the network. Since tweets and hashtags form Twitter's core, we
    analyze these key elements to comprehend the authenticity function of diverse
    actors and behaviors. We utilize Natural Language Processing techniques to group
    text data and match tweets to logical categories. Comparing the categories with
    their respective clusters validates our approach. This comparison demonstrates
    that text clustering accuracy can exceed 90%, enhancing the predictive model's
    performance. The prediction and validation results indicate our model can map
    actors within such networks. Moreover, visualizing and analyzing Twitter activities
    and patterns aids in understanding Russian troll behaviors. Due to the complexity
    of activity data, we employ dimensional reduction techniques. This allows us to
    visualize Twitter actions and analyze patterns, uncovering intriguing relationships
    and behaviors.
- poster: 138998
  presenter: "David, Darlington S."
  email: "545059a01afee98e"
  title: |-
    Predictive Modeling of Complex Dynamical Systems Using Machine Learning:
    A Comparative Analysis
  abstract: |-
    Dynamical systems exhibit intricate behaviors that challenge traditional
    modeling techniques. The integration of machine learning methods provides a promising
    avenue for understanding and predicting the evolution of such complex systems.
    This research endeavors to conduct a comprehensive comparative analysis of various
    machine learning approaches employed in predictive modeling for complex dynamical
    systems. The study will delve into the performance of both traditional time-series
    models and state-of-the-art deep learning techniques in capturing the intricate
    dynamics of diverse systems. We will consider applications across different domains,
    ranging from financial markets and climate science to biological systems. The
    primary objectives include evaluating the accuracy, efficiency, and robustness
    of these models in capturing the underlying dynamics of complex systems. The comparative
    analysis will encompass a meticulous examination of predictive capabilities under
    varying conditions and datasets. We aim to elucidate the strengths and weaknesses
    of each approach, shedding light on their suitability for specific types of dynamical
    systems. The findings of this research will contribute valuable insights into
    the selection of optimal modeling techniques, aiding researchers and practitioners
    in making informed decisions when confronted with complex dynamical systems.
- poster: 139930
  presenter: "Deighton, Jared"
  email: "453f2c6ca149fe7"
  title: |-
    MS79: A Multi-Cellular Information-Theoretic Approach to Place Cell Population
    Learning
  abstract: |-
    How the brain encodes its environment is a much-studied question with
    significant scientific and engineering implications. Place cells and grid cells,
    which fire at specific regions in an environment, enable mammals to distinguish
    locations and track positions. While the efficiency of grid cell coding has been
    extensively studied, the computational role of place cells is less well understood.
    This gap is partially due to the limited measure of spatial information, which,
    until now, has only been defined for one place cell. We derive and implement a
    higher-order spatial information measure, allowing for the study of the emergence
    of multiple place cells in a self-supervised manner. We show that emergent place
    cells have many desirable features, including high-accuracy spatial decoding.
    To our knowledge, this is the first work in which higher-order spatial information
    measures that depend solely on place cells' firing rates have been derived and
    which focuses on the emergence of multiple place cells via self-supervised learning.
    By enabling the quantification of spatial information for multiple place cells,
    we can now rigorously formulate hypotheses about place cell formation, function,
    and capabilities.
- poster: 139839
  presenter: "Delshad, Afrouz"
  email: "f79c92d08dc08d8b"
  title: |-
    Efficacy of Hybrid Echo State Networks in Forecasting Cardiac Electrophysiological
    Signals
  abstract: |-
    Understanding cardiac electrophysiological signaling is key to discovering,
    preventing, and treating cardiac arrhythmias. Much of the modeling research in
    this area has utilized mechanistic mathematical descriptions of the cardiac action
    potential to predict the behavior of the electrical potential of cardiac cells.
    Recently, machine learning, including neural networks, has provided a different
    approach for this problem. Echo State Networks, a data-driven machine learning
    technique, show promise for learning from past input signals to predict future
    ones. An Echo State Network is a type of Recurrent Neural Network with sparsely
    connected neurons, where only the output layer weights are subject to adjustment,
    and other parameters remain untrained. The special structure of Echo State Networks
    provides them with high memory capacity and the ability to capture the features
    of input signals properly. The hybrid approach of introducing a traditional knowledge-based
    model to the data-driven Echo State Network may outperform strategies that rely
    only on mathematical models or the reservoir approach. In this poster, we explore
    the performance of the Hybrid Echo State Network when applied to synthetic and
    real-world cardiac data.
- poster: 139977
  presenter: "Deng, Joshua"
  email: "78d452402ea45d3"
  title: |-
    Neural Operator Simulation of Turbulent Multiphase Flow with Applications
    in Ocean Wave Modeling
  abstract: |-
    Global warming has increased wave heights and the frequency of extreme
    wave events. As a result, ocean waves have been contributing to more coastal erosion,
    flooding, and fatigue damage of ocean-based structures. Researchers interested
    in modeling the effects of ocean waves usually rely on slow iterative methods
    such as particle-in-cell (PIC). Therefore, researchers have good occasion to find
    a faster alternative for modeling ocean waves. To this end, we have created a
    Neural Operator (NO) that can predict ocean wave behavior on grids with random
    obstructions. To create data to train our NO, we used the particle-in-cell method
    to create 290 examples of an ocean wave crashing against a random obstruction.
    Each simulation modeled 38,000 water particles for 600 time steps, which required
    30 minutes. From these simulations, we created training and testing datasets of
    274 and 16 image pairs, respectively, where the input image is the grid with the
    random obstruction, and the output image is the shape of the ocean wave. After
    training the NO, we validated it against our 16 test points. At a resolution of
    256 x 256, our NO had an inference time of only 0.073 second. The NO results were
    quite accurate, especially given that only 274 training points were used. Our
    approach can be expanded to create fast 3D time-varying simulations. These NOs
    can be used to predict levee overtopping, flooding, blast waves, and even the
    effect of water drops over wildfires.
- poster: 140282
  presenter: "Depaul, Greg"
  email: "3101ff01437987bf"
  title: |-
    MS91: Leveraging Derived Statistics from Differential Geometry for Understanding
    High-Dimensional Tda
  abstract: |-
    We build a novel filtered complex, utilizing density estimation and classical
    linear fitting methods, from multidimensional Gaussian balls that serve as an
    approximate statistic for the local tangent space. This construction leverages
    density estimation, which is often more robust against outliers than traditional
    TDA methods. We apply this approach to large point-cloud datasets in the hopes
    of more faithful, humanly-intelligible visualizations.
- poster: 140214
  presenter: "Dereventsov, Anton"
  email: "b42bf55a72a58246"
  title: |-
    MS48: Exploring Policy Entropy in Reinforcement Learning for Personalization
    Environments
  abstract: |-
    This talk delves into the intricate behavior of reinforcement learning
    systems operating within personalized environments, shedding light on the distinct
    policy entropy characteristics shaped by different learning algorithms. We elucidate
    that Policy Optimization agents frequently exhibit low-entropy policies during
    training, leading to a tendency to favor certain actions while neglecting others.
    In contrast, Q-Learning agents demonstrate a remarkable resilience to such behavior,
    maintaining consistently high-entropy policies throughout training, a trait highly
    desirable in practical applications. Through a comprehensive array of numerical
    experiments complemented by theoretical insights, we unveil the underlying reasons
    behind these entropy disparities attributed to the choice of learning algorithms.
    Our findings not only provide empirical evidence but also offer theoretical underpinnings,
    enriching our understanding of how learning algorithms influence policy entropy
    dynamics in reinforcement learning contexts. This presentation offers valuable
    insights for practitioners seeking to optimize policy entropy for enhanced adaptability
    and performance in personalized environments.
- poster: 139011
  presenter: "Deutsch, Shay"
  email: "14d460e1f6909a88"
  title: "MS4: Ms-Umap - Multi-Scale Manifold Learning Through Probabilistic Sampling"
  abstract: |-
    Deriving meaningful representations from complex, high-dimensional data
    in unsupervised settings is crucial across diverse machine learning applications.
    A prevalent strategy involves nonlinear dimensionality reduction utilizing manifold
    learning and graph embedding techniques.     This study introduces a graph network
    statistical framework that leverages a diffusion-based approach for sampling network
    features. Customized for enhancing and guiding the optimization process through
    stochastic gradient descent (SGD), our framework aims to achieve a more precise
    representation of the inherent network structure.     Our approach introduces
    a spectrum of sampling techniques, specifically targeting the selection of edges
    or nodes based on their perceived significance within the underlying graph structure.
    These techniques encompass the computation of node importance distributions, derived
    from graph topology measurements like edge betweenness centrality or diffusion
    wavelets, and subsequent sampling from these distributions.      Incorporating
    multi-scale graph representation into these sampling techniques during graph embedding
    optimization results in a robust and expressive representation. We validate the
    effectiveness of our proposed graph embedding through a range of diverse downstream
    tasks, including clustering.
- poster: 139860
  presenter: "Devarakonda, Aditya"
  email: "d77f76318e9b64db"
  title: "MS84: S-Step SGD and Federated SGD for Large Scale Optimization"
  abstract: |-
    We develop a hybrid, 2D parallel distributed-memory SGD algorithm by combining
    ideas from $s$-step methods and federated learning. The proposed algorithm attains
    a continuous tradeoff between accuracy and performance, while scaling beyond what
    is currently enabled by $s$-step SGD and federated SGD alone. We show empirical
    results that highlight better convergence and better performance when combining
    the $s$-step and federated SGD approaches into a 2D parallel algorithm. We also
    show theoretical parallel cost analyses which highlight the benefits of the hybrid,
    2D SGD algorithm.
- poster: 139947
  presenter: "Dhakal, Uttam"
  email: "e4e9840f326deecf"
  title: "MS74: Evaluating the Efficacy of LLMs: Applications and Challenges"
  abstract: |-
    Large Language Models (LLMs) are at the forefront of advancements in artificial
    intelligence, demonstrating remarkable capabilities in a variety of natural language
    processing tasks. These models, characterized by their vast number of parameters
    and deep neural networks, excel in understanding and generating human-like text,
    making them invaluable for applications ranging from automated writing assistants
    to sophisticated conversational agents. As LLMs continue to evolve, their application
    has expanded beyond text generation to include roles in decision support systems,
    educational tools, and entertainment, showcasing their versatility and potential.
    However, the rapid development of LLMs also brings challenges, particularly in
    terms of scalability, ethical concerns, and the risk of perpetuating biases present
    in their training data. The research community is actively exploring evaluation
    frameworks that not only measure the linguistic abilities of these models but
    also their fairness, transparency, and impact on privacy and security. As technology
    advances, the integration of LLMs with other AI technologies opens up new possibilities
    for synergistic applications, promising to enhance the effectiveness of AI systems.
    Thus, the ongoing research and development in LLMs are not just about reaching
    new heights in language understanding and generation but also about ensuring that
    these technologies are developed responsibly, aligning with broader goals and
    ethical standards.
- poster: 139949
  presenter: "Dhakal, Chandra"
  email: "ce7645976a64324d"
  title: |-
    MS44: Association Between Socioeconomic Status and Cardiovascular Disease
    in the U.S.: A Deep Learning Approach
  abstract: |-
    This study investigated the relationship between socioeconomic status
    (SES) and cardiovascular disease (CVD) prevalence in the United States from 2015-2022.
    Previous research has linked lower SES to higher CVD risk, and machine learning
    techniques have the potential to uncover novel patterns in this association over
    the 8-year period. We applied advanced machine learning and deep learning models
    to data from the Behavioral Risk Factor Surveillance System (BRFSS).  The dataset
    comprised 3,485,529 observations with demographic, socioeconomic, health behavior,
    and CVD outcome variables. SES indicators included education level, income, employment
    status, and neighborhood characteristics. Deep neural networks, gradient-boosted
    trees, and other machine learning models were trained to predict CVD outcomes
    based on SES and additional variables.   The most important SES predictors were
    education, income, and neighborhood wealth index. After controlling for other
    factors, the predicted CVD risk was 75% higher for individuals with less than
    a high school education compared to college graduates. Risk steadily decreased
    with higher income brackets. This large longitudinal study demonstrates a strong
    negative relationship between SES and CVD risk. Machine learning methods applied
    to multi-year survey data revealed potent socioeconomic disparities predictive
    of cardiovascular outcomes. These disparities persist in the U.S. despite a decline
    in overall CVD rates over the past decade.
- poster: 139909
  presenter: "Dhibar, Sibasish"
  email: "56c737c0728dd8c5"
  title: |-
    D3censent: A New Cnn Ensemble Network for White Blood Cell Classification
    with Lime Explainable
  abstract: |-
    White blood cells (WBC) are crucial components of our immune system, defending
    our body against infections by destroying viruses, bacteria, parasites, and fungi.
    Understanding the variety and total count of WBCs can tell us a lot about our
    health. Convolutional neural networks (CNN), a type of deep learning, are used
    to identify and recognize blood cells by examining parts of an object. Various
    CNN models exhibit potential; however, their development often involves ad-hoc
    processes that neglect unnecessary layers, leading to issues with unbalanced datasets
    and insufficient data augmentation. To tackle these challenges, we suggest an
    innovative ensemble strategy that incorporates three CNN architectures, each customized
    with distinct dropout and maxpooling layers configurations to improve feature-level
    learning. As a result, our proposed ensemble network that is called D3CENSENT
    achieves an optimal bias-variance trade-off. Upon evaluation using the well-known
    Rabbin-WBC dataset, our model surpasses existing state-of-the-art networks by
    achieving a mean accuracy of 98.53%. It also excels in precision, recall, F1 score,
    and Area Under the ROC Curve (AUC) across each category. To delve deeper into
    the interpretability of classifiers, we employ reliable post-hoc explanation techniques,
    including Local Interpretable Model-Agnostic Explanations (LIME). These methods
    simplify understanding a black-box model by explaining how feature values are
    linked to predictions.
- poster: 140121
  presenter: "Diagne, Khady"
  email: "f2cf410d9d71f10d"
  title: |-
    MS34: Using Mathematical Models to Identify the Mechanism of Premature Ventricular
    Complexes in Cardiac Arrhythmias
  abstract: |-
    Premature ventricular complexes (PVCs) are early heartbeats originating
    in the ventricles of the heart, instead of the right atrium. At high frequency,
    PVCs can impair cardiac function and increase the risk for more serious cardiac
    arrhythmia. By understanding the mechanism of PVCs, we can improve their treatment
    by targeting the problem instead of the symptoms. In parasystole, an ectopic pacemaker
    in the ventricles is responsible for these premature beats. We aim to identify
    parasystolic mechanisms of premature ventricular complexes from electrocardiograms
    and determine if they lead to more serious arrhythmias. We built an experimental
    model of a periodically entrained oscillator using a beating heart cell aggregate
    coupled with a non-beating aggregate stimulated with blue light. From these experiments,
    we identified parasystolic rhythms and the phase resetting curve of heart cells
    which we integrate into computational models. With our physiologically-derived
    models, we evaluate the bifurcation structure of parasystolic dynamics as the
    stimulus frequency is varied and observe a variety of complex rhythms and bifurcations.
    The experimental results and mathematical analysis were used to classify PVC dynamics
    of patients with frequent PVCs using machine learning methods. With these models
    and classifications, we aim to improve the identification of PVC mechanisms, which
    may provide early warning for progression onto more serious cardiac arrhythmias.
- poster: 140253
  presenter: "Diaz, Hugo"
  email: "d58fc9560eabe4f8"
  title: "Bayesian D-Optimal Experimental Design for Weak-Constraint 4D-Var"
  abstract: |-
    In data assimilation, the weak-constraint approach provides a method to
    incorporate model errors in the dynamics of the governing equations. We propose
    a new framework for optimal sensor placement in the weak-constrained setting,
    by first deriving the D-optimal criterion assuming that the model error is Gaussian
    and the dynamics are linear. We discuss algorithmic approaches to efficiently
    evaluate this criterion and provide an algorithm to find near-optimal experimental
    designs using column subset selection. We show the effectiveness of our approach
    and the computational benefits of our algorithms on model problems.
- poster: 139575
  presenter: "Diaz-Viera, Martin"
  email: "1d25cebd4f8933c6"
  title: |-
    Machine Learning Vs Copula-Based Quantile Regression Methods for Prediction
    of Petrophysical Properties with Seismic Attributes: A Comparative Study
  abstract: |-
    In recent years, machine learning methods have become popular to solve
    many data-driven problems in earth sciences. But this does not necessarily imply
    that they are always the only and the best option from the point of view of precision
    and efficiency.  This work presents a comparison of several standard machine learning
    methods with respect to the copula-based quantile regression method for the prediction
    of petrophysical properties from seismic attributes in the framework of geological-petrophysical
    modeling of oil fields.  From the methodological point of view, a systematic methodology
    of Data Science is applied, showing each stage of the application of the methods,
    which includes the statistical analysis of the data, the modeling of the joint
    dependency relationship, training, validation and prediction.  The results of
    petrophysical property predictions from both approaches are compared with reference
    data at the well log scale. Finally, both approaches are applied to a real case
    study both at the well log scale and at the seismic scale in a 2D inline section.
- poster: 140057
  presenter: "Dmitriev, Daniil"
  email: "b8d91f0d51f397df"
  title: "Lower Bounds for Private Online Learning"
  abstract: |-
    We provide lower bounds for Differentially Private (DP) Online Learning
    algorithms. Our result shows that, for a broad class of $(\varepsilon,\delta)$-DP
    online algorithms, for number of steps $T$ such that $\log T\leq O(1/\delta)$,
    the expected number of mistakes incurred by the algorithm grows as $\Omega(\log
    T / \delta)$. This matches the upper bound obtained by Golowich and Livni (2021)
    and is in contrast to non-private online learning where the number of mistakes
    is independent of $T$. Our work partially addresses the open question in Sanyal
    and Ramponi (2022).
- poster: 139035
  presenter: "Doddaiah, Ramesh"
  email: "d9637b638f206bd3"
  title: "Learning Salient Semantic Subsequences to Explain Deep Time Series Classifiers"
  abstract: |-
    Explainable AI helps end users trust deep learning solutions for time
    series classification. However, existing explainability methods for time series
    classifiers focus on explaining individual time points to generate a local view
    or they generate shapelets based explanations to obtain a global view. Time-points
    based explanations are too hard for end users to understand without a higher-level
    abstraction.  None of these methods produce semantically meaningful explanations.
    Semantic based methods produce explanations using standard time series characteristics
    such as burst, pulses, trends, and seasonality. End users can easily understand
    the explanation in terms of time series characteristics. In this paper, we formalize
    the notion of semantic based explanations, studying the open problem of semantic-based
    explainability for deep time series classifiers, a challenging, and meaningful
    problem setting. We design a novel explainability method, SMUX, which learns semantically
    meaningful subsequences for explaining deep time series classifiers by adaptively
    ensuring that its explanation spotlights the regions in an input time series that
    a model uses specifically to its predicted class. SMUX adopts a gradient-based
    approach composed of three interdependent modules that combine to generate consistent,
    class-specific semantic subsequences that remain faithful to the classifier's
    behavior yet are intuitively understood by end users.
- poster: 140207
  presenter: "Dogan, Gunay"
  email: "df6b7437b15e7d21"
  title: "Scikit-Shape: Python Toolbox for Shape Analysis and Segmentation"
  abstract: |-
    Many tasks in image processing, e.g. segmentation, surface reconstruction,
    are naturally expressed as energy minimization problems, in which the free variables
    are shapes, curves in 2d or surfaces in 3d. We typically express such problems
    as energies with data (or target) mismatch and geometric regularization components,
    to be minimized algorithmically to attain the optimal shape. To solve such problems,
    we have implemented a suite comprising various building blocks of such problems
    and algorithms to perform the minimization, including geometric regularization,
    statistical shape priors, adaptive geometric discretization, and fast Newton-type
    minimization schemes. Moreover, we have developed crucial shape analysis algorithms
    for statistical analysis and evaluation of the shapes computed, based on elastic
    shape distance framework. Our main applications are image and data analysis problems,
    but the infrastructure is quite general, and can be used for problems in other
    fields as well. All our algorithms are implemented in Python, leveraging on the
    NumPy/SciPy ecosystem, making them as easy to use as Matlab, also compatible with
    existing Python tools. Our algorithms is freely available as an open source package
    for the research community at: http://scikit-shape.org
- poster: 140204
  presenter: "Dogan, Gunay"
  email: "df6b7437b15e7d21"
  title: "Vemos: A Gui for Evaluation of Similarity Metrics on Complex Data Sets"
  abstract: "Similarity and dissimilarity metrics are a fundamental component of many tasks requiring the analysis and comparison of complex, often visual data. Applications ranging from computer vision to forensics require ways to effectively identify images, find clusters or outliers in data sets, or retrieve data items similar to a query item. However, finding an effective metric for a specific task is challenging due to the complexity of modern data sets and the myriad of possible similarity metrics arising from that complexity. We present VEMOS, a Python package that provides an accessible graphical user interface (GUI) for the evaluation of such comparison metrics. VEMOS provides user-friendly ways to examine individual data items or groups in a data set alongside analyses of metrics’ performance on the whole data set, such as clustering, multi-dimensional scaling, and retrieval performance analyses. VEMOS aims to help researchers and practitioners evaluate multiple comparison metrics (of similarity or dissimilarity) on rich, diverse data sets."
- poster: 140288
  presenter: "Dong, Yijun"
  email: "307dc3e804d31789"
  title: "MS66: Robust Blockwise Random Pivoting"
  abstract: |-
    The interpolative decomposition (ID) aims to construct a low-rank approximation
    formed by a basis consisting of row/column skeletons in the original matrix and
    a corresponding interpolation matrix. This work explores fast and accurate ID
    algorithms from five essential perspectives for empirical performance: (a) skeleton
    complexity that measures the minimum possible ID rank for a given low-rank approximation
    error, (b) asymptotic complexity in FLOPs, (c) parallelizability of the computational
    bottleneck as matrix-matrix multiplications, (d) error-revealing property that
    enables automatic rank detection for given error tolerances without prior knowledge
    of target ranks, (e) ID-revealing property that ensures efficient construction
    of the optimal interpolation matrix after selecting the skeletons. While a broad
    spectrum of algorithms have been developed to optimize parts of the aforementioned
    perspectives, practical ID algorithms proficient in all perspectives remain absent.
    To fill in the gap, we introduce robust blockwise random pivoting (RBRP) that
    is parallelizable, error-revealing, and exact-ID-revealing, with comparable skeleton
    and asymptotic complexities to the best existing ID algorithms in practice. Through
    extensive numerical experiments on various synthetic and natural datasets, we
    empirically demonstrate the appealing performance of RBRP from the five perspectives
    above, as well as the robustness of RBRP to adversarial inputs.
- poster: 139978
  presenter: "Ducellier, Ariane"
  email: "d0b3eb09b7442468"
  title: "Domain-Specific Raking with Uncertainty Propagation and Quantification"
  abstract: |-
    Raking is widely used in survey inference to adjust the sampling weights
    of the cases in a contingency table in order to match the marginal totals of the
    adjusted weights to the corresponding totals for the population. We propose a
    statistical approach, and a corresponding optimization algorithm, that is able
    to ensure that the entries after raking stay in a predefined interval of values
    based on domain knowledge. We also investigate several extensions of the method,
    including uncertainty quantification for posterior (raked) estimates. We use the
    implicit function theorem to create a generalized delta method that rigorously
    propagates the variance of the initial data and of the marginal totals through
    the constrained optimization problem to obtain the variance of the raked estimates.
    The proposed approach is applied to a multidimensional raking example where cause-specific
    mortality estimates across subgroups are raked to (uncertain) observed values
    of all-cause mortality for the total population.
- poster: 140141
  presenter: "Dunlavy, Danny"
  email: "4e0de1247137adfe"
  title: |-
    Constrained Tensor Decompositions for Low-Rank Modeling of Multiphysics Simulation
    Data
  abstract: |-
    High-dimensional data is ubiquitous across scientific computing disciplines,
    including plasma physics, fluids, earth systems, and mechanics.  Such data is
    naturally represented as a tensor, consisting of the value of each simulation
    variable at each point in space and time.  Tensor decomposition methods, akin
    to matrix factorization methods for two-dimensional data, facilitate powerful
    analysis/reduction of such data, including data compression, surrogate modeling,
    pattern identification, and anomaly detection.  However, existing tensor decomposition
    methods target simple statistical error metrics, most commonly least-squares loss,
    resulting in low-rank models of the data that fail to faithfully represent important
    physics quantities of interest or invariants arising from conservation principles.  In
    this work, we explore new formulations of two common tensor decomposition methods,
    the Canonical Polyadic (CP) and Tucker decompositions, that attempt to better
    preserve these quantities by incorporating them directly in the optimization problems
    that define the resulting low-rank models.  We then explore solving these optimization
    problems and investigate their ability to preserve these quantities compared to
    their overall reconstruction accuracy.  Computational results of applying this
    approach to CP and Tucker decomposition of data arising from simulation of plasma
    physics and combustion will be presented.
- poster: 140230
  presenter: "Eamaz, Arian"
  email: "7ca0c2973b49fe0a"
  title: |-
    Beyond the Shadow of a Doubt: Certifying Dnn Training Performance with Real-Time
    Data-Aware Guarantees
  abstract: |-
    Non-convex optimization is the new frontier in machine learning, particularly
    in the application of deep neural networks (DNNs). Due to the difficulties encountered
    in finding the global optima of non-convex problems, local optimization algorithms
    have been a key component of the non-convex literature. Recently, there has been
    a spurt of interest in non-convex optimization methods that can guarantee the
    quality of their approximate solutions. When it comes to guaranteed non-convex
    optimization, one may look for certain conditions in which a non-convex problem
    can be solved optimally in polynomial-time. However, a more useful approach in
    the context of deep learning is to establish data-aware guarantees for a rather
    large class of problem instances. Such data-aware guarantees can contribute to
    machine learning on two different levels: i) guarantees for optimal training and
    ii) guarantees for the effectiveness of DNN models. Within this framework, in
    this paper, we will explore the first guarantee type, namely developing the data-aware
    training guarantees for DNNs. This methodology not only allows us to evaluate
    the efficiency of training in relation to the highest potential training outcomes
    dictated by the data but also offers a mechanism to certify the proficiency of
    the training process.
- poster: 140225
  presenter: "Eamaz, Arian"
  email: "7ca0c2973b49fe0a"
  title: |-
    Unrolled 1-Bit Foundation Inference Models: Training, Effectiveness, and
    the Precision-Layer-Complexity Trade-Off
  abstract: |-
    Unrolled deep neural networks have attracted significant attention for
    their success in various practical applications. We investigate the impact of
    one-bit quantization on the performance of unrolling techniques when optimizing
    regularized objective functions, where every weight of the unrolled network is
    confined to a binary scheme $\{-1, 1\}$. It is demonstrated that by employing
    one-bit quantization during training, the model can match the performance of the
    full-precision network. Our approach allows for links to assume a zero value in
    addition to $\{-1, 1\}$, achieving an effective resolution of approximately 1.58
    bits per link, simultaneously allowing for sparse formation of networks, and requiring
    near-zero multiplication operations, thereby achieving low energy consumption,
    and superior throughput. We adopt a principled approach to robustness and interpretability
    of the forward inference model by the algorithm unrolling methodology, which relies
    on unrolling well-established iterative methods into the layers of a neural network
    and then learning an enhanced algorithm by leveraging available data. We show
    that this approach produces speed-ups of at least 100x to 1000x when compared
    to legacy inference algorithms, is more easily understood in connection to associated
    physical and naturally creates sparse forward networks. Given the sparse nature
    of our network, the bit per link rate approaches 1 instead of the 1.58 bits the
    LLM works.
- poster: 139910
  presenter: "Ebrahimi, Behrang"
  email: "57197a310cea81c2"
  title: "A Graph-Based Feature Selection Algorithm"
  abstract: |-
    We propose a feature selection algorithm specifically designed for datasets
    with high dimensions and low number of samples. It makes use of autoencoder neural
    networks and weighted graphs for dimension reduction and data importance analysis
    respectively. The algorithm starts by extracting a low-dimensional representation
    for the input data. The resulting dimensions give us an inexact estimation of
    the number of features to be selected. To select the most important features,
    we evaluate the contribution rate of each feature based on its weight in the hidden
    layer of the autoencoder. By analyzing these weights, we can identify the optimal
    combination of features for creating this low-dimensional representation. For
    each node in the hidden layer (encoder), we determine the input feature with the
    highest weight and add it to a candidate set while removing it from the input
    set. Finally, we obtain a set of candidate features that are most relevant to
    achieve the desired low-dimensional representation. In addition, the neural network
    encoder layer acts as a feature extraction mechanism and enables us to obtain
    valuable representations from the input data. According to our tests, this algorithm
    provides comprehensive results for feature selection and extraction in high-dimensional
    datasets with limited sample size. It uses autoencoder neural networks to extract
    a low-dimensional representation and identify the most influential features by
    optimally update the weights.
- poster: 140305
  presenter: "Eikenberry, Keenan"
  email: "cecbb424b9616211"
  title: "MS24: Maximizing Wasserstein Dependence in Autoencoders"
  abstract: |-
    Wasserstein dependence is an optimal transport-based analog of mutual
    information that has been gaining traction in both statistical inference and representation
    learning research. We show that (sliced) Wasserstein dependence maximization serves
    as an effective and computationally efficient training objective for autoencoders,
    significantly reorganizing the latent space as compared to a Gaussian prior. The
    proposed method additionally admits a theoretically rigorous Bayesian interpretation
    that helps clarify the extent to which statistics computed in the latent space
    can be decoded into accurate statistical estimates in the original, high-dimensional
    data space.
- poster: 140165
  presenter: "Eldred, Christopher"
  email: "c79e55b87e07b3e9"
  title: "MS7: HdR Roms: Hodge-DeRham Reduced Order Models"
  abstract: |-
    The Hodge deRham complex and it's compatible discretization is a fundamental
    tool in the development of structure-preserving numerical methods, that are widely
    used in full order models. For example, it underlies the stability of mixed FEM
    methods for elliptic problems. However, existing methods for reduced order models
    (ROM) methods, such as proper orthogonal decomposition (POD), are not able to
    obtain a discrete version of the deRham complex. In this poster we will discuss
    the development of a POD-based ROM with a discrete version of the deRham complex.
    This is based on exploiting the discrete Hodge decomposition of the underlying
    FOM discretization to obtain a reduced model in terms of exact, coexact and harmonic
    components. We will present results for HdR ROM applied to a variety of elliptic
    problems, and illustrate it's advantages compared to standard POD ROM approaches.
- poster: 139086
  presenter: "Eliasof, Moshe"
  email: "3caaeb15aa988804"
  title: "Weather Forecast by Graph Neural Networks"
  abstract: ""
- poster: 140554
  presenter: "Eliasof, Moshe"
  email: "3caaeb15aa988804"
  title: "MS18: Feature Transportation Improves Graph Neural Networks"
  abstract: |-
    Graph neural networks (GNNs) have shown remarkable success in learning
    representations for graph-structured data. However, GNNs still face challenges
    in modeling complex phenomena that involve feature transportation. In this paper,
    we propose a novel GNN architecture inspired by Advection-Diffusion-Reaction systems,
    called ADR-GNN. Advection models feature transportation, while diffusion captures
    the local smoothing of features, and reaction represents the non-linear transformation
    between feature channels. We provide an analysis of the qualitative behavior of
    ADR-GNN, that shows the benefit of combining advection, diffusion, and reaction.
    To demonstrate its efficacy, we evaluate ADR-GNN on real-world node classification
    and spatio-temporal datasets, and show that it improves or offers competitive
    performance compared to state-of-the-art networks.
- poster: 140040
  presenter: "Engman, Christian"
  email: "6072df02ec3f6a6b"
  title: "MS9: Sparse Recovery of Poincare-Steklov Operators from Matrix-Vector Products"
  abstract: |-
    Poincare-Steklov operators arise from elliptic partial differential equations,
    representing transformations between different types of boundary conditions. Poincare-Steklov
    operators and their discretizations are similar in structure to integral and differential
    operators, and the Neumann-to-Dirichlet map in particular resembles an elliptic
    solution operator. Previous work by Schaefer and Owhadi has shown that the Cholesky
    factors of elliptic solvers in $d$ spatial dimensions can be approximated to accuracy
    $\varepsilon$ in $O(\log(N) \log^d(N/\varepsilon))$ matrix vector products. In
    this work, we give an algorithm that aims to approximate the Cholesky factor of
    a Neumann-to-Dirichlet operator to accuracy $\varepsilon$ in $O (\log(N) \log^{d-1}(N/\varepsilon))$
    matrix-vector products with the operator, accessed only as a black box using carefully
    chosen right-hand sides. This cost-accuracy tradeoff is taken from Schaefer and
    Owhadi's algorithm, and we have empirically observed results in line with this
    bound for the Neumman-to Dirichlet map. Additionally, we have found that our approximation
    can be applied to inverse problems arising from elliptic PDEs. We have seen promising
    results when using our approximation as a stand-in for the exact operator when
    solving these inverse problems, typically with better performance compared to
    an optimal low-rank approximation derived from an equal number of matrix-vector
    products.
- poster: 140535
  presenter: "Epperly, Ethan"
  email: "dc00ad0d655c559e"
  title: |-
    MS54: Embrace Rejection! Faster Low-Rank Approximation by Rejection Sampling-Accelerated
    Randomly Pivoted Cholesky
  abstract: |-
    The randomly pivoted Cholesky (RPCholesky) algorithm is an effective
    method for producing a low-rank approximation to a positive semidefinite kernel
    matrix, leading to significant speed-ups for kernel machine learning algorithms.
    For even faster speed, one can use a blocked version of the RPCholesky algorithm,
    though the blocked version can produce inaccurate approximations for some examples.
    This raises the question: is there one algorithm that has the accuracy of ordinary
    RPCholesky and the speed of the block version? This talk answers this question
    in the affirmative by introducing accelerated RPCholesky, a rejection sampling-based
    version of RPCholesky that produces the exact same distribution of outputs as
    ordinary RPCholesky while taking advantage of the computational efficiencies of
    blocking. The effectiveness of accelerated RPCholesky is demonstrated using numerical
    experiments, theoretical results, and applications to scientific machine learning.
- poster: 139888
  presenter: "Fan, Xiantao"
  email: "7c7d233e6f2dfbac"
  title: |-
    MS85: Neural Differentiable Modeling with Diffusion-Based Superresolution
    for 2D Spatiotemporal Turbulence
  abstract: |-
    In this paper, we introduce an innovative neural differentiable modeling
    framework designed to enhance the predictability and efficiency of spatiotemporal
    turbulence simulations. Our approach features hybrid differentiable modeling techniques
    that seamlessly integrate deep neural networks with numerical PDE solvers within
    a differentiable programming framework, synergizing deep learning techniques with
    physics-based CFD modeling. Specifically, a hybrid differentiable neural solver
    is constructed on a coarser grid to capture large-scale turbulent phenomena, followed
    by the application of a Bayesian conditional diffusion model that generates the
    small-scale turbulence conditioned on large-scale flow predictions. Two innovative
    hybrid architecture designs are studied, and their performance is evaluated through
    comparative analysis against conventional large eddy simulation techniques with
    physics-based subgrid-scale closures and purely data-driven deep neural solvers.
    The findings underscore the neural differentiable modeling framework's potential
    to significantly enhance the accuracy and computational efficiency of turbulence
    simulations. This study not only demonstrates the efficacy of merging deep learning
    with physics-based numerical solvers but also sets a new precedent for advanced
    CFD modeling techniques, highlighting the transformative impact of differentiable
    programming in scientific computing.
- poster: 140270
  presenter: "Fan, Tiffany"
  email: "edd8ed9034000407"
  title: |-
    Surrogate Modeling for Particle Accelerator Simulations Using Point Clouds
    and Graph Networks
  abstract: |-
    Simulations are crucial tools in particle physics, enabling the design
    and optimization of modern particle accelerator systems. These simulations face
    a significant challenge in balancing speed and accuracy, especially when dealing
    with complex beam behaviors and interactions among numerous accelerator components.
    The computational intensity of these simulations presents a substantial hurdle
    for comprehensive studies. In this work, we leverage data-driven machine learning
    techniques, with a focus on point cloud networks and graph networks, to build
    surrogate models that help address the computational challenges and draw physical
    insights about the underlying system. Point cloud networks efficiently extract
    symmetry-invariant global features from large-scale datasets; graph networks facilitate
    modeling physical interactions between particles via various message-passing algorithms.
    We combine the two ideas in a framework to enable efficient surrogate modeling
    that captures particle interactions in a reduced dimensional space. We test the
    proposed framework in the context of particle accelerator simulations, including
    predicting downstream beam parameters and image-based model diagnostics.
- poster: 140342
  presenter: "Fang, Zhenghan"
  email: "3c20cf5c21d7c270"
  title: "MS32: What's in a Prior? Learned Proximal Networks for Inverse Problems"
  abstract: |-
    Proximal operators are ubiquitous in inverse problems, commonly appearing
    as part of algorithmic strategies to regularize problems that are ill-posed. Modern
    deep learning models have been applied to these tasks too, as in the framework
    of plug-and-play or deep unrolling, where they loosely resemble proximal operators.
    Yet, something essential is lost in these data-driven approaches: there is no
    guarantee that a general deep network represents the proximal operator of any
    function, nor is there any characterization of the function for which the network
    might provide some approximate proximal. This not only makes guaranteeing convergence
    of iterative schemes challenging but, more fundamentally, complicates the analysis
    of what has been learned by these networks about the training data. Herein we
    present learned proximal networks (LPN), prove that they provide exact proximal
    operators for a data-driven nonconvex regularizer, and show how a new training
    strategy, dubbed proximal matching, provably promotes the recovery of the log-prior
    of the true data distribution. Such LPN provide general, unsupervised, expressive
    proximal operators that can be used for general inverse problems with convergence
    guarantees. We illustrate our results in a series of cases of increasing complexity,
    demonstrating that these models not only result in state-of-the-art performance,
    but provide a window into the resulting priors learned from data.
- poster: 139710
  presenter: "Farcas, Ionut-Gabriel"
  email: "f1e05da167760727"
  title: "Distributed Computing for Physics-Based Data-Driven Reduced Modeling at Scale"
  abstract: |-
    High-performance computing (HPC) and data-driven reduced modeling offer
    two complementary perspectives on numerical simulations of complex systems. HPC
    focuses on parallel architectures, algorithms, and software implementations to
    conduct high-fidelity simulations on supercomputers. In contrast, data-driven
    reduced modeling aims to construct computationally inexpensive yet sufficiently
    accurate approximations to enable tasks that are computationally too expensive
    in terms of high-fidelity models. The proposed work enables a fast and scalable
    construction of predictive physics-based reduced models from data in problems
    at a scale and complexity the exceed what standard approaches used in the model
    reduction community can afford. This is achieved by a distributed algorithm that
    integrates high-performance computing into the data-driven reduced modeling procedure.
    Our algorithm enables the efficient and scalable processing of extremely large-scale
    datasets, and the learning of structured physics-based reduced models that approximate
    the dynamical systems underlying those datasets. We demonstrate its effectiveness
    using up to $2\;048$ processing units on the Frontera supercomputer at the Texas
    Advanced Computing Center. We focus on a real-world three-dimensional rotating
    detonation rocket engine simulation with more than $75$ million degrees of freedom
    and show that our distributed algorithm enables the construction of an accurate
    and scalable reduced model within seconds.
- poster: 139837
  presenter: "Faust, Theodore"
  email: "c1abb42d91f7c7"
  title: "Community-Size Biases in Statistical Inference in Temporal Networks"
  abstract: |-
    Researchers often consider the evolution of communities --- sets of densely
    connected nodes that are connected sparsely to nodes in other communities ---
    in temporal networks. We study the performance of a class of statistical inference
    methods for such community detection. We show that models that generate community
    assignments via either a uniform distribution on community assignments or nodewise
    evolution --- with the community assignment of a node in a given layer arising
    from its community assignments in previous layers --- are biased against generating
    communities with large or small numbers of nodes. We then demonstrate that statistical
    inference methods that use such generative models tend to poorly identify community
    structure in networks with large or small communities. To rectify this, we introduce
    a novel model to generate community assignments that uses all of the community
    assignments in the previous layer to generate the community assignments for a
    given layer. Our model greatly mitigates the bias against large and small communities.
    Consequently, statistical inference that uses our generative model is beneficial
    for identifying community structure in networks with such communities.
- poster: 139726
  presenter: "Feng, Hongsong"
  email: "c8315e27a01fe7de"
  title: |-
    MS51: Multiscale Differential Geometry Learning of Networks with Applications
    to Single-Cell Rna Sequencing Data
  abstract: "Single-cell RNA sequencing (scRNA-seq) has emerged as a transformative technology, offering unparalleled insights into the intricate landscape of cellular diversity and gene expression dynamics. Differential geometry serves as a powerful mathematical tool in various applications of scientific research. In this study, we introduce a multiscale differential geometry (MDG) strategy for addressing the challenges encountered in scRNA-seq data analysis. We assume that intrinsic properties of cells lie on a family of low-dimensional manifolds embedded in the high-dimensional space of scRNA-seq data. Multiscale cell–cell interactive manifolds are constructed to reveal complex relationships in the cell–cell network, where curvature-based features for cells can decipher the intricate structural and biological information. We showcase the utility of our novel approach by demonstrating its effectiveness in classifying cell types. This innovative application of differential geometry in scRNA-seq analysis opens new avenues for understanding the intricacies of biological networks."
- poster: 139847
  presenter: "Feng, Xue"
  email: "fc31cb6cb1be2c6e"
  title: "MS84: Anderson Acceleration in Federated Learning"
  abstract: |-
    Federated Learning is a decentralized machine learning approach that enables
    multiple local clients  and  a  central servers to collaboratively learn a model
    while keeping their  data locally.  In this work, we introduce FedOSAA, an innovative
    optimization algorithm for federated learning that combines the simplicity of
    first-order methods with the accelerated convergence performance typically associated
    with second-order methods.   During the local training,  FedOSAA takes a few local
    gradient descent steps, followed by one Anderson acceleration step which accelerates
    the convergence while avoiding the common expense of Newton-based methods, i.e.,
    the  construction and inversion of the Hessian matrix.   We establish a linear
    convergence rate to the global minimizer for FedOSAA on strongly convex losses.  We
    compare FedOSAA with other state-of-the-art federated learning methods such as
    FedSVRG and GIANT on logistic regression problems.  Numerical evidence demonstrates
    the superior performance of our algorithm in terms of communication and computation
    efficiency, which are bottlenecks in federated learning.
- poster: 139956
  presenter: "Fink Shustin, Paz"
  email: "b70fba66f30fd22e"
  title: "Pce-Net: High Dimensional Surrogate Modeling for Learning Uncertainty"
  abstract: |-
    Learning data representations under uncertainty is an important task that
    emerges in numerous machine learning applications. However, uncertainty quantification
    (UQ) techniques are computationally intensive and become prohibitively expensive
    for high-dimensional data. In this paper, we present a novel surrogate model for
    representation learning and uncertainty quantification that aims to deal with
    data of moderate to high dimensions. The proposed model combines a neural network
    approach for dimensionality reduction of the (potentially high-dimensional) data,
    with a surrogate model method for learning the data distribution. We first employ
    a variational autoencoder (VAE) to learn a low-dimensional representation of the
    data distribution. We then propose to harness polynomial chaos expansion (PCE)
    formulation to map this distribution to the output target. The coefficients of
    PCE are learned from the distribution representation of the training data using
    a maximum mean discrepancy (MMD) approach. Our model enables us to (a) learn a
    representation of the data and a mapping between input and output distributions
    under uncertainty, (b) estimate the uncertainty in the high-dimensional data system,
    and (c) match high-order moments of the output distribution; without any prior
    statistical assumptions on the data. Numerical experimental results are presented
    to illustrate the performance of the proposed method.
- poster: 140063
  presenter: "Fisher, Nicholas"
  email: "a6dbfb7fb0d039a1"
  title: |-
    MS16: Learning Divergence-Free Vector Fields from Data Via the Occupation
    Kernel Method
  abstract: |-
    The occupation kernel method (OCK) has proven itself as a robust and efficient
    method for learning nonparametric systems of ordinary differential equations from
    trajectories in arbitrary dimensions. Using an implicit formulation provided by
    vector-valued reproducing kernel Hilbert spaces, we aim to show how the OCK method
    can be adapted to learn vector fields satisfying physical constraints. In this
    presentation, we focus on how particular choices of kernel can ensure that the
    learned vector fields are analytically divergence-free. We validate the proposed
    method through experiments on a variety of simulated and real datasets. It is
    shown that the added constraints often lead to better approximations in these
    application specific problems.
- poster: 140222
  presenter: "Fishman, Nic"
  email: "380857a14daa0abe"
  title: "MS33: Diffusion Models and Flow Matching on General Constrained Domains"
  abstract: |-
    Diffusion and Flow Matching models are two related classes of generative
    models which achieve state-of-the-art results in many domains such as image generation
    and text-to-speech tasks. Diffusion models consist of a noising process destroying
    the data and a backward stage defined as the time-reversal of the noising diffusion.
    Flow matching models work by learning over linear interpolations between two distributions,
    where the distributions are coupled by an optimal transport mapping. Building
    on their success, both types of models have recently been generalized to the Riemannian
    manifold setting. While this setting encompasses many important applications,
    it does not by default include manifolds defined via a set of inequality constraints,
    which are ubiquitous in many scientific domains such as robotics, protein design,
    and quantum state estimation. We discuss methods which attempt to incorporate
    these constraints in both the diffusion paradigm and in the flow matching paradigm,
    discussing costs and benefits of the various approaches.
- poster: 139002
  presenter: "Franks, Ryan"
  email: "9da4549bcedbf587"
  title: "MS4: Wasserstein-Based Explainability for Fairness of ML Models"
  abstract: |-
    The objective of this work is to introduce a fairness interpretability
    framework for measuring and explaining the bias in classification and regression
    models at the level of a distribution. In our work, we measure the model bias
    across sub-population distributions in the model output using the Wasserstein
    metric. To properly quantify the contributions of predictors, we take into account
    the favorability of both the model and predictors with respect to the non-protected
    class. The quantification is accomplished by the use of transport theory, which
    gives rise to the decomposition of the model bias and bias explanations to positive
    and negative contributions. To gain more insight into the role of favorability
    and allow for additivity of bias explanations, we adapt techniques from cooperative
    game theory.
- poster: 140523
  presenter: "Fridovich-Keil, Sara"
  email: "2b76debe04f44609"
  title: "MS32: Thermal Radiance Fields: Regularization for Sensor Fusion (Poster)"
  abstract: |-
    Thermal imaging has a variety of applications, from agricultural monitoring
    to building inspection to imaging under poor visibility, such as in low light,
    fog, and rain. However, reconstructing thermal scenes in 3D presents several challenges
    due to the comparatively lower resolution and limited features present in long-wave
    infrared (LWIR) images. To overcome these challenges, we propose a unified framework
    for scene reconstruction from a set of LWIR and RGB images, using a multispectral
    radiance field with specialized multispectral regularizers. We calibrate the RGB
    and infrared cameras with respect to each other, as a preprocessing step using
    a simple calibration target. We demonstrate our method on real-world sets of RGB
    and LWIR photographs captured from a handheld thermal camera, showing the effectiveness
    of our method at scene representation across the visible and thermal spectra.
    We show that our method is capable of thermal super-resolution, as well as visually
    removing obstacles to reveal objects that are occluded in either the RGB or thermal
    channels.
- poster: 139648
  presenter: "Fularczyk, Nickolas"
  email: "18536b0754e5f56d"
  title: "Convolutional Framelets Using a Tensor Representation"
  abstract: |-
    Patch-based representations of images have shown success as an efficient
    signal representation in image analysis. Convolutional framelets is a novel representation
    of one-dimensional signals which integrates local and nonlocal properties of the
    signal. The potential of this approach led us to investigate generalizing the
    framework to two-dimensional signals. Our approach incorporates techniques from
    tensor analysis with the aim of preserving the structure of the original signal.
    We then show numerical results which suggests our approach is better than the
    convolutional framelets approach where the respective representations are derived
    from a higher order singular value decomposition and the singular value decomposition.
    Our numerical comparison adapts a method, which to our knowledge is one of the
    few, that quantitatively shows that tensor representations, particularly the t-svd,
    are quantitatively better than vectorized representations of multi-dimensional
    data.
- poster: 139864
  presenter: "Gaby, Nathan"
  email: "136ef9b8c587a33b"
  title: "Approximation of Solution Operators for High-Dimensional Pdes"
  abstract: |-
    We propose a finite-dimensional nonlinear model to approximate solution
    operators for evolutional partial differential equations (PDEs), particularly
    in high-dimensions. By employing a general reduced-order model, such as a deep
    neural network, we connect the evolution of the model parameters with trajectories
    in a corresponding function space. Using the computational technique of neural
    ordinary differential equation, we learn the control field over the parameter
    space such that from any initial starting point, the controlled trajectories closely
    approximate the solutions to the PDE. Approximation accuracy is justified for
    a general class of second-order nonlinear PDEs. Numerical results are presented
    for several high-dimensional PDEs, including real-world applications to solving
    Hamilton-Jacobi-Bellman equations. These are demonstrated to show the accuracy
    and efficiency of the proposed method.
- poster: 139689
  presenter: "Gaebler, Johann"
  email: "f70f649c91b81d7d"
  title: "MS88: The Robust Outcome Test: A Simple, Statistically Robust Test of Discrimination"
  abstract: "In observational studies of discrimination, the most common statistical    approaches consider either the rate at which decisions are made (benchmark    tests) or the success rate of those decisions (outcome tests). Both tests,    however, have well-known statistical limitations, sometimes suggesting    discrimination even when there is none. Here we introduce the robust    outcome test and show it yields strong statistical guarantees. Our test    indicates discrimination against a group if both the benchmark test and    the standard outcome test point toward discrimination against that group. We    show this hybrid test yields correct conclusions when the group-specific risk    distributions satisfy the monotone likelihood ratio property (MLRP). We    present empirical evidence that the MLRP holds approximately in examples from    several important domains—including lending, education, and criminal    justice—and that our test is robust to moderate violations of the MLRP. We    apply our robust outcome test to study 2.8 million police stops across    California, finding evidence of widespread racial discrimination."
- poster: 139560
  presenter: "Gajjar, Aarshvi"
  email: "e4c96308890d533f"
  title: |-
    MS39: Agnostic Active Learning of Single Index Models with Linear Sample
    Complexity
  abstract: |-
    We study active learning methods for single index models of the form $F({\mathbf
    x}) = f(\langle {\mathbf w}, {\mathbf x}\rangle)$, where $f:\mathbb{R} \to \mathbb{R}$
    and ${\mathbf x,\mathbf w} \in \mathbb{R}^d$. In addition to their theoretical
    interest as simple examples of non-linear neural networks, single index models
    have received significant recent attention due to applications in scientific machine
    learning like surrogate modeling for partial differential equations (PDEs). Such
    applications require sample-efficient active learning methods that are robust
    to adversarial noise. I.e., that work even in the challenging agnostic learning
    setting.          We provide two main results on agnostic active learning of single
    index models. First, when $f$ is known and Lipschitz, we show $\tilde{O}(d)$ samples
    collected via statistical leverage score sampling suffice to learn a near-optimal
    model. Leverage score sampling is simple, efficient, and widely used for learning
    linear models. Our result needs no assumptions on data distribution, is optimal
    up to log factors, and improves quadratically on a recent ${O}(d^{2})$ bound.
    Second, we show $\tilde{O}(d)$ samples suffice even when $f$ is unknown. Our results
    use tools from high dimensional probability, including Dudley's inequality and
    dual Sudakov minoration, and a novel, distribution-aware discretization of Lipschitz
    functions.
- poster: 140022
  presenter: "Garcke, Jochen"
  email: "47be4f8de7b3443a"
  title: "Using the Signature Transform for Analysing Reinforcement Learning Agents"
  abstract: |-
    Defining differences is a necessary prerequisite to distinguish solutions
    to a given problem. We aim to investigate the performance and behavior of different
    reinforcement learning (RL) agents based on trajectories obtained during roll-outs.
    Here, often the obtained reward of a RL agent is used as a single quantitative
    evaluation metric, but it can often be seen that different agents obtain the same
    reward using different behavioral characteristics. The signature transform offers
    a method to measure distances between paths, where the signature of a path is
    the collection of specific iterated integrals over the path.  We propose and investigate
    the signature transform obtained from roll-out trajectories as a general behavior
    descriptor to distinguish solutions of different methods to reinforcement learning
    problems. Furthermore, we define a robustness score and profile, which allows
    a structured analysis of the behavior of agents and populations of agents under
    a changing environment.   The robustness analysis substantiates the abstract diversity
    induced by distance in signature space through tangible effects and allows a different
    view on generalization and robustness. We show that these ideas have merit in
    the analysis of reinforcement learning algorithms. We illustrate that statistical
    differences between agents can be converted into distances, allowing further analysis,
    e.g. by dimensionality reduction preserving the distance between the signatures
    of trajectories.
- poster: 139891
  presenter: "Geng, Yuwei"
  email: "2d184145a99d3919"
  title: "MS70: Data-Driven Approaches for Solving the Allen-Cahn Equation"
  abstract: |-
    The Allen-Cahn equation describes the process of phase separation and
    transition in phase field modeling of multi-component physical systems. High-fidelity
    numerical simulations of the Allen-Cahn equation can be computationally expensive
    due to its stiffness, compounded by the necessity for multiple simulations in
    parametric Allen-Cahn scenarios within a multi-query setting. In this presentation,
    we will introduce two approaches to accelerate the numerical simulations of the
    Allen-Cahn equation. One approach is based on model order reduction, wherein we
    develop a structure-preserving, gradient-preserving operator inference technique
    for learning the reduced operators. The other approach is deep learning-based,
    involving the design of specialized convolutional neural network models to learn
    fully-discrete operators. We will present numerical experiments demonstrating
    the efficiency of the proposed methods.
- poster: 139630
  presenter: "Ghirardelli, Marta"
  email: "4cb95baaa0b87ab3"
  title: |-
    MS1: Optimisation Algorithms on Riemannian Manifolds with Application to
    Neural Networks
  abstract: |-
    We consider neural networks (NN) as discretizations of continuous dynamical
    systems. There are two relevant systems: the NN architecture on one side and the
    gradient flow for optimizing the parameters on the other. In both cases, stability
    properties of the discretization methods can be relevant e.g. for adversarial
    robustness. Moreover, to prevent the problem of exploding or vanishing gradient,
    it is common to consider NNs whose feature space and/or parameter space is a Riemannian
    manifold. We therefore investigate the stability of numerical one-step integrators
    defined on Riemannian manifolds. We focus on the analysis of geodesic versions
    of the Implicit Euler (GIE) and Explicit Euler (GEE) schemes. We apply these methods
    to non-expansive systems to discuss whether they reflect their contractivity or
    not.
- poster: 140281
  presenter: "Ghosh, Ipsita"
  email: "d1477785a637c77a"
  title: |-
    MS75: Solving Euclidean Distance Geometry Problems from Few Samples Via
    Iteratively Reweighted Least Squares
  abstract: |-
    Euclidean Distance Geometry (EDG) problems arise in molecular conformation,
    sensor network localization and in manifold learning, where the provided data
    includes pairwise distance information, and the goal is to find an Euclidean point
    embedding from a minimal number of sampled pairwise distances. We propose an iterative
    algorithm based on iteratively reweighted least squares (IRLS) which minimizes
    quadratic models derived from a series of continuously differentiable, non-convex
    relaxations of the rank function. We establish a local convergence analysis of
    this IRLS method which applies if a minimal random set of observed distance is
    provided. As a technical tool, we establish a Restricted Isometry Property for
    EDG measurement operators on the rank-r manifold tangent space which might be
    of independent interest for the analysis of other non-convex approaches.  Furthermore,
    we assess data efficiency, scalability and generalizability through numerical
    experiments with simulated data as well as real-world data, demonstrating the
    proposed algorithm's ability to identify the underlying geometry from fewer distance
    samples compared to the state-of-the-art.
- poster: 139741
  presenter: "Ghosh, Ishika"
  email: "c54f74f2e243a980"
  title: |-
    MS91: Computing a Loss Function to Bound the Interleaving Distance for Mapper
    Graphs
  abstract: |-
    Mapper graphs preserve the connected components of the inverse image function
    $f:\mathbb{X} \rightarrow \mathbb{R}$ over any given cover.   Inspired by the
    interleaving distance for Reeb graphs, (Chambers et al.2024) extends this notion
    of distance to discretized mapper graphs. The distance is upper-bounded using
    a loss function. Unlike the NP-hard interleaving distance computation for Reeb
    graphs, the algorithm of the loss function has polynomial complexity.   In this
    paper, we implement the categorical framework of mapper graphs and compute the
    loss function to bound the interleaving distance.
- poster: 140020
  presenter: "Giammarese, Adam"
  email: "533fc786f698e064"
  title: "Tree-Based Learning for High-Fidelity Prediction of Chaos"
  abstract: "Data-driven forecasting of chaotic systems using machine learning is highly sought after in many applications, such as forecasting climate and weather phenomena, stock market indices, and predicting pathological activity in biomedical signals. However, existing solutions, like neural networks-based reservoir computing (RC) and long short-term memory (LSTM), contain numerous model hyperparameters that must be tuned, often requiring high computational resources and large training datasets. Here, we propose a chaotic time series prediction technique with single-digit hyperparameters that employs time delay overembedding and regression tree ensembles. Furthermore, we derive optimal values for these hyperparameters through extensive statistical testing based on training data, eliminating the need for users to tune hyperparameters. Lastly, we demonstrate our proposed approach's state-of-the-art performance on various benchmark tasks, including the discrete Henon map, the continuous Lorenz system, the spatiotemporal Kuramoto-Shivashinsky system, and Southern Oscillation Index (SOI), a crucial but noisy climate time series with few samples, to demonstrate its effectiveness in real-world settings. Additionally, we demonstrate not only the ability of our proposed approach to perform accurate forecasts with less training volume than alternative methods but also the ability of our hyperparameter prescriptions to balance the accuracy and speed of the proposed approach."
- poster: 140318
  presenter: "Gleich, David"
  email: "2408d2b0d8afdb16"
  title: "Better Than Best Low-Rank Approximation with the Singular Value Decomposition"
  abstract: |-
    The Eckhart-Young theorem states that the best low-rank approximation
    of a matrix can be constructed from the leading singular values and vectors of
    the matrix. Here, we illustrate that the practical implications of this result
    crucially depend on the organization of the matrix data. In particular, we will
    show examples where a rank 2 approximation of the matrix data in a different representation
    more accurately represents the entire matrix than a rank 5 approximation of the
    original matrix data -- even though both approximations have the same number of
    underlying parameters. Beyond images, we show examples of how flexible orientation
    enables better approximation of time series data, which suggests additional applicability
    of the findings. Finally, we conclude with a theoretical result that the effect
    of data organization can result in an unbounded improvement to the matrix approximation
    factor as the matrix dimension grows.
- poster: 140224
  presenter: "Glover, Cory"
  email: "dadd24d858ad60bd"
  title: |-
    Deconstructing Reconstruction: Structural Biases in Networks Reconstructed
    from Time Series Dynamics
  abstract: "Many real-world systems—from biology to financial markets—are governed by complex networks of interactions where the network structures are either directly unobservable or must be inferred from data. Often, researchers perform this network reconstruction task by using time series recordings of node-level dynamics in the system. A variety of network reconstruction techniques exist, ranging from causal methods for time series analysis to statistical methods. Despite the multitude of reconstruction techniques, many of the techniques’ underlying biases remain unexamined. Our study explores these biases using network models which control for specific network statistics and a pipeline of network reconstruction which allows us to test diverse time series analysis and reconstruction approaches. Across many different pipeline variations (i.e., dynamical process, time series analysis, reconstruction technique, sparsification procedure), we find that reconstructed networks consistently show characteristic values for a variety of summary statistics, regardless of the original network’s characteristics. Our study offers a new perspective on the structural biases in network reconstruction and hints at potential limitations in reconstructing networks with dynamically redundant edges. Additionally, we scrutinize the preserved network statistics in these reconstructions, offering insights into the network properties targeted by reconstruction methods."
- poster: 140051
  presenter: "Glover, Cory"
  email: "dadd24d858ad60bd"
  title: "Measuring Entanglement in Physical Networks"
  abstract: |-
    Many complex networks, from the brain to the network of atoms or molecules
    in materials, have true physical manifestation. Hence, the nodes and links of
    the network cannot cross each other. Because links cannot cross, they force themselves
    into nontrivial paths, entangling themselves around each other, a phenomena called
    network entanglement. Previously, network entanglement has been measured using
    the graph linking number, a computationally costly metric which can only be applied
    to small systems. Instead, we propose the average crossing number as an alternative
    measure of network entanglement with significantly lower computational cost. We
    analytically derive the dependence of the average crossing number on network density,
    average link length, degree heterogeneity, and community structure. Furthermore,
    we show that the predictions accurately estimate the entanglement of both network
    models and of large scale physical networks found in biological and material systems.
- poster: 139003
  presenter: "Golgoon, Ashkan"
  email: "3bc29867c0ed3e9b"
  title: "MS4: Mechanistic Interpretability of Financial Large Language Models"
  abstract: |-
    Large Language Models (LLMs) like GPT have significant power in generating
    human-like output for a variety of tasks. However, due to their nature, these
    models pose significant challenges for interpreting their underlying decision-making
    processes. This is even more crucial when it comes to financial institutions,
    where concerns regarding bias, fairness, and reliability are highly critical.
    Mechanistic interpretability (MechInterp) [Olah, 2022] deals with reverse engineering
    complex AI models such as transformers and DNNs. Particularly, there have been
    recent advances in making the algorithms that these models implement legible to
    humans, namely: Neel, et al. 2023 providing an explanation for grokking using
    MechInterp for a small transformer; Kevin, et al. 2022 finding a circuit responsible
    for an IOI task; and more recently, Callum, et al. 2023 finding an attention head
    with a functionality known as Copy Suppression. This poster aims at leveraging
    MechInterp in order to demystify the inner workings of financial LLMs. Specifically,
    we start with creating algorithmic tasks tailored to understanding open source
    LLMs outputs when faced with prompts related to the financial industry. Using
    a causal intervention method known as path patching (see [Kevin, et al. 2022]),
    we try to find a circuit and attention heads responsible for completing various
    tasks that investigate bias and fairness in LLMs. Our work is a preliminary step
    in leveraging MechInterp to understand financial LLMs.
- poster: 139971
  presenter: "Golovin, Shira"
  email: "54b95dfafb40b516"
  title: "New Family of Refinable Functions, Easily Learned by Neural Networks"
  abstract: |-
    Two-scale difference equations, also entitled the refinable functions,
    are the pillar stones in many fields, among them are approximation theory, signal,
    and image processing. By applying different refinement rules, it's possible to
    generate subdivision schemes, B-splines, Wavelets, and fractals. Even though earlier
    work proved that all refinable functions can be implemented, up to arbitrary high
    precision, by ReLu-based Neural Networks, it was far from clear how such functions
    could be learned from data. We propose a different type of refinement that involves
    not only translation and rescaling but also mirroring; functions satisfying the
    resulting reflecto-refinable equations still generate multiresolution hierarchies
    that provide an excellent approximation for many functional spaces of interest,
    yet are also adapted to ReLu networks. We will illustrate the proposed methodology
    to create new function families.
- poster: 140203
  presenter: "Gomez-Leos, Alejandro"
  email: "7e59777567b572a9"
  title: "Exponential Speedups in Tensor Completion Via the Average Spectrum Norm"
  abstract: |-
    We consider the problem of low-rank tensor completion (TC) from a noisy
    subset of entries.       Focusing on the canonical polyadic decomposition (CPD),
    we provide severely reduced sample rates for tensor recovery under general measurement
    noise.      Our novel contribution establishes that an $N$-way, CPD rank-$R$ tensor
    with largest dimension $I$ can be accurately approximated from $\mathcal{O}(IR^2
    \log^{N}(I))$ revealed observations under sub-Gaussian noise.      Previously,
    the best upper-bounds in terms of the rank for our setting scaled as $R^{\mathcal{O}(N)}$,
    even for the fundamental task of TC under Gaussian additive noise.      At the
    core of our proof technique is the recently studied Average Spectrum Norm (Lopez
    et al., 2024), which we utilize to circumvent the challenges faced in previous
    works.      We corroborate our results with comprehensive experiments.
- poster: 140186
  presenter: "Gong, Xindi"
  email: "9c1dce2aa98df05e"
  title: "MS5: Derivative-Informed Neural Operators for Optimization under Uncertainty"
  abstract: |-
    In this work we investigate the application of Derivative-Informed Neural
    Operators (DINOs) to PDE-constrained optimization under uncertainty (OUU). Building
    on DINO, we propose an operator learning framework for parametric PDE problems,
    with additional training on derivatives of the input-output map. Through this,
    we attain an accurate approximation of the PDE solution operator and its derivatives
    for the optimization problem. We present numerical examples such as flow control
    and aerodynamic shape optimization problems to test our method and demonstrate
    its performance. Through the numerical experiments, we show that DINOs can be
    orders of magnitude more sample-efficient than non-derivative informed operator
    learning strategies.
- poster: 140530
  presenter: "Goren, Matan"
  email: "3374f161a47a52ad"
  title: |-
    MS18: Physics-Guided Full Waveform Inversion Using Encoder-Solver Convolutional
    Neural Networks (Poster)
  abstract: |-
    Full Waveform Inversion (FWI) is an inverse problem for estimating the
    wave velocity distribution in a given domain, based on observed data on the boundaries.
    The inversion is computationally demanding because we are required to solve multiple
    forward problems, either in time or frequency domains, to simulate data that are
    then iteratively fitted to the observed data. We consider FWI in the frequency
    domain, where the Helmholtz equation is used as a forward model, and its repeated
    solution is the main computational bottleneck of the inversion process. To ease
    this cost, we integrate a learning process of an encoder-solver preconditioner
    that is based on convolutional neural networks (CNNs). The encoder-solver is trained
    to effectively precondition the discretized Helmholtz operator given velocity
    medium parameters. Then, by re-training the CNN between the iterations of the
    optimization process, the encoder-solver is adapted to the iteratively evolving
    velocity medium as part of the inversion. Without retraining, the performance
    of the solver deteriorates as the medium changes. Using our light retraining procedures,
    we obtain the forward simulations effectively throughout the process. We demonstrate
    our approach to solving FWI problems using 2D geophysical models with high-frequency
    data.
- poster: 139846
  presenter: "Gounley, John"
  email: "d4d77fc49ed7dded"
  title: "MS84: Federated Learning with Heterogenous Private Data Silos"
  abstract: |-
    Biomedical data, such as electronic medical records, is typically siloed
    within institutions and contains private information that limits sharing. Federated
    learning approaches incorporating privacy-preserving algorithms, such as differential
    privacy, provide an avenue to enabling the development of deep learning models
    from biomedical data. However, incorporating privacy mechanisms poses challenges
    to maintaining model accuracy, particularly in the case of underrepresented classes
    in heterogeneous data silos. We consider the effectiveness of centralized and
    distributed synthetic data generation approaches to addressing these challenges,
    using both naive and generative AI methods. We evaluate these approaches in the
    context of developing models with privacy-preserving federated learning on cancer
    surveillance datasets from the National Cancer Institute's (NCI) Surveillance,
    Epidemiology, and End Results (SEER) program.
- poster: 139968
  presenter: "Goyal, Agam"
  email: "84c14c8bf826ebe0"
  title: "MS14: A Latent Linear Model for Nonlinear Coupled Oscillators on Graphs"
  abstract: "A system of coupled oscillators on an arbitrary graph is locally driven by the tendency to mutual synchronization between nearby oscillators, but can and often exhibit nonlinear behavior on the whole graph. Understanding such nonlinear behavior has been a key challenge in predicting whether all oscillators in such a system will eventually synchronize. In this paper, we demonstrate that, surprisingly, such nonlinear behavior of coupled oscillators can be effectively linearized in certain latent dynamic spaces. The key insight is that there is a small number of ‘latent dynamics filters’, each with a specific association with synchronizing and non-synchronizing dynamics on subgraphs so that any observed dynamics on subgraphs can be approximated by a suitable linear combination of such elementary dynamic patterns. Taking an ensemble of subgraph-level predictions provides an interpretable predictor for whether the system on the whole graph reaches global synchronization. We propose algorithms based on supervised matrix factorization to learn such latent dynamics filters. We demonstrate that our method performs competitively in synchronization prediction tasks against baselines and black-box classification algorithms, despite its simple and interpretable architecture. This poster is associated with the minisymposium entitled Probabilistic Methods in Machine Learning and Complex Systems."
- poster: 139885
  presenter: "Grauer, Samuel"
  email: "cf10eb74912a1942"
  title: |-
    MS16: Flow Reconstruction from Parameterized Lagrangian Tracks with Hard
    Data-Based Constraints
  abstract: "Flow reconstruction is an inverse problem in which the state of a fluid flow – velocity, pressure, etc. – is determined from a set of sparse measurements. The state is resolved in space $\\Omega \\in \\mathrm{R}^d$, for $d = 2$ or 3, and time $T \\in \\mathrm{R}^+$, and the flow is governed by the Navier–Stokes equations (NSE). We consider observations of particle trajectories, termed Lagrangian tracks, in a turbulent flow. These tracks correspond to streaklines for small particles of null mass, while larger, heavier, or buoyant particles are subject to inertial transport and “slip.” Particle motion is governed by the Maxey–Riley equation (MRE), with one equation per particle, parameterized by the particle’s size and density. Reconstructing flow states from Lagrangian tracks thus amounts to solving the coupled NSE and MSE and estimating the properties of each particle. We report a neural solver that can tackle this problem. Accurate solutions may be obtained even when the particles’ size and density are unknown and vary from particle to particle. We embed the particle trajectories as a hard constraint and use coordinate neural networks to represent flow states. The NSE and MRE are weakly enforced by minimizing their aggregate residual, integrated over $\\Omega \\times T$. Flow states, particle states, and unknown particle properties are optimized using backpropagation. Synthetic and experimental demonstrations are reported."
- poster: 140071
  presenter: "Graziani, Carlo"
  email: "f692ca7f17d30a42"
  title: "MS68: A Gaussian Process Language Model"
  abstract: |-
    We present a Gaussian process language model (GPLM). GPLM is a two-level
    hierarchical Bayesian model, with the base level representing categorical token
    probabilities as a function of sequence location, and the top level establishing
    correlations among those token probabilities by means of a multi-output Gaussian
    process over a latent space that maps to the simplex of categorical probabilities.  The
    latent-space model is stationary with respect to sequence location, which permits
    covariances to be estimated directly from the data without resort to parameter
    optimization.  We show results obtained using genomic sequences to predict (infill)
    sequence gaps and predict likely regions of variation.
- poster: 139828
  presenter: "Gruber, Anthony"
  email: "7c4e18e3234c16b9"
  title: |-
    MS30: Reversible and Irreversible Bracket-based Dynamics for Deep Graph
    Neural Networks
  abstract: |-
    Recent works have shown that physics-inspired architectures allow the
    training of deep graph neural networks (GNNs) without oversmoothing. The role
    of these physics is unclear, however, with successful examples of both reversible
    (e.g., Hamiltonian) and irreversible (e.g., diffusion) phenomena producing comparable
    results despite diametrically opposed mechanisms, and further complications arising
    due to empirical departures from mathematical theory. This work presents a series
    of novel GNN architectures based upon structure-preserving bracket-based dynamical
    systems, which are provably guaranteed to either conserve energy or generate positive
    dissipation with increasing depth. It is shown that the theoretically principled
    framework employed here allows for inherently explainable constructions, which
    contextualize departures from theory in current architectures and better elucidate
    the roles of reversibility and irreversibility in network performance. Code is
    available at the Github repository \url{https://github.com/natrask/BracketGraphs}.
- poster: 140242
  presenter: "Gu, Hyemin"
  email: "19bd83d6f046f21c"
  title: "MS3: Lipschitz Regularized Gradient Flows and Latent Generative Particles"
  abstract: |-
    We developed a generative particle algorithm(GPA) that solves particle
    systems for Wasserstein gradient flows minimizing a regularized variant of $f$-divergences
    which are called Lipschitz regularized $f$-divergences. The Lipschitz regularized
    $f$-divergence is formulated as an $f$-divergence infimally convolved with Wasserstein-1
    proximal, forming a robust framework. In our algorithm the $f$-divergence is estimated
    only using samples by solving its dual formulation in a space of Lipschitz continuous
    neural network functions. The optimizer function equals the first variation of
    the Lipschitz regularized $f$-divergence, and therefore its gradient produces
    a vector field for particle dynamics.  Lipschitz regularization imposes a speed
    limit on the vector field and enhances stability of the dynamics. We also show
    that a specific selection of $f$ enables stable generation of heavy-tailed data
    distributions.  As a generative model, GPA excels in generating samples from scarce
    target data and is scalable to high-dimensional problems such as MNIST image generation.
    Since GPA offers the freedom to select the initial distribution, we show an application
    of GPA to gene expression data integration.
- poster: 140171
  presenter: "Gu, Yanbing"
  email: "3efec9cbce1ec883"
  title: "MS93: Learning Dynamical Systems with the Spectral Exterior Calculus"
  abstract: |-
    We introduce a data-driven framework for learning dynamical systems on
    Riemannian manifolds based on the spectral exterior calculus. Using eigenfunctions
    and eigenvalues of the Laplacian on smooth functions, approximated by the diffusion
    maps algorithm, the spectral exterior calculus represents vector fields as linear
    combinations of frame elements (i.e., elements of overcomplete bases), which act
    as generators of dynamical systems on the manifold of interest. We present an
    implementation of this framework that consistently represents vector fields using
    Monte Carlo approximation from data points sampled on low-dimensional manifolds,
    such as the circle and 2-torus. In addition, we solve initial-value prediction
    problems using our vector field representations and compare the performance with
    solutions under the true systems.
- poster: 140194
  presenter: "Guadagni, Gianluca"
  email: "949ef9587968e00b"
  title: "Sequential Gradient Descent"
  abstract: "In biological neural systems the concept of local interactions plays a crucial role. Inspired by the way biological neurons communicate primarily with their immediate neighbors, we propose a neural network training approach based on sequential parameter updates. Here’s a concise overview: We begin by updating the parameters of the rightmost layer (typically the output layer).  Subsequently, we move layer by layer toward the input layer.  At each step, we re-evaluate the loss function and adjust the parameters of the current layer using (stochastic) gradient descent.  This local context mirrors the biological principle of neurons being aware only of nearby influences.  There are expected benefits from this approach, such as fine-grained control, interpretability, as the approach aligns with our understanding of biological neural systems and their local influence among layers, which can help capture local patterns.  The challenge include delayed learning, as the updates may transfer slowly to early layers, computational overhead, with frequent loss function evaluations, and sensitivity to initialization, with poorly initialized layers impacting subsequent ones.  Our goal is to compare this method with the usual stochastic gradient descent method for ANN and CNN."
- poster: 139699
  presenter: "Guerra, Martin"
  email: "6c1cb686a9942d95"
  title: "MS27: Swarm-Based Gradient Descent Meets Simulated Annealing"
  abstract: |-
    In generic non-convex optimization, one needs to be able to pull samples
    out of local optimal points to achieve global optimization. Two common strategies
    are deployed: adding stochasticity to samples such as Brownian motion, as is done
    in simulated annealing (SA), and employing a swarm of samples to explore the whole
    landscape, as is done in Swarm-Based Gradient Descent (SBGD). The two strategies
    have severe drawbacks but complement each other on their strengths. SA fails in
    the accuracy sense, i.e., finding the exact optimal point, but succeeds in always
    being able to get close, while SBGD fails in the probability sense, i.e., it has
    non-trivial probability to fail, but if succeeds, can find the exact optimal point.
    We propose to combine the strength of the two and develop a swarm-based stochastic
    gradient method with samples automatically adjusting their annealing. Using mean-field
    analysis and long-time behavior PDE tools, we can prove the method to succeed
    in both the accuracy sense and the probability sense. Numerical examples verify
    these theoretical findings.
- poster: 139635
  presenter: "Guinovart, David"
  email: "e399fd59a25fc61c"
  title: |-
    Utilizing Dynamical Systems and Multipopulation Models to Track Disease
    Spread in Cuba: A Focus on Dengue, Covid-19, and HIV
  abstract: |-
    The management and prediction of infectious diseases such as dengue, COVID-19,
    and HIV are critical challenges in public health, particularly in regions like
    Cuba, where these diseases significantly impact the population. This work introduces
    innovative approaches using dynamical systems and multipopulation models to understand
    and predict the spread of these diseases. By integrating real-time data and leveraging
    the versatility of these models, we aim to provide robust analytical tools that
    can assist in making informed public health decisions. We specifically explore
    the effectiveness of these models in capturing the dynamics of disease transmission
    among different population groups and geographical distributions in Cuba. The
    results showcase the potential of mathematical modeling in enhancing disease surveillance
    and response strategies, offering insights crucial for health policy planning
    and implementation. This approach not only aids in managing the current health
    issues but also in preparing for future outbreaks, thereby contributing to a resilient
    health infrastructure.
- poster: 140537
  presenter: "Guo, Ziheng"
  email: "b0a3b91495df233f"
  title: "MS55: Learning from High-Dimensional Stochastic Dynamics"
  abstract: |-
    We develop an innovative learning framework that incorporates the noise
    structure to infer the governing equations from observation of trajectory data
    generated by stochastic dynamics. Our approach can proficiently capture both the
    noise and the drift terms.  Furthermore, it can also accommodate a wide range
    of noise types, including correlated and state-dependent variations. Moreover,
    our method demonstrates scalability to high-dimensional systems when combined
    with popular deep learning approaches. Through extensive numerical experiments,
    we showcase the exceptional performance of our learning algorithm in accurately
    reconstructing the underlying stochastic dynamics.
- poster: 139936
  presenter: "Guo, Wei"
  email: "3438cabfe56ac07"
  title: "MS42: Discrete Stochastic Interpolant"
  abstract: |-
    We introduce the discrete stochastic interpolant, a novel method for interpolating
    between two probability distributions on a discrete space. This technique facilitates
    sampling from an unknown target distribution. We demonstrate the application of
    our method, particularly in the context of natural language generation.
- poster: 140268
  presenter: "Gupta, Kanan"
  email: "ff3c83a558256d"
  title: "Nesterov Acceleration Despite Very Noisy Gradients"
  abstract: "Momentum-based gradient descent methods use information gained along the trajectory, in   addition to the local information from the gradient, in order to achieve an accelerated rate of   convergence. These methods have been well-studied for convex optimization. Computing the   gradient is often too expensive and it is approximated using stochastic gradient estimates in   practice. However, there’s a lack of theoretical analyses of accelerated methods in the setting   of stochastic gradient descent, even for the simple case of convex functions. We address this   gap with a novel descent algorithm (AGNES), which provably achieves acceleration for smooth   convex minimization tasks with noisy gradient estimates if the noise intensity is proportional to   the magnitude of the gradient. Nesterov’s accelerated gradient descent does not converge under   this noise model if the constant of proportionality exceeds one. AGNES fixes this deficiency   and provably achieves an accelerated convergence rate no matter how small the signal to noise   ratio in the gradient estimate. Empirically, we demonstrate that this is an appropriate model   for mini-batch gradients in overparameterized deep learning. Finally, we show that AGNES   outperforms stochastic gradient descent with momentum and Nesterov’s method in the training   of CNNs."
- poster: 139615
  presenter: "Gupta, Gaurav"
  email: "d1041cb042b87c01"
  title: |-
    MS74: Enhancing Ai Interactions: Comprehensive Review on Prompt Engineering
    Techniques
  abstract: |-
    In this poster, we conduct a comprehensive survey on different prompt
    engineering techniques within the sphere of Large Language Models (LLMs). Prompt
    engineering has become an important technology for improving the generation capabilities
    of LLMs across tasks. This study presents a systematized analysis of existing
    research on prompt engineering techniques, with the aim of clarifying their effects,
    comparing the methodologies, and revealing the possible routes for future innovation.
    Through a systematic review of peer-reviewed articles and conference papers, we
    analyze various approaches to prompt engineering, demonstrating how the different
    techniques affect the performance of LLMs and under which circumstances certain
    approaches become superior to others. We find a consensus on the potential of
    prompt engineering to improve the performance of LLMs substantially, though the
    efficacy of prompting methods for specific tasks is highly model-architecture-dependent.
    We also demonstrate the emerging trends in prompt engineering, like dynamic prompts
    and context-aware prompting, and discuss the direction of future studies. Our
    review underscores the need for further investigation on prompt engineering methods,
    especially on how to establish more adaptive, secure, and intelligent prompting
    mechanisms that keep up with the improving generation capabilities of LLMs.
- poster: 140206
  presenter: "Gupta, Anant"
  email: "e78127f742ee0bd0"
  title: "MS93: A Reduced Operator Newton Method for Bayesian Filtering"
  abstract: |-
    Bayesian filtering is an iterative Bayesian inference problem, where the
    goal is to sample from the posterior distribution of the state of a dynamical
    system given past observations. A popular choice for high-dimensional Bayesian
    inference utilizes the concept of measure transport by finding through explicit
    parameterizations or nonparametrically through particle evolutions an invertible
    transformation of samples of a reference measure into those of the target. Despite
    much research effort into efficient transport-based sampling and inference in
    the setting where the score (gradient of log density) of the target is known,
    state-of-the-art methods still have an exponential scaling of the computational
    cost with dimension. In this work, we introduce a novel notion of low-dimensional
    structure that arises in the filtering problem when the underlying dynamics is
    chaotic, that of conditional absolute continuity. We exploit the absolute continuity
    of the target measures on low-dimensional manifolds and hyperbolicity of the dynamics
    to develop new algorithms to estimate a conditional score associated with the
    target. We then develop an operator-theoretic Newton-Raphson method that uses
    the learned scores to compute transport maps on low-dimensional manifolds. Both
    the intrinsic dimension reduction as well as the speed of convergence of Newton
    methods leads to potentially scalable solutions to filtering in high-dimensional
    chaotic systems, such as in geophysical applications.
- poster: 139187
  presenter: "Gutierrez Hernandez, Sebastian"
  email: "16039facb7aa97e9"
  title: |-
    MS65: Control-Deeponet: Deep Operator Networks for Pde-Constrained Optimal
    Control under Uncertainty
  abstract: "In this presentation, we address the challenges of applying the DeepONet architecture to PDE-constrained optimization under uncertainty. While DeepONet has been effective in providing fast surrogates in solving time-dependent PDE-constrained optimization (PDECO) problems [Wang, Sifan, et al. 'Fast PDE-Constrained Optimization via Self-Supervised Operator Learning,' arXiv, 25 Oct. 2021], it encounters limitations in cases where the operator depends on multiple inputs. In PDECO under uncertainty, the solution operator maps the product of two Banach spaces – the control space and the uncertain parameter space (e.g., heat conduction field for the heat equation, permeability coefficient for Darcy flow diffusion equation, material property in elasticity equation) to the PDE solution Banach space. To address this challenge, we employ the Multiple Input Operator Network (MIONet) [Jin, Pengzhan, et al. 'MIONet: Learning Multiple-Input Operators via Tensor Product,' SIAM Journal on Scientific Computing, vol. 44, no. 6, Dec. 2022]. MIONet provides a versatile framework for learning surrogate models that can handle operators dependent on multiple inputs. We demonstrate how this surrogate model can be effectively used to optimize risk-averse measures and obtain robust controls under uncertainty."
- poster: 139812
  presenter: "Gutierrez Hernandez, Sebastian"
  email: "16039facb7aa97e9"
  title: "MS23: Control Problems in Wasserstein Space Via Conditional Flow Matching."
  abstract: |-
    The space of squared integrable probability densities in $\mathbb{R}^d$,
    equipped with the Wasserstein Metric ($W_2$), has a manifold structure suitable
    for studying the evolution of densities.  This study focuses on optimizing the
    transportation of a reference density $\mu,$ to a target density $\nu$ given a
    dynamic transportation cost. We present an algorithm that uses conditional flow
    matching and parametric Hamiltonian Flows techniques to approximate solutions
    to control problems in $W_2$.
- poster: 139565
  presenter: "Häberle, Konstantin"
  email: "aa77ecc1350b9714"
  title: "Separating Capacity of Convolutional Neural Networks"
  abstract: |-
    Convolutional neural networks (CNNs) have demonstrated remarkable success
    in the realm of pattern classification, a core task in the field of machine learning.
    Particularly notable are their state-of-the-art results in image and sound signal
    classification. In these applications, CNN-based feature extraction is typically
    performed first, a trainable classifier such as, e.g., a support vector machine
    is then applied to the extracted features. Feature extractors generated by CNNs
    are multi-layer networks, where each layer consists of a convolution with an atom
    of a frame (i.e., a redundant spanning set), followed by a pointwise nonlinearity
    and a pooling operator. We explain the reasons behind the success of such feature
    extractors by studying them through the lens of function-counting theory, a framework
    for pattern classification established by Cover, 1965. In particular, the notion
    of separating capacity plays a crucial role in this framework and is based on
    counting the number of realizable dichotomies (i.e., binary label assignments).
    We compute the separating capacity of CNN-based feature extractors in terms of
    their network parameters and architectures (i.e., frames, nonlinearities, pooling
    operators, widths, and depths). By doing so, we deduce that generic CNNs exhibit
    a large separating capacity, which explains why such networks excel at feature
    extraction and hence in classification tasks.
- poster: 139735
  presenter: "Hadi, Ido"
  email: "5fe03a0869e90804"
  title: "MS95: SE(3) Synchronization by Dual Quaternions"
  abstract: |-
    In synchronization problems, the goal is to estimate elements of a group
    from noisy measurements of their ratios. A popular estimation method for synchronization
    is the spectral method. It extracts the group elements from eigenvectors of a
    block matrix formed from the measurements. The eigenvectors must be projected,
    or "rounded", onto the group. The rounding procedures are constructed ad hoc and
    increasingly so when applied to synchronization problems over non-compact groups.
    We developed a spectral approach to synchronization over the non-compact group
    SE(3), the group of rigid motions of $\mathbb{R}^3$. We based our method on embedding
    SE(3) into the algebra of dual quaternions, which has deep algebraic connections
    with the group SE(3). These connections suggest a natural rounding procedure considerably
    more straightforward than the current state-of-the-art for spectral SE(3) synchronization,
    which uses a matrix embedding of SE(3). We show by numerical experiments that
    our approach yields comparable results to the current state-of-the-art in SE(3)
    synchronization via the spectral method. Thus, our approach reaps the benefits
    of the dual quaternion embedding of SE(3), while yielding estimators of similar
    quality.
- poster: 139189
  presenter: "Hadia, Xian"
  email: "f13e4986aadb7707"
  title: |-
    MS65: Harnessing Data Efficiency with a Fixed-Point Iteration Analogous
    to Adaptive Mesh Refinement
  abstract: "Representing complex physical systems through statistical models necessitates extensive data collection, which can be both resource-intensive and computationally demanding. Experimentation often requires significant labor and resources, and high-fidelity simulations impose heavy computational burdens. This challenge necessitates more data-efficient regimes. Our approach draws inspiration from two concepts: mesh refinement and deep equilibrium models. Mesh refinement discretizes the computational domain into varying resolutions based on the specific complexity required by different regions of the domain, and deep equilibrium models are a type of deep learning model that map inputs to steady-state solutions. We propose a novel framework that integrates these two concepts to enhance the training of surrogate models. We apply this framework to training Physics-Enhanced Deep Surrogate (PEDs), a type of surrogate model that ensembles a neural network with an approximate physics solver to predict the target properties of physical systems governed by partial differential equations (PDEs). By employing a fixed-point iteration solver, our framework recursively feeds the output or target property back into the surrogate model. This feedback loop provides the neural network with continuous information on the dynamics of the physical system, enriching the model’s understanding without utilizing additional data. This thereby increases the model’s accuracy while being data efficient."
- poster: 139819
  presenter: "Halikias, Diana"
  email: "84b6780df38c0d81"
  title: "Adjoint-Free Operator Learning and Matrix Recovery"
  abstract: |-
    There is a mystery at the heart of operator learning: how can one recover
    a non-self-adjoint operator from data without probing the adjoint? Practical approaches
    to operator learning suggest that one can accurately recover an operator while
    only using data generated by the forward action of the operator, without access
    to the adjoint. However, while existing theory justifies low sample complexity
    in operator learning, it relies on access to the adjoint. We prove that without
    querying the adjoint, one can approximate a family of non-self-adjoint infinite-dimensional
    compact operators via projection onto a Fourier basis. We apply this result to
    Green's functions to derive the first rigorous adjoint-free sample complexity
    bound for elliptic PDE learning. Still, we also show that for some fundamental
    tasks, such as low-rank matrix approximation and norm estimation, sampling the
    adjoint is essential.
- poster: 140115
  presenter: "Hamel, Jackson"
  email: "d781ff57bc1f5e03"
  title: |-
    MS86: Medical Image Classification by Graph Clustering Methods Applied to
    a Neural Network Approach
  abstract: |-
    We are given a set of medical image data and would like to classify these
    images. There  are many neural network algorithms and topological machine learning
    algorithms to do the image classification. However, this problem can be formulated
    as a graph clustering problem. The vertices of a graph are images and the vertices
    within the same cluster can be assumed to share similar features and properties,
    thus making the applications of graph clustering techniques very useful for image
    classification. We shall first use a box spline based wavelet-framelet method
    to clean the images and help building up the adjacency matrix for the given image
    data. Recently, the approach based on sparse solutions of linear systems for graph
    clustering finds clusters more efficient than the traditional spectral clustering
    method. We propose to use the two newly developed graph clustering methods based
    on sparse solution methods for linear system for image classification. The performance
    of our graph clustering methods will be shown to be very effective to classify
    an image. For the focus of this presentation, we also found ways to incorporate
    these clustering methods into a convolutional neural network (CNN) to perform
    more detailed image data classification. Additionally, the images have been pre-processed
    for these techniques in ways that have not been commonly applied in CNNs of this
    type. Numerical results will be demonstrated.
- poster: 140289
  presenter: "Han, Fuqun"
  email: "2da86b9d1a41c646"
  title: |-
    MS66: Tensor Train Based Sampling Algorithms for Approximating Regularized
    Wasserstein Proximal Operators
  abstract: |-
    In this poster, we will introduce an algorithm based on tensor train approximations
    for sampling from a target distribution. We employ a semi-backward Euler discretization
    on the score dynamic and a kernel formula that comes from approximating the regularized
    Wasserstein operator. Moreover, we will utilize tensor train approximation to
    capture the evolution of high-dimensional probability density in Langevin dynamics.
    A rigorous analysis of the algorithm on Gaussian distributions will be presented.
    To show the effectiveness of our methods, we will provide numerical examples including
    sampling from various distributions and solving Bayesian inverse problems.
- poster: 140561
  presenter: "Hanks, Tyler"
  email: "74282a84c26bf38f"
  title: "MS12: A Compositional Framework for First-Order Optimization (Poster)"
  abstract: |-
    Optimization decomposition methods are a fundamental tool to develop distributed
    solution algorithms for large scale optimization problems arising in fields such
    as machine learning and optimal control. In this poster, we present an algebraic
    framework for hierarchically composing optimization problems defined on hypergraphs
    and automatically generating distributed solution algorithms that respect the
    given hierarchical structure. We present an implementation of this framework in
    a Julia package called AlgebraicOptimization.jl and use our implementation to
    empirically demonstrate that hierarchical dual decomposition outperforms standard
    dual decomposition on problems with hierarchical structure.
- poster: 139631
  presenter: "Hansen, Martine Dyring"
  email: "ffda9f14e877d29"
  title: |-
    MS1: Variational Structure-Preserving Neural Networks for Forced Lagrangian
    Systems
  abstract: |-
    Structure preserving machine learning has recently become an active area
    of research. Popular models in this field, such as Hamiltonian neural networks,
    would typically require data on the system's momentum and this can be a limitation
    of the approach. Instead, we consider a method for learning the differential equations
    describing the dynamics of a forced Lagrangian system. The method requires time-series
    measurements of the system's position only and can learn external forces, e.g.,
    dissipative frictional forces. We investigate how learning external forces impact
    the training of the network, and discuss regularization approaches.
- poster: 139861
  presenter: "Harrington, Grayson"
  email: "69bbf689750c2bd6"
  title: |-
    MS9: Dataset Design to Accelerate Training Process-Structure Surrogate Models
    of Microstructure Evolution
  abstract: "Surrogate models for the evolution of material microstructures – spatial fields that characterize the material – are needed to perform important engineering tasks like manufacturing optimization. However, traditional strategies face serious bottlenecks in collecting sufficient training data, primarily relying on simulating entire evolution pathways. Computational costs restrict the total number of simulations, producing highly correlated and redundant datasets. In this study, we propose a novel approach that integrates microstructure generation protocols with active learning (AL) to address these challenges: (1) AL identifies points in the microstructure space of high model uncertainty, (2) representative microstructures are generated, (3) microstructure evolution simulations produce multiple frames of interest, (4) the model is trained with new data, and (5) the process repeats until a robust model is achieved. By incorporating microstructure generation into the AL loop, our approach enables simulations to start from diverse initial conditions, enhancing the robustness of surrogate models and eliminating the need for exhaustive dataset generation. Our research aims to provide a more efficient and reliable solution for surrogate modeling of microstructure evolution simulation tools in the field of materials science."
- poster: 139115
  presenter: "Hart, Emma"
  email: "6b938e6b55a45fe"
  title: "MS69: Paired Autoencoders for Inference and Regularization"
  abstract: |-
    In this work, we describe a reduced approach that exploits technologies
    from machine learning (e.g., neural networks and auto-encoder networks) and dimensionality
    reduction models (e.g., low-rank and latent representations) to advance various
    technologies for inverse problems.  We consider a decoupled approach for surrogate
    modeling, where unsupervised learning approaches are used to efficiently represent
    the input and target spaces separately, and a supervised learning approach is
    used to represent the mapping from one latent space to another.  We demonstrate
    that our approach can outperform others in scenarios where training data for unsupervised
    learning is easily available, but the number of input/target pairs for supervised
    learning is small, and we introduce how the approach can be used for defining
    regularization or prior knowledge, and/or as a surrogate model for inversion,
    forward propagation, and adjoint free methods.
- poster: 140278
  presenter: "Hartman, Emmanuel"
  email: "824cbce4a41e1ac8"
  title: "MS90: Geometric Deep Learning for Unregistered Shape Data"
  abstract: |-
    We will present various geometric deep frameworks designed to generate
    feature vector representations of 3D geometric data. Specifically, we will delve
    into techniques that seek to produce feature vectors invariant to parameterization,
    ensuring that different observations of the same shape (viewed as different parameterizations
    of the geometric data) are mapped to identical feature vectors. Further we will
    demonstrate the utility of these feature vectors in a variety of downstream learning
    tasks such as classification, registration, and shape reconstruction.
- poster: 139999
  presenter: "Hassan, Danish"
  email: "ad9c3363f8c98b6f"
  title: |-
    Forecasting Solar-Geomagnetic Activity Using Ai: A Markov Chain Monte Carlo
    Approach
  abstract: |-
    The compatibility of solar and geomagnetic activity is calculated using
    the Markov chain Monte Carlo (MCMC) technique, an application of artificial intelligence
    (AI). The study covers various sunspots durations (1749-2017, 1932-2017, & 2009-2017)
    and related geomagnetic (Ap) durations (1932-2017) by employing a Markov 4-states
    transition matrix. The classification of the matrix is determined by the maximum
    to minimum range of each cycle in the selected data sets. This AI-driven analysis
    establishes a behavioral relationship and draws samples from complex distributions
    of sunspots and Ap index using the MCMC technique. The expected number of sunspots
    and geomagnetic activity for each state remains almost constant, demonstrating
    the AI's ability to analyze and predict patterns in complex datasets. Significant
    results of predicted frequencies during 20, 40, and 80 years are aligned with
    observed frequencies of sunspot and Ap index, indicating the AI's effectiveness
    in predictive modeling. Based on model-generated results, the study suggests that
    the future behavior of the studied parameters will follow a similar progression,
    showcasing the application of AI in forecasting natural phenomena. Additionally,
    the effects of the sun on geomagnetic index are confirmed, and the frequency for
    upcoming cycles can be predicted by this AI-driven study.
- poster: 139707
  presenter: "Haubner, Johannes"
  email: "2cc92281b95ad4ca"
  title: |-
    MS73: Learning Mesh Motion Techniques with Application to Fluid-Structure
    Interaction
  abstract: |-
    The concept of domain transformation is a common tool used in shape optimization
    via the method of mappings and in monolithic Arbitrary Lagrangian-Eulerian (ALE)
    formulations of Fluid-Structure Interaction (FSI) problems. Involving appropriate
    transformations, both can be formulated on a fixed reference domain. In order
    to obtain the transformations, one typically extends boundary or interface deformation
    to the interior of (parts of) the domain. If, at a discrete level, the transformed
    mesh degenerates, the arising systems of equations are not solvable, which yields
    numerical issues in performing shape optimization and simulating FSI. Several
    heuristic approaches based on partial differential equations (PDEs) have been
    proposed in the literature, including harmonic extension operators, linear elastic
    extensions (with space-dependent coefficients), biharmonic extension operators,
    and nonlinear techniques using the p-Laplace equations. In the scope of this talk,
    we discuss two choices of operators that are motivated by techniques from machine
    learning and require appropriately chosen neural network (NN) architectures. We
    consider a hybrid PDE-NN approach and a NN-corrected approach. Moreover, we discuss
    computational costs and generalizability of the proposed methods.
- poster: 140273
  presenter: "Havrilla, Alexander"
  email: "f5b27db1143f0071"
  title: |-
    Predicting Scaling Laws with Approximation and Statistical Theory for Transformer
    Neural Networks on Intrinsically Low-Dimensional Data
  abstract: |-
    During training, transformer generalization error can be empirically predicted
    as a power-law in terms of the number of training samples and the network size.
    We establish approximation and statistical theory for transformer neural networks
    that relates the exponent of the power-law to the intrinsic dimension of the data
    manifold.
- poster: 140175
  presenter: "Hayes, Nicole"
  email: "4f3f0b8f28241725"
  title: |-
    MS51: Integrating Transformer, Autoencoder, and Circular Fingerprint Techniques
    with Spectral Graph Methods to Predict Scarcely Labeled Molecular Data
  abstract: |-
    In molecular and biological sciences, due to the expensive and time-consuming
    nature of experiments, data sets are often small or scarcely labeled. Although
    transfer learning may help address the challenge of predicting proprties from
    these data sets, it requires the existence of a related large data set, which
    may be difficult to ensure. This poster reviews three graph-based models (utilizing
    Merriman-Bence-Osher (MBO) techniques) that the authors proposed to overcome this
    challenge. The models consist of graph-based modifications of the MBO scheme integrated
    with a home-made transformer, an autoencoder, and an extended-connectivity fingerprint
    algorithm, as well as a consensus method. The authors validated the proposed models
    on five benchmark data sets and provided thorough comparisons to other competing
    methods, such as support vector machines, random forests, and gradient boosting
    decision trees. Residue-similarity (R-S) scores and R-S indices were also used
    to analyze the performance of the proposed methods. Extensive computational experiments
    and theoretical analysis demonstrated that the new models perform very well even
    when as little as 1% of the data set is used as labeled data.
- poster: 140267
  presenter: "He, Huan"
  email: "51e8558ce9947eb0"
  title: "MS94: Tensor Kernel in Causal Inference"
  abstract: |-
    This study introduces a novel approach to causal inference by applying
    tensor decomposition kernels within Support Vector Machines (SVMs) for synthetic
    control. Traditional synthetic control methods often struggle with high-dimensional
    and complex datasets. Our approach uses tensor decomposition to enhance SVM's
    ability to capture multi-dimensional structural relationships, improving the accuracy
    of treatment effect estimates and enabling the detection of interaction effects
    across multiple variables. We validated our method through experiments with both
    simulated and real-world datasets, demonstrating superior precision in causal
    estimates compared to traditional methods. The results confirm the effectiveness
    of tensor decomposition kernels in handling complex causal scenarios, making this
    approach a valuable tool for researchers in various fields such as economics and
    epidemiology. This study advances the application of machine learning techniques
    in causal inference, particularly in settings involving intricate variable interactions.
- poster: 140119
  presenter: "He, Zhiheng"
  email: "897310eb5fbbd6f1"
  title: "Application of Neural Operators to the Phase Field Modeling of Brittle Fractures"
  abstract: |-
    Brittle fracture is a common yet volatile phenomenon that occurs in hard
    substances. Its unpredictability poses challenges in numerous domains, including
    medicine, engineering, and environmental science. Typically, brittle fracture
    is modeled using the finite element method (FEM), which is extremely slow. Recent
    breakthroughs in machine learning have led to the use of convolutional neural
    networks (CNNs) to model brittle fracture. Although a CNN is faster than FEM or
    other conventional methods, its accuracy is limited by its focus on local relationships
    rather than the global relationships that determine fracture paths. In this work,
    we use a Fourier Neural Operator (FNO) to model brittle fracture. This method
    performs spectral convolutions in the frequency domain, thereby allowing the FNO
    to capture the global influence of all cracks and cavities. We trained the FNO
    on 2080 input-output image pairs of size 512 x 512, where the input image is a
    plate containing arbitrarily-shaped cavities, and the output image is the fracture
    path if the plate were to be loaded under tension. After training, the FNO predicted
    fracture path of a previously unseen input in 2 seconds. Compared to FEM, which
    required 25 minutes to solve the same problem, our FNO is 750 times faster. Also,
    we found that the FNO is one-third more accurate than a CNN in predicting the
    correct fracture path. This work shows the potential of FNOs in revolutionizing
    brittle fracture analysis across diverse fields.
- poster: 139749
  presenter: "Hernandez, Hortencia"
  email: "37d55546fc852260"
  title: "Protecting Data Privacy in Social Network Studies Using Synthetic Data"
  abstract: |-
    As network data becomes increasingly integral in research, synthetic data
    are essential aids in understanding social networks and preserving data confidentiality.
    Synthetic data allow for researchers to uncover patterns on the changes of networks
    while maintaining confidentiality. Much of synthetic data generation in categorical
    and mixed data settings is specific to healthcare contexts, where data privacy
    is of utmost importance. Synthetic data generators developed for healthcare data
    offer versatile solutions for preserving privacy in longitudinal social network
    studies. Confidentiality in social network data is important, especially when
    social networks involve sensitive topics, such as risky health behavior or illegal
    activities. Focusing on understanding networks over regularly tested intervals,
    we employ various methodologies, including approximating the joint probability
    distribution as a series of conditional distributions, GAN modeling approaches,
    and a combination of Bayesian networks with variational autoencoders. We will
    also use novel methods to compare the original to the synthetically generated
    data to assess quality of the generated data. With synthetic data, researchers
    can mimic real-world scenarios, enabling the exploration of social dynamics while
    ensuring the protection of individual privacy. This study underscores the transformative
    potential of synthetic data methodologies, emphasizing their role as ethical imperatives
    in social network studies.
- poster: 140249
  presenter: "Hickok, Abigail"
  email: "f5ca61822954b95d"
  title: "MS6: Curvature Estimation for Point Clouds, Graphs, and Finite Metric Spaces"
  abstract: |-
    Real-world data sets frequently turn out to have low-dimensional manifold
    structure that can be studied using ideas from differential geometry. In particular,
    the curvature of a manifold is an important invariant that characterizes the extent
    to which the manifold deviates from being flat. We introduce an estimator for
    the scalar curvature of a data set presented as a finite metric space (e.g., a
    distance matrix, a point cloud, or a graph with the shortest-path metric). Our
    estimator depends only on the metric structure of the data (not on an embedding
    in Euclidean space), and it converges to the ground-truth scalar curvature as
    the number of points increases.
- poster: 139805
  presenter: "Ho, Caitlin"
  email: "45786a5b5be4c6b8"
  title: "Machine Learning for System Identification and Parameter Estimation"
  abstract: |-
    Incorporating a priori physics knowledge into machine learning leads
    to more robust and interpretable algorithms. In this work, we combine deep learning
    techniques with numerical methods to solve two problems in dynamical systems theory:
    dynamics discovery and parameter estimation. We present numerical results of applying
    these proposed approaches to highly oscillatory and chaotic problems. Finally,
    we compare the performance of various numerical schemes, such as Runge-Kutta and
    linear multistep families of methods, in predicting system dynamics and estimating
    physical parameters for these problems.
- poster: 140173
  presenter: "Holtz, Chester"
  email: "f89d7a9cf0da8610"
  title: "Continuous Partitioning for Graph-Based Semi-Supervised Learning"
  abstract: |-
    Laplace learning algorithms for graph-based semi-supervised learning have
    been shown to be degenerate in the low label-regime and thus yield poor performance
    at low label rates and in imbalanced class settings. Furthermore, the performance
    of these algorithms is sensitive to post-hoc decision rules that map continuous
    predictions to labels. We propose a framework for graph-based semi-supervised
    learning based on continuous nonconvex quadratic programming which provably obtains
    integer solutions. Our approach, CutSSL, is motivated by an exact quadratic relaxation
    of a cardinality-constrained minimum-cut graph partitioning problem, a nondegenerate
    problem at low label rates. Furthermore, we show our formulation is related to
    an optimization problem whose approximate solution is the mean-shifted Laplace
    learning heuristic, thus providing new insight into the improved performance of
    this heuristic over Laplace learning at low label rates.  We introduce a scalable
    algorithm based on ADMM and demonstrate that CutSSL surpasses the current state-of-the-art
    in node classification tasks on k-nearest neighbor graphs derived from classic
    image classification benchmarks and large real-world citation and product networks
    across a variety of label rates, class imbalance, and label imbalance regimes.
- poster: 140526
  presenter: "Hong, YoungJoon"
  email: "263984634b22e2f8"
  title: |-
    MS96: Simple Arithmetic Operation in Latent Space Can Generate a Novel Three
    Dimensional Graph Metamaterials
  abstract: |-
    Recent advancements in AI-based design strategies for metamaterials have
    revolutionized the creation of customizable architectures across nano- to macro-scale
    dimensions, achieving mechanical behaviors surpassing the inherent properties
    of constituent materials. However, these methods' growing complexity challenges
    the generation of diverse metamaterials without significant human and computational
    resources, hindering adoption. Addressing this, our study introduces a design
    strategy generating three-dimensional graph metamaterials using simple arithmetic
    operations in latent space. By integrating hidden representations of disentangled
    latent space and latent diffusion processes, our approach comprehensively understands
    complex design spaces, generating diverse graph metamaterials. This versatile
    methodology creates structures from repetitive lattice structures to functionally
    graded mechanical metamaterials and serves as an inverse design strategy for diverse
    lattice structures, including crystalline and trabecular bone structures. This
    foundational step advances comprehension of the intricate latent design space,
    potentially establishing a unified model for traditional generative models in
    mechanical metamaterials.
- poster: 139717
  presenter: "Hosseinmardi, Shirin"
  email: "7d8414b105370aa3"
  title: |-
    Mitigating Spectral Bias of Neural PDE Solvers by Parametric Grid Convolutional
    Encoding
  abstract: "Deep Neural Networks (DNNs) are increasingly utilized to solve partial differential equations (PDEs) for modeling physical phenomena. However, these networks often exhibit decreased accuracy with complex PDEs and suffer from spectral bias, favoring lower-frequency solution components. To overcome these limitations, we introduce a novel encoder-decoder architecture featuring Parametric Grid Convolutional Attention Networks (PGCANs). PGCANs employ a parametric grid convolutional encoder to map the input space to a structured high-dimensional feature space and facilitate information propagation from boundaries to domain interiors, followed by a DNN decoder that incorporates attention mechanisms to prioritize essential features. Our model benefits from a gradient-based technique to balance loss terms during training and can handle irregular 3D domain geometries despite using cartesian grids. We perform Power Spectral Density (PSD) analysis on error profiles to quantify spectral bias for different models, showcasing PGCAN’s capability to learn frequency components more evenly across the spectrum. We also benchmark our model against competing models and demonstrate PGCANs’ superior accuracy in various PDEs, including complex systems like Navier-Stokes, without relying on labeled data, offering a robust alternative to conventional solvers."
- poster: 139901
  presenter: "Hou, Jianguo"
  email: "29309593b88ee52a"
  title: |-
    Applying Machine Learning to Continuous Monitoring Data for Glucose Level
    Forecasting in Septic Patients
  abstract: |-
    We present a comparative study to evaluate the accuracy of predictive
    models for forecasting glucose levels in septic patients with continuous monitoring
    data acquired from bedside devices. The study compares four state-of-the-art machine
    learning models based on transformer technologies and one dynamic linear (DLinear)
    model. Our findings demonstrate the superiority of the DLinear and PatchTST models
    in forecasting glucose levels for septic patients. These two models exhibit outstanding
    predictive accuracy for 15-minute forecasts ( MMPE$\approx 3%$ in PatchTST), excellent
    performance for 30-minute forecasts (MMPE $\approx 7.8%$ in DLinear), and clinically
    acceptable predictions up to 60 minutes ahead (MMPE $\approx 14.4%$ in DLinear),
    based on the prior 30 minutes of continuously acquired glucose data. Between these
    two competing models, the DLinear model delivers slightly superior performance.
    These predictive tools have the potential to be further refined and integrated
    into future digital twins for personalized patient care management. By leveraging
    the capabilities of machine learning and continuous monitoring data, our work
    paves the way for improved glycemic control and enhanced clinical outcomes in
    critically ill patients with diabetes mellitus.
- poster: 140244
  presenter: "Houssou, Kodjo"
  email: "fd4e5fc30b84babb"
  title: |-
    Convergence Rates for Poisson Learning to a Poisson Equation with Measure
    Data
  abstract: |-
    Graph-based learning is a field within machine learning that uses similarities
    between datapoints to create efficient representations of high-dimensional data
    for tasks like semi-supervised classification, clustering, and dimension reduction.
    Poisson learning was recently proposed for graph-based semi-supervised learning
    problems with very few labeled examples, where the widely used Laplacian regularization
    performs poorly. In contrast to Laplacian regularized learning, where labels are
    represented as Dirichlet boundary conditions, Poisson learning encodes the labels
    as point sources and sinks in a graph Poisson equation. In this work, we prove
    quantitative convergence rates for discrete to continuum convergence for Poisson
    learning. The problem is challenging since the source term is measure-valued in
    the continuum, and the continuum Poisson equation does not admit a variational
    interpretation. This work gives a rigorous mathematical justification for using
    Poisson learning for semi-supervised learning at low label rates.
- poster: 139952
  presenter: "Hoyos, Paulina"
  email: "c5a57b797fe2995f"
  title: |-
    MS50: Manifold Learning in the Presence of Symmetries: the G-Invariant Graph
    Laplacian
  abstract: |-
    Graph-based manifold learning algorithms assume that data lie on or near
    a d-dimensional manifold M embedded in some high-dimensional Euclidean space;
    by using a kernel function to measure pairwise affinities of data points, such
    algorithms construct a graph Laplacian matrix from the data, whose eigenvalues
    and eigenvectors are then used for tasks such as dimensionality reduction, function
    representation and approximation, and denoising. In this work, we consider data
    sets whose data points satisfy an additional symmetry invariance assumption: given
    a compact Lie group G, for any g in G and data point x, the point gx resulting
    from the action of g on x is a valid data point (which is not necessarily in the
    data set, but can be added to it). A possible approach for exploiting this symmetry
    invariance of data is to construct a G-invariant graph Laplacian (G-GL) by analytically
    incorporating the pairwise affinities between all the pairs of points generated
    by the action of G on the data set. The G-GL converges to the Fokker-Planck operator
    on the data manifold M, with a significantly improved convergence rate compared
    to the standard graph Laplacian.
- poster: 139770
  presenter: "Hozumi, Yuta"
  email: "5baf577f64208adf"
  title: "MS38: K-Mer Topology for Whole Genome Analysis"
  abstract: |-
    DNA is the building block of all life. Similarities and differences between
    DNA sequences provide insights into evolutionary relationships, mutations, genetic
    drifts, genome assembly, gene annotation, and more. Having a robust and scalable
    method that allows for comparisons within species and across species is essential
    for understanding the foundation of biology. In this poster, we present k-mer
    topology, an alignment-free sequence analysis method. Utilizing tools from topological
    data analysis, such as persistent homology, k-mer patterns are extracted from
    nucleotide sequences. Then, we define a metric on the k-mer topology features,
    which is utilized for phylogenetic analysis and viral classification.
- poster: 140124
  presenter: "Hozumi, Yuta"
  email: "5baf577f64208adf"
  title: |-
    MS51: Analyzing Single Cell Rna Sequencing with Topological Nonnegative
    Matrix Factorization
  abstract: |-
    Single-cell RNA sequencing (scRNA-seq) is a relatively new technology
    that has stimulated enormous interest in statistics, data science, and computational
    biology due to the high dimensionality, complexity, and downstream analysis. Due
    to the high dimensionality of the original data, dimensionality reduction is often
    employed as the first step. Nonnegative matrix factorization (NMF), in particular,
    offers a unique approach due to its interpretation of resulting low-dimensional
    components as meta-genes. Extensions, such as manifold regularization, have been
    utilized for scRNA-seq. However, such regularization only captures the data at
    a single scale and lacks multiscale consideration. In this poster, we introduce
    persistent Laplacian regularized NMF, which captures the geometrical and topological
    information of the data at multiscale. Similar to persistent homology, a tool
    from topological data analysis, persistent Laplacian captures the topological
    invariants, namely the Betti numbers, as well as the homotopic shape of evolution.
    We validated our approach on real scRNA-seq data.
- poster: 140543
  presenter: "Hsu, Alexander Win"
  email: "95bb81295a61f9b2"
  title: "MS42: Approximating Measures on Function Spaces: Transport and Truncation"
  abstract: |-
    We study the problem of computing approximations to probability measures
    on infinite dimensional spaces in the context of Bayesian inverse problems. We
    examine truncations of the likelihood and posterior that enable practical computation
    while preserving the structural properties of the prior. The error analysis for
    these approaches can be understood from the perspective of the perturbation analysis
    of Bayesian inverse problems, providing rigorous convergence guarantees and mesh
    invariance. Further, we examine the interplay between truncation and transport,
    elucidating interesting and useful structures that appear for the transport map
    from prior to posterior when the action of the likelihood is restricted to a finite
    dimensional subspace--these results are analogous to the representer theorem for
    Hilbert spaces but in a stochastic setting.
- poster: 140035
  presenter: "Hu, Mengqi"
  email: "f0239ca19ce53d21"
  title: |-
    MS49: Analyzing Bold Signals with Neural Networks and the Koopman Operator
    for Neurodegenerative Disease Study
  abstract: |-
    This study explores the application of neural networks (NNs) and the Koopman
    operator in the analysis of Blood Oxygen Level Dependent (BOLD) signals derived
    from functional Magnetic Resonance Imaging (fMRI). We propose a novel methodology
    that leverages NNs to learn the eigenmodes of various brain features and behaviors,
    which are critical in the characterization of neurodegenerative diseases. By integrating
    the predictive power of deep learning with the dynamical systems perspective offered
    by the Koopman operator, our approach not only enhances the understanding of BOLD
    signals but also holds potential for early identification and study of neurodegenerative
    conditions. Preliminary results demonstrate the efficacy of our method in capturing
    significant patterns that correlate with the progression of these diseases, promising
    a new avenue for neuroscientific research and clinical diagnosis.
- poster: 140041
  presenter: "Hu, Rui"
  email: "f99f967cd79a9da3"
  title: "MS62: Federated Learning with Differential Privacy and More"
  abstract: "In the burgeoning field of machine learning, federated learning (FL) has emerged as a pivotal paradigm for training models directly on distributed local datasets without centralizing them, thus mitigating privacy leakage. However, challenges remain in ensuring that the machine learning process itself does not leak sensitive information. This talk explores the integration of differential privacy within FL to address these challenges. We will discuss the principles of differential privacy and its implementation techniques that help protect participants’ local datasets during the FL process. Further, we will introduce recent advancements in this approach, highlighting their effect on the trade-off between model accuracy and privacy loss."
- poster: 140078
  presenter: "Huan, Stephen"
  email: "8f4135a89118baf"
  title: "MS9: Möbius Inversion Meets Tensors: Inference by Edgeworth Series"
  abstract: "Spatial statisticians, physicists, and data scientists all make  Gaussian approximations of non-Gaussian phenomena of interest.  Statistical inference in Gaussian random variables is highly tractable  as operations like conditioning, evaluation of the (log-)likelihood, and  sampling all involve numerical computation with the covariance matrix  which can be accelerated with the Cholesky factor.  We generalize these results to non-Gaussian random  variables through the formalism of measure transport.  The Cholesky factor becomes the Knothe-Rosenblatt rearrangement, a unique  transport map whose Jacobian is lower triangular with positive diagonal.  In the non-Gaussian case, we cannot hope to match  only the mean and covariance, as in Gaussians.  Instead, we will use higher-order cumulant tensors to characterize  probability distributions, which can be estimated directly from samples.  Expansions based on cumulants such as the Edgeworth series and  Cornish-Fisher expansion give simple polynomial expressions for approximate  densities, log-densities, and transport maps through Möbius inversion.  We can also estimate functionals of the  distribution such as entropy directly from samples."
- poster: 139863
  presenter: "Huang, Yinan"
  email: "9236117efc0e8e6"
  title: "MS11: On the Stability of Expressive Positional Encodings for Graphs"
  abstract: "Designing effective positional encodings for graphs is key to building powerful graph transformers and enhancing message-passing graph neural networks. Although widespread, using Laplacian eigenvectors as positional encodings faces two fundamental challenges: (1) Non-uniqueness: there are many different eigendecompositions of the same Laplacian, and (2) Instability: small perturbations to the Laplacian could result in completely different eigenspaces, leading to unpredictable changes in positional encoding. Despite many attempts to address non-uniqueness, most methods overlook stability, leading to poor generalization on unseen graph structures. We identify the cause of instability to be a “hard partition” of eigenspaces. Hence, we introduce Stable and Expressive Positional Encodings (SPE), an architecture for processing eigenvectors that uses eigenvalues to “softly partition” eigenspaces. SPE is the first architecture that is (1) provably stable, and (2) universally expressive for basis invariant functions whilst respecting all symmetries of eigenvectors. Besides guaranteed stability, we prove that SPE is at least as expressive as existing methods, and highly capable of counting graph structures. Finally, we evaluate the effectiveness of our method on molecular property prediction, and out-of-distribution generalization tasks, finding improved generalization compared to existing positional encoding methods. Our code is available at https://github.com/Graph-COM/SPE."
- poster: 139843
  presenter: "Huang, Bill"
  email: "ee9c23e1df62dc37"
  title: |-
    MS41: Unsupervised Solution Operator Learning for Mean-Field Games Via Sampling-Invariant
    Parametrizations
  abstract: ""
- poster: 139951
  presenter: "Huang, Jisui"
  email: "304f201b6192205f"
  title: "MS19: 3D Manifold Topology Based Medical Image Data Augmentation"
  abstract: |-
    Data augmentation is an effective and universal technique for improving
    the generalization performance of deep neural networks. Current data augmentation
    implementations usually involve geometric and photometric transformations. However,
    none of them considers the topological information in images, which is an important
    global invariant of the three-dimensional manifold. In our implementation, we
    design a novel method that finds the generator of the first homology group, i.e.
    closed loops cannot shrink to a point, of 3D image and erases the bounding box
    of a random loop. To the best of our knowledge, it is the first time that data
    augmentation based on the first homology group of the three-dimensional image
    is applied in medical image augmentation. Our numerical experiments demonstrate
    that the proposed approach outperforms the state-of-the-art method.
- poster: 139190
  presenter: "Huang, Xiangming"
  email: "4f828b9b413200b6"
  title: "MS65: Towards Optimization of Biological Systems Leveraging Pde Model Discovery"
  abstract: |-
    Deep learning has transformed computational physics. Physics-enhanced
    deep surrogate models and physics-informed neural networks (PINNs) offer new solutions
    for complex physical problems. These models use machine learning to approximate
    physical systems, incorporating known physical laws to enhance accuracy and generalization.
    They can outperform conventional neural networks in accuracy, require less training
    data, and are faster than high-fidelity numerical solvers.  Single-cell and spatial
    omics technologies are revolutionizing our understanding of biological systems.
    They allow for high-resolution analysis of individual cells and the spatial organization
    of tissues. These technologies illuminate the intricate dynamics of biological
    systems, from organ development to disease treatment.  There are emerging synergies
    between these fields. Biological systems are complex and data-scarce but well-studied.
    The methodologies developed in computational physics are well-suited for such
    situations. The cross-pollination between these fields promises to advance our
    understanding of both machine learning and biological systems.
- poster: 139830
  presenter: "Huang, Juntao"
  email: "b695becd9cbc8230"
  title: |-
    MS70: On the Rotational Invariance and Hyperbolicity of Shallow Water Moment
    Equations for Free Surface Flows
  abstract: |-
    In this poster, we introduce our work on the incompressible Navier-Stokes
    equations with free surfaces. We explore a novel class of models that describe
    this system, traditionally modeled using shallow water equations. We demonstrate
    the rotational invariance and hyperbolicity of these newly-derived shallow water
    moment models.
- poster: 139776
  presenter: "Imura, Hiroki"
  email: "fb733ea6cf4bae05"
  title: |-
    MS9: Virtual X-Ray Diffraction Signals in Simulations of Dislocated Crystals:
    a Data Generation Pipeline
  abstract: "The relationship between dislocations—the line defects responsible for plastic deformation in metals—and the macroscopic behavior of a material is complex. While machine learning would lend itself to identifying relationships between the dislocation configuration and macroscopic responses of the crystal, this would require three components: 1) an ‘output’ macroscopic response which is determined by the dislocation configuration, 2) an ‘input’ metric of the dislocation configuration which is in some way causally linked to this macroscopic response, 3) a training dataset encompassing a variety of dislocation configurations and their respective macroscopic response. An existing library of line bundle dislocation dynamics simulation could serve as an appropriate dataset if paired with a post-processing pipeline which could extract from this data the macroscopic response and dislocation configuration metrics. This poster will present one such pipeline. Simulation of the virtual response of the simulated domain in an X-ray diffraction experiment will serve as the macroscopic response, with the two-point statistic of the Kröner-Nye tensor (a dislocation density) serving as the input metric. Typical time series behavior of both will be discussed."
- poster: 139879
  presenter: "Izquierdo Lehmann, Pedro"
  email: "b93197baa1af913a"
  title: "MS83: Primal-Dual Hybrid Gradient for Degenerate Partially Smooth Problems"
  abstract: ""
- poster: 140000
  presenter: "Janik, Konrad"
  email: "4732f4c17ad0cfe1"
  title: "Parametric SympNets"
  abstract: |-
    The numerical integration of Hamiltonian systems is a central topic in computational physics and theoretical chemistry. SympNets [P. Jin, Z. Zhang, A.Zhu, Y. Tang, and G. E. Karniadakis., SympNets: intrinsic structure-preserving symplectic networks for identifying Hamiltonian system, Neural Networks, 132:166–179, dec 2020] are a widely used tool when it comes to learning symplectic integrators for Hamiltonian systems from data. More often then not Hamiltonian systems of interest depend on a set of parameters. Currently, SympNets do not respect parameters. Therefore, SympNets have to be retrained for every new parameter combination. To eliminate this necessity, we propose to extend the SympNet architecture to handle parameters explicitly, making it suitable for parametric Hamiltonian systems. This approach is coined "ParSympNets". We perform different numerical experiments to investigate the approximation and generalization capabilities of ParSympNets. Furthermore, we present a new universal approximation theorem, which shows that ParSympNets can approximate arbitrary parametric symplectic maps.
- poster: 140304
  presenter: "Jarvis, Tyler"
  email: "be44e33f8fe3f931"
  title: "Double Descent and Generalized Aliasing"
  abstract: "A central problem in data science is to use noisy samples of an unknown function to predict values for unseen inputs.  Traditionally, this error is estimated by using hold-out data from a training procedure and observing the accuracy of this test set.  In classical statistics the predictive error for different model classes is seen as a tradeoff between the bias and the variance that balances model simplicity with its ability to fit complex functions.  However, in modern machine learning, over-parameterized models often exhibit “double descent' in which models of increasing complexity exhibit decreasing generalization error.  We present an alternative paradigm to the bias/variance tradeoff that we call the aliasing/invertibility decomposition.  We explain double descent as a systematic “de-aliasing' that occurs in over-parameterized models.  In the limit of large models, the contribution due to aliasing vanishes, leaving an expression for the asymptotic total error as the invertibility failure of infinitely large models on (finite) training points.  Our decomposition is in terms of general operators acting on a representation, so that the decomposition can be explicitly calculated without seeing any data.  This enables us to answer questions related to experimental design and model selection before collecting data or performing experiments."
- poster: 140090
  presenter: "Jayakumar, Abhijith"
  email: "38b0f1507a690513"
  title: |-
    MS68: Learning While Stuck: Reconstructing Energy-Based Models from Metastable
    States.
  abstract: "In this work we rigorously show that energy based models can be learned efficiently from samples drawn from a class of metastable states of reversible Markov chain samplers. This holds even when these states are far away from the true  model in terms of global metrics like KL divergence, energy, or other order parameters. We establish that metastable states that satisfy a strong metastability condition can approximate the true model well in single variable conditionals. This allows us to learn the true model using methods like Pseudo-Likelihood, even when the samples come from a metastable state concentrated in a small region of the state space. Explicit examples of such meta-stable states can be constructed from regions that effectively bottleneck the probability flow and cause poor mixing of the Markov chain. We show several numerical examples where the correct energy function is learned from samples drawn from a Markov chain’s metastable state, well before the chain mixes to the desired model.  This gives an explicit example of an ML problem where learning succeeds despite having the “wrong' data but the right hypothesis. Our work also opens the possibility of learning or verifying energy based deep-learning models in cases where the true distribution is hard to sample from."
- poster: 140524
  presenter: "Jedynak, Bruno"
  email: "b83a0e8a09c56299"
  title: |-
    MS16: Learning Differential Equations from Data: Evaluating the Occupation
    Kernel Algorithm Using the Common Task Framework of the Ai Institute for Dynamical
    Systems
  abstract: "As essential tools of statistical learning theory, kernel methods exploit the power of reproducing kernel Hilbert spaces (RKHS). Kernel methods excel in modeling nonlinear relationships by implicitly mapping data into high-dimensional or infinite feature spaces. This work employs occupation kernel (OCK) functions to learn vector fields driving ordinary differential equations (ODEs). The OCK algorithm captures snapshots of ODE trajectories as data-points, recovering the vector field governing the ODE and employing penalized least squares. The solutions are expressed using OCK functions and RK4 numerical integration. We evaluate the OCK algorithm using the Common Task Framework (CTF) of the AI Institute for Dynamical Systems [1], with model training on six datasets using various Matérn kernels. Bayesian optimization optimizes hyper-parameters before employing the OCK algorithm for prediction. Performance is assessed using the CTF forecasting and reconstruction scores. The OCK algorithm performs competitively in other contexts [2], and the CTF allows for further evaluation of this algorithm. [1] \\texttt{https://maths4dl.ac.uk/wp-content/uploads/2024/04/Kutz_CTF.pdf} [2] Lahouel, K., Wells, M., Rielly, V., Lew, E., Lovitz, D., and Jedynak, B.M., 2024. Learning nonparametric ordinary differential equations from noisy data. Journal of Computational Physics, p.112971"
- poster: 139997
  presenter: "Jerez-Hanckes, Carlos"
  email: "6efb4afb907ac61"
  title: "Uncertainty Quantification in Electromagnetics Via Neural Networks"
  abstract: |-
    Areas ranging from biomedical imaging and energy harvesting to remote
    sensing and space exploration benefit from the  ever-increasing computational
    power to simulate (EM) electromagnetic phenomena. Moreover, due to growingly stringent
    performance requirements for these applications, prototyping and design tools
    must account for uncertainties arising from data measurement, manufacturing tolerance
    or operation conditions, to name a few. Thus, the need to develop efficient numerical
    methods to solve partial differential equations (PDEs) or boundary integral equations
    (BIEs) with uncertain input data and solutions.   In this work, we present mathematically
    rigorous EM surrogates (low-fidelity) models via neural networks (NNs) and apply
    them to solving infinite-parametric EM models arising when performing uncertainty
    quantification.
- poster: 139671
  presenter: "Jeter, Russell"
  email: "f6ac02b7e0d44ac1"
  title: |-
    MS44: Stroke Rehabilitation with Machine Learning-Based Residual Severity
    Classification
  abstract: "Stroke therapy is essential to reduce impairments and improve motor movements by engaging autogenous neuroplasticity. This study uses supervised learning methods to address a clinician’s autonomous classification of stroke residual severity labeled data towards improving in-home robotics-assisted stroke rehabilitation. Thirty-three stroke patients participate in in-home therapy sessions using the Motus Nova robotics rehabilitation technology to capture upper and lower body motion. The therapy session summary data is based on high-resolution movement and assistance data and clinician-informed discrete stroke residual severity labels. We demonstrate that the light gradient boosting method provides the most reliable autonomous detection of stroke severity. This method achieved an average of 94% accuracy, measured using the F1-score performance measure with 10-fold cross-validation. We show how objectively measured rehabilitation training paired with machine learning methods can be used to identify the residual stroke severity class with efforts to enhance in-home self-guided, individualized stroke rehabilitation. As data from rehabilitation practices are often of comparable size and nature to the data collected in our study, this suggests that the light gradient boosting method should be considered a standard, more efficient tool for this analysis."
- poster: 140136
  presenter: "Jha, Sumit"
  email: "686bb57e7e686bee"
  title: |-
    MS62: Gradient-Based Methods for Explanations and Confidence Metrics in
    Distributed Learning
  abstract: "Gradient-based methods provide a practical computational approach for obtaining explanations of AI models. Algorithms based on path integrals have been shown to satisfy commonsense axiomatic properties similar to those expected from Shapley values in collaborative game theory. In this talk, we present a new gradient-based method for explaining AI decisions using integrated decision gradients – an adaptive approach to computing path integrals that leads to robust explanation. In particular, the approach resolves the saturation problem of classical integrated gradients by employing an importance factor to focus on the area of the path integral where the model makes its decision. This approach naturally leads to an efficient attribution-based confidence metric for AI models. Finally, we demonstrate how these approaches naturally extend to distributed learning models. The talk will build upon our recent results published at NeurIPS’19, IJCAI’22, AAAI’24, and DAC’24."
- poster: 140238
  presenter: "Jha, Shreya"
  email: "e91f01a70de1438c"
  title: "MS9: Higher Order Cumulant Truncation for Improved Moment Closure"
  abstract: |-
    Moment methods are extensively used to compute the evolution of probability
    distributions over time. These methods can involve nonlinear transformations where
    changes in lower order moments depend on higher order moments, resulting in an
    unclosed system. As a result, a method to express higher order moments in terms
    of lower order ones is required for system closure. A common closure method is
    the Gaussian, which approximates higher order moments using only in terms of the
    first and second cumulants, disregarding other relevant higher order information
    that could have been used. To address this, we propose higher order cumulant truncation
    methods that incorporate higher order information. Although only the Gaussian
    possess a finite number of non-zero cumulants, our approximation leverages the
    property of higher order cumulants decaying faster than lower order ones, suggesting
    its potential effectiveness. To obtain moments that correspond to feasible probability
    distributions, we project our higher order moment approximations onto the moments
    obtained from the set of feasible probability distributions using the set of polynomial
    sum-of-squares. We tested our methods through function approximations on various
    distributions, simulated particle collisions, moment closure applications on the
    Duffing oscillator, and moment closure applications on microbial consortium dynamics.
    We have identified situations where higher order truncation performs better than
    the Gaussian.
- poster: 140246
  presenter: "Ji, Yao"
  email: "d5ec1d7142f83678"
  title: "MS59: Distributed Stochastic Optimization and Sparse Statistical Recovery"
  abstract: "We study statistical estimation of high-dimensional sparse parameters over a network. The estimation problem is formulated as Stochastic Approximation. Assuming agents only have access to unbiased estimates of the gradients of their local cost functions, we propose a composite multistage algorithm; each stage being solved to a prescribed accuracy by the non-Euclidean composite distributed stochastic dual averaging. We show that, with high probability, the iterates generated by each agent linearly converge during the first “preliminary” phase of the routine and is sublinear during the second “asymptotic” phase. Further, we characterize the transient time needed for composite-DSDA to approach the asymptotic convergence rate. Numerical experiments demonstrate the tightness of the theoretical results."
- poster: 139804
  presenter: "Jiang, Hanyang"
  email: "71c9b41b23179147"
  title: |-
    MS28: Conformal Prediction for Multi-Dimensional Time Series by Ellipsoidal
    Sets
  abstract: |-
    Conformal prediction (CP) has been a popular method for uncertainty quantification
    because it is distribution-free, model-agnostic, and theoretically sound. For
    forecasting problems in supervised learning, most CP methods focus on building
    prediction intervals for univariate responses. In this work, we develop a sequential
    CP method called MultiDimSPCI that builds prediction regions for a multivariate
    response, especially in the context of multivariate time series, which are not
    exchangeable. Theoretically, we estimate finite-sample high-probability bounds
    on the conditional coverage gap. Empirically, we demonstrate that MultiDimSPCI
    maintains valid coverage on a wide range of multivariate time series while producing
    smaller prediction regions than CP and non-CP baselines.
- poster: 140142
  presenter: "Jiang, Ning"
  email: "23a30dc4b8005cb2"
  title: "MS45: Neural Network Prediction of Covid-19 Daily Infection Count"
  abstract: |-
    It is well known that the number of confirmed infection cases of COVID-19
    is much lower than the true cases. The undercount ratio (the ratio of true cases
    to confirmed cases) depends on the testing effort. In this poster, we use an artificial
    neural network to uncover the hidden connection between testing data, confirmed
    caseload, and the true caseload. The output of our training set, i.e., the true
    case count, is derived from solving a deconvolution problem using both the death
    count and the infection fatality rate (IFR). Multiple factors, such as age distribution,
    variants, and vaccination, are considered in estimating the time series of IFR.
- poster: 140219
  presenter: "Jiang, Ruoxi"
  email: "3e13a7004f98b9b1"
  title: |-
    MS14: Training Neural Operators to Preserve Invariant Measures of Chaotic
    Attractors
  abstract: |-
    Chaotic systems make long-horizon forecasts difficult because small perturbations
    in initial conditions cause trajectories to diverge at an exponential rate. In
    this setting, neural operators trained to minimize squared error losses, while
    capable of accurate short-term forecasts, often fail to reproduce statistical
    or structural properties of the dynamics over longer time horizons and can yield
    degenerate results. We propose an alternative framework designed to preserve invariant
    measures of chaotic attractors that characterize the time-invariant statistical
    properties of the dynamics. Specifically, in the multi-environment setting (where
    each sample trajectory is governed by slightly different dynamics), we consider
    two novel approaches to training with noisy data. First, we propose a loss based
    on the optimal transport distance between the observed dynamics and the neural
    operator outputs. This approach requires expert knowledge of the underlying physics
    to determine what statistical features should be included in the optimal transport
    loss. Second, we show that a contrastive learning framework, which does not require
    any specialized prior knowledge, can preserve statistical properties of the dynamics
    nearly as well as the optimal transport approach. In various chaotic systems,
    our method is shown empirically to preserve invariant measures of chaotic attractors.
    This poster is associated with the minisymposium Probabilistic Methods in Machine
    Learning and Complex Systems.
- poster: 139842
  presenter: "Jin, Yijie"
  email: "b96996cbbf207ad"
  title: |-
    MS41: Parametrized Wasserstein Gradient Flow with Application in Porous-Medium
    Equation
  abstract: ""
- poster: 139867
  presenter: "Jin, Yijie"
  email: "b96996cbbf207ad"
  title: "MS35: Parameterized Equations for Nongradient System on Wasserstein Manifold"
  abstract: |-
    The Wasserstein manifold presents a powerful framework for modeling probability
    distributions and their evolution over time. In this poster, we introduce a novel
    approach for analyzing nongradient systems on the Wasserstein manifold using parameterized
    equations. Our method leverages the geometric structure of the Wasserstein space
    to develop a set of equations that capture the dynamics of these systems without
    relying on gradient information.
- poster: 140068
  presenter: "Jones, Benjamin"
  email: "99151c7554d9aee4"
  title: "MS38: A Persistent Directed Flag Laplacian"
  abstract: |-
    Topological data analysis (TDA) has had enormous success in science and
    engineering in the past decade. Persistent topological Laplacians (PTLs) overcome
    some limitations of persistent homology, a key technique in TDA, and provide substantial
    insight to the behavior of various geometric and topological objects. This work
    extends PTLs to directed flag complexes, which are an exciting generalization
    to flag complexes, also known as clique complexes, that arise naturally in many
    situations. We introduce the directed flag Laplacian and show that the proposed
    persistent directed flag Laplacian (PDFL) is a distinct way of analyzing these
    flag complexes. Example calculations are provided to demonstrate the potential
    of the proposed PDFL in real world applications.
- poster: 139639
  presenter: "Ju, Lili"
  email: "e1a620adbe5b009e"
  title: |-
    MS28: A Deep Learning Method for the Dynamics of Classic and Conservative
    Allen-Cahn Equations Based on Fully-Discrete Operators
  abstract: "The Allen-Cahn equation is a well-known stiff semilinear parabolic equation used to describe the process of phase separation and transition in phase field modeling of multi-component physical systems, while the conservative Allen-Cahn equation is a modified version of the classic Allen-Cahn equation that can additionally conserve the mass. In this work, we present a novel deep learning method for predicting the dynamics of the classic and conservative Allen-Cahn equations. Specifically, we design two special convolutional neural network models, one for each of the two equations, to learn the fully-discrete operators between two adjacent time steps.  The loss functions of the two models are defined using the residual of the fully-discrete systems, which result from applying the central finite difference discretization in space and the Crank–Nicolson approximation in time.  This approach enables us to train the models without requiring any ground-truth data. Moreover, we introduce an effective training strategy that automatically generates useful samples along the time evolution to facilitate training of the models. Finally, we conduct extensive experiments in two and three dimensions to demonstrate outstanding performance of our proposed method, including its dynamics prediction and generalization ability under different scenarios."
- poster: 139972
  presenter: "Kalra, Dayal Singh"
  email: "e0df963d17a33725"
  title: |-
    MS59: Universal Sharpness Dynamics in Neural Network Training: Fixed Point
    Analysis, Edge of Stability, and Route to Chaos
  abstract: |-
    In gradient descent dynamics of neural networks, the top eigenvalue of
    the Hessian of the loss (sharpness) displays a variety of robust phenomena throughout
    training.   This includes early time regimes where the sharpness may decrease
    during early periods of training (sharpness reduction), and later time behavior
    such as progressive sharpening and edge of stability.   We demonstrate that a
    simple $2$-layer linear network (UV model) trained on a single training example
    exhibits all of the essential sharpness phenomenology observed in real-world scenarios.   By
    analyzing the structure of dynamical fixed points in function space and the vector
    field of function updates, we uncover the underlying mechanisms behind these sharpness
    trends.   Our analysis reveals (i) the mechanism behind early sharpness reduction
    and progressive sharpening, (ii) the required conditions for edge of stability,
    and (iii) a period-doubling route to chaos on the edge of stability manifold as
    learning rate is increased.   Finally, we demonstrate that various predictions
    from this simplified model generalize to real-world scenarios and discuss its
    limitations.
- poster: 140047
  presenter: "Kan, Kelvin"
  email: "1f59a7ffd94d451d"
  title: "MS22: Multivariate Quantile Function Forecaster"
  abstract: |-
    We propose Multivariate Quantile Function Forecaster (MQF${}^2$), a global
    probabilistic forecasting method constructed using a multivariate quantile function
    and investigate its application to multi-horizon forecasting. Prior approaches
    are either autoregressive, implicitly capturing the dependency structure across
    time but exhibiting error accumulation with increasing forecast horizons, or multi-horizon
    sequence-to-sequence models, which do not exhibit error accumulation, but also
    do typically not model the dependency structure across time steps. MQF${}^2$ combines
    the benefits of both approaches, by directly making predictions in the form of
    a multivariate quantile function, defined as the gradient of a convex function
    which we parametrize using input-convex neural networks. By design, the quantile
    function is monotone with respect to the input quantile levels and hence avoids
    quantile crossing. We provide two options to train MQF${}^2$: with energy score
    or with maximum likelihood. Experimental results on real-world and synthetic datasets
    show that our model has comparable performance with state-of-the-art methods in
    terms of single time step metrics while capturing the time dependency structure.
- poster: 139188
  presenter: "Kang, Dayoung"
  email: "d0643caeebff833e"
  title: |-
    MS65: Multifidelity Linear Regression for Scientific Machine Learning from
    Scarce Data (Poster)
  abstract: |-
    Machine learning (ML) methods have garnered significant interest as potential
    methods for learning surrogate models for complex engineering systems for which
    traditional simulation is expensive. However, in many scientific and engineering
    settings, training data are scarce due to the cost of generating data from traditional
    high-fidelity simulations. ML models trained on scarce data have high variance
    and are sensitive to vagaries of the training data set. We propose a new multifidelity
    training approach for scientific machine learning that exploits the scientific
    context where data of varying fidelities and costs are available; for example
    high-fidelity data may be generated by an expensive fully resolved physics simulation
    whereas lower-fidelity data may arise from a cheaper model based on simplifying
    assumptions. We use the multifidelity data to define new multifidelity Monte Carlo
    estimators for the unknown parameters of linear regression models, and provide
    theoretical analyses that guarantee accuracy and improved robustness to small
    training budgets. Numerical results show that multifidelity learned models achieve
    order-of-magnitude lower expected error than standard training approaches when
    high-fidelity data are scarce.
- poster: 139666
  presenter: "Karanam, Venkat Sai Suman Lamba"
  email: "4402898c5ad14208"
  title: |-
    Looking Beyond Geometric and Probability-Based Distances for Multiclass Classification
    in Network Communication Systems
  abstract: "Uncertainty quantification and outlier detection in input datasets is fundamental to improve the efficiency of the multiclass classification problem. Anomaly detection in network communication systems uses multiclass classification algorithms extensively. Outliers and anomalies present themselves as “noise” in training data and hence can have a disproportionate impact on accuracy. At the same time, if the outliers (or noise) follow the same probability distribution in both train and test datasets, they can go undetected while still impacting the accuracy. Both Geometric distances and Probability distribution-based distances have been used to detect outliers in the network communication datasets-each with its own merits and drawbacks. In this work, we study three distinct classes of distance measures for outlier/uncertainty detection, namely, (1) Geometric, (2) Probability distribution-based, and (3) Optimal Transport (OT)-based distance called Wasserstein distance. Our problem is set in the anomaly and intrusion detection area of network communication systems. We employ each of these distance measures as cost functions in multiclass classification and clustering problem, aimed at detecting anomalies. We also investigate a combined formulation of the distance measures and present use cases where they may outperform individual distance measures."
- poster: 140241
  presenter: "Karris, Nicholas"
  email: "54db87f3bb561063"
  title: |-
    MS26: Using Linearized Optimal Transport to Predict the Movement of Stochastic
    Particle Systems
  abstract: |-
    Many problems involve systems of particles evolving over time, typically
    with the individual particles moving quickly and chaotically. In many applications,
    however, the behavior of interest is in the overall distribution of particles,
    which often evolves much slower and more smoothly. To model and predict this slow
    behavior, we assume the particles form an empirical estimate of a time-dependent
    distribution and that the evolution of this distribution is smooth. This motivates
    the use of Linearized Optimal Transport to embed these distributions in an L2-space
    in order to compute a derivative, and we use this derivative in an Euler-like
    scheme to predict the distribution's movement. We show that this derivative is
    exactly the velocity flow vector field which describes the distribution's evolution,
    and we show that the Euler-like scheme using this vector field gives a prediction
    which is first-order accurate in Wasserstein distance. By computing a discrete
    optimal transport map across a sufficiently long time step, we can ignore the
    stochasticity of the particles and approximately recover this vector field using
    finite differences. We then develop a prediction scheme which takes an Euler step
    using this derivative estimate, and we demonstrate its effectiveness on a few
    example stochastic particle systems.
- poster: 139758
  presenter: "Katsidoniotaki, Eirini"
  email: "d8d82b0467e76a3f"
  title: "Learning Models for Real-World Dynamical Systems from Multifidelity Data"
  abstract: |-
    Accurate modeling of dynamical systems is crucial for supporting operations
    and decision-making in real-world applications. However, obtaining high-fidelity
    (HF) data for training these models is often challenging. Conversely, more accessible
    low-fidelity (LF) data may not adequately capture complex nonlinear dynamics.
    We present a multifidelity (MF) GPR framework, integrating nonlinear autoregressive
    schemes to exploits correlation across heterogeneous data sources, allowing for
    the implicit derivation of functional relationships and uncertainty quantification
    in predictions. We specifically apply and evaluate this framework for predicting
    the dynamics of a floating structure in maritime environment. The data comprises
    both LF numerical simulations and HF in-situ sensor measurements. The initial
    step of our MF framework involves developing a GPR surrogate model trained on
    LF data, efficiently replacing  time-consuming numerical solvers. Given the high
    dimensionality of this data, we apply PCA for dimension reduction, which may result
    in information loss. To assess this impact, we conducted a comparative study between
    the PCA-based GPR model and a GCN model that processes multidimensional data without
    reduction. Our findings suggest that while PCA introduces errors, our framework
    effectively corrects these inaccuracies, ensuring reliable predictions. The MF
    GPR model captures intricate nonlinear behaviors and aligns closely with noisy
    sensor data.
- poster: 139638
  presenter: "Katz, Joshua"
  email: "869ed7912d4da64f"
  title: "MS95: Generic Orbit Recovery from Invariants of Very Low Degree."
  abstract: |-
    Motivated by the multi-reference alignment problem and questions in equivariant
    neural networks we study the problem of recovering the generic orbit in a unitary
    representation of a compact group from invariant tensors of degree three or less.  We
    explore the similarities and differences between the descriptive power of low
    degree polynomial and unitary invariant tensors and provide some evidence that
    in many cases of interest they have similar descriptive power. In particular we
    prove that for the regular representation of a finite group polynomial invariants
    of degree at most three separate generic orbits. We also investigate these questions
    for subregular representations of finite groups and prove that for the defining
    representation of the dihedral group, polynomial invariants of degree three and
    less separate generic orbits.
- poster: 139767
  presenter: "Ke, Hongyu"
  email: "3fe61c1230110599"
  title: "MS22: Mambev: Vision State Space Models for Bird’s-Eye-View Generation"
  abstract: |-
    Camera-only 3D perception attracts a large amount of attention for autonomous
    driving systems from academia and industry. CNNs and Transformers have cemented
    themselves as the backbones of choice for processing vision data in Birds-Eye
    View (BEV) object detection tasks. CNNs excel in quick and lightweight feature
    extraction while Transformer based architectures have shown powerful results in
    improving mAP across the board, however neither architecture is well equipped
    to process temporal information in long context settings with low memory, a critical
    aspect of the problem required for in usage in vehicles. In this work, we present
    a state space model based framework to learn unified BEV representations with
    the spatiotemporal transformers in a top-down manner to support multiple autonomous
    driving perception downstreaming tasks. In a nutshell, we introduce a novel approach
    to BEV using MAMBA, a recent development in state space models, which offers a
    significant improvement in both throughput speed and memory usage over transformers
    while offering a longer context window which can accommodate sequences of arbitrary
    length.
- poster: 140039
  presenter: "Kelly, Conlain"
  email: "b4a5e2f56bcf725"
  title: "MS9: Hybrid Data-Driven Solvers for Statistical Continuum Mechanics"
  abstract: |-
    In this work we propose a new class of data-driven models which predict
    local deformations over heterogeneous material microstructures. This problem --
    termed the localization problem -- is a core component of numerous open challenges
    in multiscale materials design. The inherent stochasticity of manufacturing means
    that constructing forward (process -> property) or inverse (property -> process)
    UQ models requires solving a variable-coefficient PDE repeatedly for many microstructure
    instantiations. Moreover, the discontinuity and disorder of the PDE coefficients
    leads to a high-dimensional, poorly-conditioned system of equations. Our work
    combines traditional numerical solvers and data-driven methods to construct a
    hybrid approximation for the coefficient-to-solution map. In particular, we utilize
    the Lippmann-Schwinger formulation of localization to guide the design of a thermodynamically-informed
    implicit (Deep Equilibrium) neural operator. Applied to both two-phase composites
    and polycrystalline materials, our methodology shows improved stability and extrapolation
    capabilities compared to existing machine learning methods. Moreover, we obtain
    an improved error-accuracy tradeoff for both local and homogenized properties
    compared to both traditional solvers and feedforward ML methods. Finally, we find
    that embedding thermodynamic encodings into the architecture provides improved
    data efficiency and generalizability.
- poster: 140250
  presenter: "Khaliq, Yousaf"
  email: "83350354279f686a"
  title: "Quantum-Enhanced Neural Network for Efficient Financial Market Prediction"
  abstract: |-
    In this study, we explore the application of quantum machine learning
    for predicting financial market trends, aiming to surpass the performance of traditional
    machine learning models in both accuracy and computational efficiency. Utilizing
    historical financial data, including stock prices and market indicators, we implement
    a Quantum Neural Network to develop predictive models on quantum hardware. These
    quantum models are benchmarked against their classical counterparts, such as Support
    Vector Machines and Recurrent Neural Networks, to assess their predictive accuracy
    and computational speed. Our findings reveal that quantum-enhanced machine learning
    models offer an improvement in processing speed, leveraging quantum parallelism
    and entanglement to achieve faster data processing and model training times. Moreover,
    the quantum models exhibit enhanced predictive accuracy, showcasing their potential
    to offer a quantum advantage in the analysis of complex financial datasets. This
    research not only highlights the advantages of applying quantum computing to financial
    market predictions but also opens new avenues for future investigations into the
    scalability of quantum machine learning algorithms and their applicability across
    various domains.
- poster: 139682
  presenter: "Khan, Abraham"
  email: "5ebef607c58fc523"
  title: "Parametric Kernel Low-Rank Approximations Using Tensor Train"
  abstract: |-
    Kernel matrices are prevalent in applied mathematics with applications
    in scientific computing (integral equations) and data science (Gaussian processes).
    However, kernel matrices arising from practical applications are often dense,
    large, and depend on specific hyperparameters; thus, there is a need for methods
    to compute and store low-rank approximations of parametric kernel matrices efficiently.
    We propose a method divided into two phases, an offline phase and an online phase,
    where the offline phase dominates the computational cost, and the online phase
    is relatively inexpensive. During the offline phase, we leverage tensorized multi-variate
    Chebyshev function approximation and the Tensor Train (TT) decomposition of the
    coefficient tensor, efficiently computed via TT cross approximation. During the
    online phase, we instantiate a kernel matrix for a particular parameter and compute
    and store its low-rank approximation. Our method has linear complexity in terms
    of the size of the kernel matrix, and its utility and efficiency are demonstrated
    by applying it to various kernels arising from different application areas, varying
    spatial configurations of the source and target points, and varying properties
    of the kernel matrix (symmetry and positive semidefinite).
- poster: 139928
  presenter: "Khan, Muhammad"
  email: "44e4684ca3bd4fa9"
  title: |-
    Market Efficiency and Price Stability of Major Cryptocurrencies: A Comprehensive
    Analysis of Recent Surge
  abstract: |-
    In this research, we conduct a comprehensive analysis of market efficiency
    and price stability for three of the largest cryptocurrencies: Bitcoin, Ethereum,
    and Binance Coin. Utilizing the Adjusted Market Inefficiency Magnitude (AMIM),
    we employ both autoregression and quantile regression techniques to assess the
    market efficiency of these cryptocurrencies since their inception. This study
    extends beyond mere efficiency analysis to explore the temporal evolution of these
    cryptocurrencies, providing insights into their market dynamics over time. Furthermore,
    we investigate the recent surge in the price of each cryptocurrency, analyzing
    the factors contributing to their price fluctuations. In addition to traditional
    regression methods, we employ advanced machine learning techniques, specifically
    a transformer model, to predict the stability in the prices of these cryptocurrencies.
    By integrating machine learning with traditional econometric approaches, we aim
    to enhance the accuracy of our price stability forecasts.
- poster: 140106
  presenter: "Kilbourne, Byron"
  email: "63586b8bcdadfa3c"
  title: "Behavior Based Entity Resolution for Big-Data Networks"
  abstract: |-
    Entity resolution techniques that rely on unreliable metadata, such as
    self-reported name, address, and phone number as features for entity-linking are
    vulnerable to manipulation. Blocking strategies facilitate computation but reduce
    the scope of the solution. Both blocking and self-reported data reduce the quality
    of the final entity linkage.  Our approach combines entity behavior graphs with
    existing entity resolution techniques and distributed similarity computation using
    Apache Spark to deliver a process capable of rapid execution on large datasets
    without the need for blocking or reliance on self-reported data.  We begin with
    a network where each unresolved entity is a node and each interaction between
    entities is an edge. Aggregating entity interactions provides a feature vector
    for each entity. Similarity is computed in parallel between feature vectors using
    Apache Spark. The result is used to create a new entity network with similarities
    as edges. Pruning this network by a similarity threshold and applying connected
    components produces independent subgraphs of like entities.   The advantages of
    this method are that it is: unsupervised, massively scalable, not reliant on self-reported
    data, and not limited by blocking.   This method has successfully identified networks
    of fraud-actors. The compute time for a driver and 4 nodes each with 64 GB memory
    and 8 cores to calculate and store similarities was: 40M nodes, 1B edges, 15 minutes;
    40M nodes, 100B edges, 18 hours
- poster: 140045
  presenter: "Kim, Dohyeon"
  email: "117e01ae5c91c790"
  title: "MS27: Recent Developments in Consensus Based Optimization"
  abstract: |-
    Consensus Based Optimization (CBO) algorithms are a recent family of particle
    methods for solving complex non-convex optimization problems. In many application
    settings, the objective function is not available in closed form. Additionally,
    derivatives may not be available, or very costly to obtain. Consensus Based Optimization
    makes use of the Laplace principle to circumvent the use of gradients and is well
    suited for black-box objectives. Most of the available analysis for this recent
    family of algorithms studies the corresponding mean-field descriptions of the
    distribution of particles. Especially convergence analysis with explicit rates
    is of interest to assess algorithm performance and has mostly been done on the
    level of the mean-field PDEs. However, all results currently in the literature
    connecting the discrete particle system to the mean-field regime are restricted
    to finite time domains. In this talk, we present recent advances regarding the
    CBO algorithm and its variants and discuss uniform-in-time mean field limits.
    We focus on second-order variants of CBO as they have numerical advantages in
    terms of convergence and provide a conceptual bridge to Particle Swarm Optimization
    (PSO), one of the most widely used particle-based optimization methods.
- poster: 139848
  presenter: "Kim, Hwanwoo"
  email: "69aeb75be66f748f"
  title: |-
    MS64: Bayesian Optimization with Noise-Free Observations: Improved Regret
    Bounds Via Random Exploration
  abstract: |-
    In this talk, we study Bayesian optimization with noise-free observations.
    We introduce new algorithms rooted in scattered data approximation that rely on
    a random exploration step to enhance the accuracy of surrogate models for the
    objective function. Our algorithms retain the ease of implementation of the classical
    GP-UCB algorithm,  but the additional random exploration step accelerates their
    convergence, nearly achieving the optimal simple regret bound established by Bull.
    Furthermore, the new algorithms outperform GP-UCB and other popular Bayesian optimization
    strategies in several examples.
- poster: 140551
  presenter: "Klawonn, Matthew"
  email: "dc0ce7ac7f4ecc0f"
  title: "MS12: Mining Sheaf Theoretic Narratives (Poster)"
  abstract: |-
    Querying time-series data or finding motifs therein is well understood
    for certain data types but remains challenging in the general case. For various
    data types or patterns sought, it is sometimes unclear what the correct definition
    of a temporal pattern is even if the analogous static pattern is well studied.
    One can look to various theories of temporal graphs, for instance, to see how
    time adds nuances and complexity to the study of common graph theoretic artifacts
    like paths. In this talk we recapitulate recent work done by collaborators to
    define temporal data via a sheaf theoretic construct that is agnostic to the object
    of study, captures richer structure than mere sequences of data, and leads to
    natural temporal versions of static patterns, among other benefits. We further
    show how this sheaf theoretic definition of temporal data can be leveraged for
    finding/counting occurrences of temporal patterns within a time-series by way
    of a homomorphism search algorithm. We will discuss empirical results of experiments
    grounded in Air Force planning applications.
- poster: 140116
  presenter: "Kohli, Dhruv"
  email: "dce3562e21146a39"
  title: |-
    MS26: Tear and Repulsion Enabled Registration of Point Clouds for Manifold
    Learning
  abstract: "We present a framework for aligning the local views of a possibly closed/non-orientable data manifold to produce an embedding in its intrinsic dimension through tearing. Through a spectral coloring scheme, we render the embeddings of the points across the tear with matching colors, enabling a visual recovery of the topology of the data manifold. The embedding is further equipped with a tear-aware metric that enables computation of shortest paths while accounting for the tear. To measure the quality of an embedding, we propose two Lipschitz-type notions of global distortion—a stronger and a weaker one—along with their pointwise counterparts for a finer assessment of the embedding. Subsequently, we bound them using the distortion of the local views and the alignment error between them. We show that our theoretical result on strong distortion leads to a new perspective on the need for a repulsion term in manifold learning objectives. As a result, we enhance our alignment approach by incorporating repulsion. Finally, we compare various strategies for the tear and repulsion enabled alignment, with regard to their speed of convergence and the quality of the embeddings produced."
- poster: 140521
  presenter: "Koike, Tomoki"
  email: "d2c69693c5de5f7f"
  title: |-
    MS7: Data-Driven Estimation of Stability Guarantees for Nonlinear Dynamical
    Systems (Poster)
  abstract: |-
    Analyzing the stability of a nonlinear dynamical system is central to
    understanding system behavior and designing controllers, and we use a Lyapunov
    function for this purpose. It is possible to guarantee the stability of a system
    if one can find a Lyapunov function that is positive definite and decreasing over
    time along the orbit of the system, thus providing a sufficient condition for
    stability. A Lyapunov function also characterizes an estimate of the domain of
    attraction, which indicates the region under which the system states asymptotically
    converge to equilibrium. The construction of a Lyapunov function is done analytically
    and ad hoc for certain nonlinear systems. However, doing so for systems with high
    nonlinearities and different dimensions is a challenging task. To address this
    problem, we present a data-driven method for discovering Lyapunov functions, called
    Lyapunov function inference (LyapInf). This new method fits a quadratic Lyapunov
    function to the state trajectory data of the dynamics via optimization, where
    the process of inferring a Lyapunov function is based on the non-intrusive model
    reduction method of Operator Inference. This method learns one of many possible
    Lyapunov functions that ensures stability and estimates the domain of attraction
    with or without access to the system model. In this work, we demonstrate this
    new method on several numerical examples.
- poster: 140563
  presenter: "Kong, Lingkai"
  email: "20fac77eb61a1b9f"
  title: "MS33: Convergence of Kinetic Langevin Monte Carlo on Lie Groups"
  abstract: |-
    Explicit, momentum-based dynamics for optimizing functions defined on
    Lie groups was recently constructed, based on techniques such as variational optimization
    and left trivialization. We appropriately add tractable noise to the optimization
    dynamics to turn it into a sampling dynamics, leveraging the advantageous feature
    that the trivialized momentum variable is Euclidean despite that the potential
    function lives on a manifold. We then propose a Lie-group MCMC sampler, by delicately
    discretizing the resulting kinetic-Langevin-type sampling dynamics. The Lie group
    structure is exactly preserved by this discretization. Exponential convergence
    with explicit convergence rate for both the continuous dynamics and the discrete
    sampler are then proved under $W_2$ distance. Only compactness of the Lie group
    and geodesically $L$-smoothness of the potential function are needed. To the best
    of our knowledge, this is the first convergence result for kinetic Langevin on
    curved spaces, and also the first quantitative result that requires no convexity
    or, at least not explicitly, any common relaxation such as isoperimetry.
- poster: 139754
  presenter: "König, Josie"
  email: "8fdb2a224e60017b"
  title: "MS94: Efficient Training of Gaussian Processes with Tensor Product Structure"
  abstract: |-
    Gaussian processes with the covariance matrix given as the sum of possibly
    multiple Kronecker products appear in spatio-temporal magnetoencephalography or
    climate data sets. This structure allows the covariance matrix to be identified
    as a tensor, which is used to represent this operator and the training data in
    the tensor train format. Determining the optimal set of hyperparameters of a Gaussian
    process based on a large amount of training data requires both linear system solves
    and trace estimation. In particular, solving a linear system with the covariance
    tensor is a major bottleneck and requires appropriate preconditioning.
- poster: 140167
  presenter: "Kook, Yunbum"
  email: "de1df972c03dce07"
  title: "MS33: Uniform Sampling under Isoperimetric Assumptions"
  abstract: |-
    We present a new random walk for uniformly sampling high-dimensional convex
    bodies. It achieves state-of-the-art runtime complexity with stronger guarantees
    on the output than previously known, namely in R\'enyi divergence (which implies
    TV, $\mathcal{W}_2$, KL, $\chi^2$). The proof departs from known approaches for
    polytime algorithms for the problem --- we utilize a stochastic diffusion perspective
    to show contraction to the target distribution with the rate determined by functional
    isoperimetric constants of the stationary density.
- poster: 140215
  presenter: "Korkmaz, Turgay"
  email: "dc6791a876a7696d"
  title: |-
    Towards Autonomous Laboratories by Integrating IoT, Robotics, Computer Vision,
    and Ai with Container Technology
  abstract: |-
    Integration of IoT edge devices, robotics, computer vision, and AI is
    vital for the realization of autonomous laboratories that can independently perform
    experiments, analyze data, and make decisions to enhance efficiency and productivity
    across various domains.  Container technology emerges as a crucial facilitator
    in achieving this integration. Our current research explored how to use lightweight
    containers in deploying CPU-based computer vision apps on edge devices while evaluating
    various container technologies and images on ARM-based edge devices. Accordingly,
    we developed an optimized OpenCV container, and showed how integrating containers
    boosts efficiency in AI vision applications requiring effective real-time data
    stream processing. However, configuring real-time stream processing systems in
    the edge setting becomes increasingly challenging with the growing number of edge
    devices and their interactions, compounded by resource constraints and multi-tenant
    hosting complexities. We are currently conducting performance benchmarking and
    analysis of Apache Storm, a representative stream processing engine, on a cluster
    of ARM-based edge devices including Raspberry Pi 4 and Nvidia Jetson Nano. Furthermore,
    we are developing an LLM agent-based automation tool for experimental setup, debugging
    configuration errors, and analyzing the root cause of performance issues when
    running advanced data science algorithms that need to process vast amount of visual
    and IoT sensory data.
- poster: 139010
  presenter: "Kotsiopoulos, Konstandinos"
  email: "a4d43ffd147731be"
  title: |-
    MS4: Approximation of Group Explainers with Coalition Structure Using Monte
    Carlo Sampling
  abstract: |-
    In recent years, many Machine Learning (ML) explanation techniques have
    been designed using ideas from cooperative game theory. These game-theoretic explainers
    suffer from high complexity, hindering their exact computation in practical settings.
    In our work, we focus on a wide class of linear game values, as well as coalitional
    values, for the marginal game based on a given ML model and predictor vector.
    By viewing these explainers as expectations over appropriate sample spaces, we
    design a novel Monte Carlo sampling algorithm that estimates them at a reduced
    complexity that depends linearly on the size of the background dataset. The advantage
    of this approach is that it is fast, easily implementable, and model-agnostic.
- poster: 140148
  presenter: "Kreider, Max"
  email: "cf0274df5020e530"
  title: |-
    MS36: An ML Approach to Inferring Network Connectivity for Coupled Stochastic
    Oscillator Networks
  abstract: |-
    Large-scale brain oscillations may reflect the synchronous behavior of
    neuron populations, yet the mechanisms underlying collective neuron dynamics are
    not well understood.  One approach is modeling neuron populations as systems of
    oscillators: ODEs with stable limit-cycle solutions.  However, neural activity
    is noisy, necessitating the study of coupled stochastic oscillators. Koopman operator
    spectral methods provide a universal description of stochastic oscillators [Perez
    et al 2023 PNAS].  However, the structure of the low-lying spectra for coupled
    stochastic oscillators remains an important open question.  It is challenging
    to solve the related PDE for the Koopman modes; standard methods suffer from the
    curse of dimensionality and often yield inadequate results in dimensions $n\geq
    4$.  Recently, machine learning (ML) methods have proven effective for solving
    high-dimensional PDEs [Zhai et al 2022 PMLR]. Here, we derive a novel ML-based
    PDE method to compute the Koopman modes of coupled stochastic oscillators.  For
    weakly coupled systems, our approach can generate eigenfunctions for a range of
    coupling strengths; we study the dependence of eigenfunctions of small oscillator
    networks on coupling strength and network connectivity.  By performing a linear
    sensitivity analysis, our approach enables us to infer coupling strength from
    observed data.  We argue that our ML-based approach can contribute  to the analysis
    of large-scale electrophysiological recordings.
- poster: 140130
  presenter: "Kumar, Shivam"
  email: "1722324ecf20332a"
  title: |-
    A Likelihood Based Approach to Distribution Regression Using Conditional
    Deep Generative Models
  abstract: |-
    In this work, we study the statistical properties of the likelihood-based
    approach for the distributional regression with conditional deep generative model
    with full dimensional noise and underlying singular support. To be precise, we
    use the assume the full dimensional response conditioned on a covariate in concentrated
    near low dimensional manifold.  We provided and proved the desirable convergence
    rate for the ambient density in relevant metrics (Hellinger and Wasserstein) for
    our approach with simplified presentation for traditional network classes. We
    demonstrate that the characterization of the learnable distribution class is broad.
    It encompasses not only smooth and analytically tractable distributions but also
    extends to the general manifold case with minimal assumptions.  Our analysis emphasizes
    the importance of introducing a small noise perturbation to the data when they
    closely align with the manifold. This observation validates the inherent structural
    challenges encountered in related manifold estimation problems with noisy data,
    as previously highlighted in the literature. Furthermore, we successfully implement
    the proposed approach in numerical studies using synthetic and real-world datasets,
    thereby providing complementary validation to our theoretical findings.
- poster: 139957
  presenter: "Kundu, Chandra"
  email: "a59423fb936776cd"
  title: "Learned Large-Scale Robust Matrix Completion Via Deep Unfolding"
  abstract: |-
    Robust matrix completion (RMC) is a technique for recovering a low-rank
    matrix from a subset of its entries, where some of the entries are corrupted.
    In this study, we introduce a deep-learning-augmented approach to RMC, called
    Learned Robust Matrix Completion (LRMC). Our approach utilizes Deep Unfolding,
    which converts each iteration of the RMC algorithm into a Deep Neural Network
    (DNN) layer and leverages DNN training to optimize the performance of the algorithm.
    Through extensive empirical experiments on synthetic datasets and real-world applications,
    we demonstrate that LRMC outperforms state-of-the-art methods, suggesting it as
    an attractive choice for addressing RMC problems.
- poster: 140556
  presenter: "Kwon, Soo Min"
  email: "a17c769b26dacb96"
  title: |-
    MS49: The Simplicity Biases in Deep Linear Networks: Flatness, Invariance,
    and Edge of Stability
  abstract: |-
    In this work, we investigate the training dynamics of deep linear networks
    with a large learning rate $\eta$, commonly used in machine learning practice
    for improved empirical performance. This regime is also known as the edge of stability,
    where the largest eigenvalue of the Hessian hovers around $2/\eta$, and the training
    loss oscillates yet decreases over long timescales. Within this regime, we observe
    an intriguing phenomenon: the oscillations in the training loss are artifacts
    of the oscillations of only a few leading singular values of the weight matrices
    within a small invariant subspace. Theoretically, we analyze this behavior based
    on the deep matrix factorization problem. Our analysis reveals that for $\eta$
    within a specific range, the oscillations occur within a 2-period fixed orbit
    of the singular values, while the singular vectors remain invariant across all
    iterations. Lastly, our analysis also reveals that deep linear networks initialized
    with small scales are implicitly biased towards solutions with a smaller trace
    of the Hessian, which may be of independent interest.
- poster: 139893
  presenter: "Lai, Jasen"
  email: "864d911acd5e7877"
  title: "MS43: Data-Regularized Operator Learning for Inverse Problems"
  abstract: |-
    Regularization is a technique used in machine learning to prevent overfitting
    and embed prior information, and it is critical for solving ill-posed inverse
    problems. Deep learning methods have seen success in learning inverse problems,
    but they often regularize by adding penalty terms in the loss function or adapting
    the model architecture to better suit the problem. However, these methods of regularization
    can lead to increased computational costs during training and may be limited to
    a specific choice of model. We use an innovative approach known as the "data-regularized
    operator learning" (DaROL) method to regularize PDE inverse problems via transformation
    of the data. The DaROL method transforms the data in a way that aids the optimization
    of the neural network, and it can embed prior information without changing the
    loss function or the model architecture. This makes the DaROL method a simple
    and flexible framework for regularization. Here we explain the DaROL method and
    demonstrate how it regularizes ill-posed inverse problems for deep learning methods.
- poster: 139950
  presenter: "Lamichhane, Bishal"
  email: "7a290aa54c53e365"
  title: "MS47: Llm Agents in Social Network Dynamics: A Study on Information Flow"
  abstract: |-
    Graph-based structures combined with Large Language Models (LLMs), which
    have demonstrated their capability to simulate human conversations, provide a
    powerful framework for simulating and analyzing dynamic social behaviors in networked
    systems. We develop such a framework that simulates conversations between LLM-powered
    Generative Agents and measures the information flow in the network using CEM (Conversation
    Evaluation Model). Each agent acts as a node in our network, characterized by
    distinct personas. These agents engage in simulated conversation based on the
    graph structure providing insights into how information propagates through various
    network topologies. Furthermore, it also reveals the inherent biases and the capabilities
    of LLMs in simulating realistic conversations. We analyze how different network
    structures and agent characteristics influence the spread and transformation of
    information. GNNs enhance our framework by enabling predictions of network behavior
    and the potential impact of various interventions. Our study opens new avenues
    for both theoretical research and practical applications in network science and
    social behavior analysis.
- poster: 139801
  presenter: "Lancellotti, Sandro"
  email: "73a0edbeded0e1c1"
  title: "Bayesian Optimization for Parameters Search in Radial Basis Function Interpolation"
  abstract: |-
    In this contribution, we introduce an innovative approach in the Radial
    Basis Function (RBF) interpolation that facilitates the efficient detection of
    the optimal shape parameter. In contrast to the classical and widely used Leave-One-Out
    Cross-Validation (LOOCV), which supposedly consists of an exhaustive grid search,
    we propose the exploitation of Bayesian Optimization (BO). BO is a statistical
    method that involves modeling the error function with a Gaussian process [Snoek
    J., Larochelle H., Adams R.P.,  Practical Bayesian Optimization of Machine Learning
    Algorithms] and, through an iterative procedure, dynamically selects the optimal
    parameter. Moreover, when the datasets increase their dimensions, the RBF interpolation
    can be applied in a Partition of Unity setting (RBF-PU) [Fasshauer G.E., McCourt
    M.J., Kernel-based Approximation Methods Using MATLAB]. This combined technique,
    on one side, allows the reduction of computational expense by solving small interpolation
    problems and combining them; on the other, it preserves the estimated error, considering
    an overlapped decomposition of the initial domain into spherical patches of variable
    radius. We also propose a bivariate BO search for the shape parameter and the
    radius of the spherical decomposition of the domain in RBF-PU. Numerical results
    show that BO sharply reduces the parameter search time with respect to the classical
    LOOCV technique in RBF and RBF-PU settings.
- poster: 140269
  presenter: "Lang, Quanjun"
  email: "4de67371b33627ab"
  title: |-
    MS32: Data Adaptive RKHS Tikhonov Regularization for Learning Kernels in
    Operators
  abstract: |-
    We present DARTR: a Data Adaptive RKHS Tikhonov Regularization method
    for the linear inverse problem of nonparametric learning of function parameters
    in operators. A key ingredient is a system intrinsic data adaptive (SIDA) RKHS,
    whose norm restricts the learning to take place in the function space of identifiability.
    DARTR utilizes this norm and selects the regularization parameter by the L-curve
    method. We illustrate its performance in examples including integral operators,
    nonlinear operators and nonlocal operators with discrete synthetic data. Numerical
    results show that DARTR leads to an accurate estimator robust to both numerical
    error due to discrete data and noise in data, and the estimator converges at a
    consistent rate as the data mesh refines under different levels of noises, outperforming
    two baseline regularizers using $l^2$ and $L^2$ norms.
- poster: 139925
  presenter: "Lanthaler, Samuel"
  email: "3eddef598a126aa8"
  title: "MS63: Complexity Bounds for Operator Learning"
  abstract: |-
    Operator learning frameworks generalize neural networks and define a methodology
    for the data-driven approximation of operators, such as the solution operator
    of a PDE. How much data is necessary to learn operators in a purely data-driven
    manner? How many tunable weights and biases are needed? This poster summarizes
    recent work providing partial answers to these questions. Upper and lower bounds
    on the complexity of operator learning are discussed.
- poster: 140154
  presenter: "Le Provost, Mathieu"
  email: "e7d16246d8683cea"
  title: "Data Assimilation Beyond the Linear and Local Gaussian Setting"
  abstract: |-
    In the filtering setting, we seek to infer the state distribution conditioned
    on all the observations available up to that time. Ensemble filters sequentially
    assimilate observations by updating a set of samples over time. Despite their
    empirical success, Gaussian filters such as the ensemble Kalman filter (Evensen,
    J. Geophys. Res., 1994) operate under simplifying assumptions: linearity of the
    observation model and Gaussianity of the forward process and observation error.
    Also it is often assumed that the observations to assimilate are local functions
    of the state, i.e, that the observations only depend on the state variables that
    are close by in physical distance. These assumptions are limiting and not representative
    of many practical settings, thus leading to fundamentally biased inference. Indeed,
    we often deal with non-Gaussian distributions as well as non-local observations
    given by integrals of linear and nonlinear functions of the state, such as radiance
    measured by satellites, fluxes through surfaces, or solutions of elliptic partial
    differential equations. Leveraging tools from measure transport (Spantini et al.,
    SIAM Review, 2022), we introduce an unifying framework to analyze ensemble filters.
    Building on the flexibility of this formulation, we present our recent advances
    to build ensemble filters able to leverage nonlinear, non-local, and non-Gaussian
    features of the filtering problems of interest.
- poster: 139118
  presenter: "Le Provost, Mathieu"
  email: "e7d16246d8683cea"
  title: "MS69: Sparsity-Promoting Ensemble Methods for Bayesian Inverse Problems"
  abstract: ""
- poster: 140013
  presenter: "Le Provost, Mathieu"
  email: "e7d16246d8683cea"
  title: "MS23: Preserving Linear Invariants in Ensemble Filtering Methods"
  abstract: |-
    Data assimilation is an elegant paradigm for estimating evolving state
    variables from observations. In the filtering setting, we seek to infer the state
    distribution conditioned on all the observations available up to that time. Ensemble
    filtering methods tackle this problem by sequentially updating a set of particles
    to form an empirical approximation for the filtering distribution. For accurate
    and robust predictions of dynamical systems, discrete solutions must preserve
    their critical invariants. While modern numerical solvers satisfy these invariants,
    existing invariant-preserving analysis steps are limited to Gaussian settings
    and are often not compatible with existing regularization techniques of ensemble
    filters.  The present work focuses on preserving linear invariants, such as mass
    or stoichiometric balance of chemical species. Using tools from measure transport
    theory (Spantini et al., 2022, SIAM Review), we introduce a generic class of nonlinear
    ensemble filters that automatically preserve desired linear invariants in non-Gaussian
    settings. By specializing this framework to the Gaussian case, we recover a constrained
    formulation of the Kalman filter. Then, we show how to combine existing regularization
    techniques for the ensemble Kalman filter (Evensen, 1994, J. Geophys. Res.) with
    the preservation of the linear invariants. Finally, we assess the benefits of
    preserving linear invariants for the ensemble Kalman filter and nonlinear ensemble
    filters.
- poster: 140066
  presenter: "Lee, Jeonghwan"
  email: "c9bc18d7b6525e6c"
  title: "MS2: Modeling Time-Varying Networks"
  abstract: |-
    Modeling dynamic networks over time is a crucial task in understanding
    how systems change. Previous research often relied on either parametric models
    or nonparametric models that assume stationarity, both of which limit flexibility
    in modeling. In this work, we introduce a smooth time-varying dynamic graphon
    model and propose a method to fit this model using double smoothing. Particularly,
    we address the problem of change point detection without assuming stationarity,
    significantly expanding the applicability of previous methods in network settings.
    We demonstrate the effectiveness of our approach through theoretical results and
    extensive numerical studies.
- poster: 139945
  presenter: "Lee, Kangwook"
  email: "cc9f56387a163d53"
  title: "MS89: The Expressive Power of Low-Rank Adaptation"
  abstract: |-
    Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method that
    leverages low-rank adaptation of weight matrices, has emerged as a prevalent technique
    for fine-tuning pre-trained models such as large language models and diffusion
    models. Despite its huge success in practice, the theoretical underpinnings of
    LoRA have largely remained unexplored. This paper takes the first step to bridge
    this gap by theoretically analyzing the expressive power of LoRA. We prove that,
    for fully connected neural networks, LoRA can adapt any model $f$ to accurately
    represent any smaller target model $\overline{f}$ if LoRA-rank $\geq(\text{width
    of }f) \times \frac{\text{depth of }\overline{f}}{\text{depth of }f}$, under a
    mild assumption. We quantify the approximation error when the LoRA-rank is lower
    than the threshold. For Transformer networks, we show any model can be adapted
    to a target model of the same size with rank-$(\frac{\text{embedding size}}{2})$
    LoRA adapters. Our study reveals numerous theoretical insights on hyperparameter
    tuning and algorithm development for LoRA, all of which are empirically validated.
- poster: 139902
  presenter: "Lee, Jonghyeok"
  email: "75fc9548e5c226f"
  title: "MS10: Flow-Based Distributionally Robust Optimization"
  abstract: |-
    We present a computationally efficient framework, called $ FlowDRO $,
    for solving flow-based distributionally robust optimization (DRO) problems with
    Wasserstein uncertainty sets while aiming to find continuous worst-case distribution
    (also called the Least Favorable Distribution, LFD) and sample from it. The requirement
    for LFD to be continuous is so that the algorithm can be scalable to problems
    with larger sample sizes and achieve better generalization capability for the
    induced robust algorithms. To tackle the computationally challenging infinitely
    dimensional optimization problem, we leverage flow-based models and continuous-time
    invertible transport maps between the data distribution and the target distribution
    and develop a Wasserstein proximal gradient flow type algorithm. In theory, we
    establish the equivalence of the solution by optimal transport map to the original
    formulation, as well as the dual form of the problem through Wasserstein calculus
    and Brenier theorem. In practice, we parameterize the transport maps by a sequence
    of neural networks progressively trained in blocks by gradient descent. We demonstrate
    its usage in adversarial learning, distributionally robust hypothesis testing,
    and a new mechanism for data-driven distribution perturbation differential privacy,
    where the proposed method gives strong empirical performance on high-dimensional
    real data.
- poster: 139873
  presenter: "Lee, Wonjun"
  email: "6007ab64b3fb4a13"
  title: |-
    MS40: Geometry-Preserving Encoder and Decoder in Latent Diffusion Models
    (Poster)
  abstract: |-
    In this poster, we conduct a mathematical analysis of a popular machine
    learning framework for contrastive learning, which has become extensively used
    in supervised, semi-supervised, and unsupervised learning algorithms. Our focus
    is on examining the geometrical properties of graphs generated by the contrastive
    learning algorithm and understanding the impact of neural network optimization
    on the solutions derived from contrastive learning. Our findings reveal that despite
    the widespread acclaim for the performance of contrastive learning losses, these
    methods may be fundamentally ill-posed due to the potential for obtaining ill-posed
    solutions. However, we demonstrate that neural network optimization plays a crucial
    role in achieving a robust feature mapping that maintains the clustering structure
    of the given dataset. To substantiate our theoretical insights, we present numerical
    results from simple toy examples and real-world datasets, confirming the alignment
    between our theoretical predictions and empirical observations.
- poster: 139668
  presenter: "Lee, Sanghyun"
  email: "2d04e3e9471b5c7e"
  title: "MS35: On the Training of Deep Operator Networks"
  abstract: |-
    We present a novel training method for deep operator networks (DeepONets),
    one of the most popular neural network models for operators. DeepONets are constructed
    by two sub-networks, namely the branch and trunk networks. Typically, the two
    sub-networks are trained simultaneously, which amounts to solving a complex optimization
    problem in a high dimensional space. In addition, the nonconvex and nonlinear
    nature makes training very challenging. To tackle such a challenge, we propose
    a two-step training method that trains the trunk network first and then sequentially
    trains the branch network. The core mechanism is motivated by the divide-and-conquer
    paradigm and is the decomposition of the entire complex training task into two
    subtasks with reduced complexity. Therein the Gram-Schmidt orthonormalization
    process is introduced which significantly improves stability and generalization
    ability. On the theoretical side, we establish a generalization error estimate
    in terms of the number of training data, the width of DeepONets, and the number
    of input and output sensors. Numerical examples are presented to demonstrate the
    effectiveness of the two-step training method, including Darcy flow in heterogeneous
    porous media.
- poster: 140604
  presenter: "Lee, Jaeyong"
  email: "7045c51c68cf2810"
  title: |-
    MS35: Finite Element Operator Network (feonet): Operator Learning for Parametric
    Pdes Without Labeled Data
  abstract: |-
    Partial differential equations (PDEs) are crucial for modeling diverse
    phenomena, yet solving them efficiently remains challenging. We propose Finite
    Element Operator Networks (FEONet), a novel approach combining deep learning and
    traditional finite element methods to solve parametric PDEs without paired training
    data. Our method demonstrates robust performance across various benchmarks, emphasizing
    accuracy, generalization, and computational efficiency. Additionally, we provide
    a theoretical analysis of FEONet's convergence for second-order linear elliptic
    PDEs, highlighting the condition number's role in error estimates. Numerical experiments
    validate our findings, highlighting FEONet's potential in complex domains requiring
    precise modeling and analysis. This research bridges deep learning and numerical
    methods, enhancing parametric PDE solutions.
- poster: 139605
  presenter: "Lehoucq, Richard"
  email: "70ffb24b0957916a"
  title: |-
    Zero-Truncated Poisson Regression for Sparse Multiway Count Data Data Corrupted
    by False Zeros
  abstract: |-
    We propose a novel statistical inference paradigm for zero-inflated multiway
    count data that dispenses with the need to distinguish between true and false
    zero counts. Our approach ignores all zero entries and applies zero-truncated
    Poisson regression on the positive counts. Inference is accomplished via tensor
    completion that imposes low-rank structure on the Poisson parameter space. Our
    main result shows that an $N$-way rank-$R$ parametric tensor $\boldsymbol{\mathscr{M}}\in(0,\infty)^{I\times
    \cdots\times I}$ generating Poisson observations can be accurately estimated from
    approximately $ IR^2\log_2^2(I)$ non-zero counts for a nonnegative canonical polyadic
    decomposition.  Several numerical experiments are presented demonstrating that
    our zero-truncated paradigm is comparable to the ideal scenario where the locations
    of false zero counts are known a-priori.
- poster: 139756
  presenter: "Lehoucq, Richard"
  email: "70ffb24b0957916a"
  title: |-
    Zero-Truncated Poisson Regression for Multiway Count Data Data Corrupted
    by False Zeros
  abstract: ""
- poster: 140575
  presenter: "Lei, Qi"
  email: "80684655ea06849b"
  title: |-
    MS66: Sketchy Moment Matching: Toward Fast and Provable Data Selection for
    Finetuning
  abstract: |-
    We revisit data selection in a modern context of finetuning from a fundamental
    perspective. Extending the classical wisdom of variance minimization in low dimensions
    to high-dimensional finetuning, our generalization analysis unveils the importance
    of additionally reducing bias induced by low-rank approximation. Inspired by the
    variance-bias tradeoff in high dimensions from the theory, we introduce Sketchy
    Moment Matching (SkMM), a scalable data selection scheme with two stages. (i)
    First, the bias is controlled using gradient sketching that explores the finetuning
    parameter space for an informative low-dimensional subspace $\mathcal{S}$; (ii)
    then the variance is reduced over $\mathcal{S}$ via moment matching between the
    original and selected datasets. Theoretically, we show that gradient sketching
    is fast and provably accurate: selecting $n$ samples by reducing variance over
    $\mathcal{S}$ preserves the fast-rate generalization $O(\dim(\mathcal{S})/n)$,
    independent of the parameter dimension. Empirically, we concretize the variance-bias
    balance via synthetic experiments and demonstrate the effectiveness of SkMM for
    finetuning in real vision tasks.
- poster: 140065
  presenter: "Lei, Na"
  email: "2d5c0eadb6c4babb"
  title: "MS19: Discrete Ricci Curvature Based Level Set Model"
  abstract: |-
    The level set method has played a critical role among many image segmentation
    approaches. Several edge detectors, such as the gradient, have been applied to
    its regularisation term. However, traditional edge detectors lack high-order information
    and are sensitive to image noise. To tackle this problem, we introduce a method
    to calculate the Ricci curvature, a vital curvature in three-dimensional Riemannian
    geometry. In addition, we propose incorporating the curvature into the regularisation
    term. Experiments suggest that our method outperforms the state-of-the-art level
    set methods and achieves a comparable result with deep learning.
- poster: 139702
  presenter: "LeJeune, Daniel"
  email: "5ac2da87e39c8bc5"
  title: "MS54: Asymptotically Free Sketching and Ridge Regression Applications"
  abstract: |-
    Classical results in sketching for dimensionality reduction in machine
    learning assert that, provided the sketch size is sufficiently large, the original
    unsketched solution is recovered at a fraction of the cost. However, in many practical
    settings, the sketch size may be smaller than needed for these guarantees. We
    provide a more general asymptotic result for sketched matrix inversion that holds
    for any sketch size and reveal that sketching is equivalent to adding ridge regularization.
    We prove our results for a broad class of asymptotically free sketches encompassing
    the spectral profiles of most sketches used in practice. We then determine the
    precise effect of sketching on the generalization error of ridge regression and
    show that the generalized cross-validation risk estimator is consistent for sketched
    ensembles, enabling the efficient evaluation of unsketched ridge regression risk
    using only sketched data.
- poster: 139688
  presenter: "Levin, Eitan"
  email: "c4de7f80f36b094b"
  title: "MS63: Any-Dimensional Equivariant Neural Networks"
  abstract: |-
    Traditional supervised learning aims to learn an unknown mapping by fitting
    a function to a set of input-output pairs with a fixed  dimension. The fitted
    function is then defined on inputs of the same dimension. However, in many settings,
    the unknown mapping  takes inputs in any dimension; examples include graph parameters
    defined on graphs of any size and physics quantities defined on an  arbitrary
    number of particles. We leverage a newly-discovered phenomenon in algebraic topology
    called representation stability to define equivariant neural networks that can
    be trained with data in a fixed dimension  and then extended to accept inputs
    in any dimension. Our approach is black-box and user-friendly, requiring only
    the network architecture and the groups for equivariance, and can be combined
    with any training procedure.
- poster: 140042
  presenter: "Levine, Matthew"
  email: "8a0d22406bd0cb34"
  title: |-
    MS71: A Jax Toolbox for Learning Stochastic Differential Equations with
    Uncertainties from Real-World Data Streams
  abstract: |-
    The development of data-informed predictive models for dynamical systems
    is of widespread interest in many disciplines. We present a unifying framework
    for blending mechanistic and machine-learning approaches to identify dynamical
    systems from noisily and partially observed data. This approach leverages Bayesian
    statistics and auto-differentiable data assimilation, and is instantiated in an
    open source Jax software package. We compare pure data-driven learning with hybrid
    models which incorporate imperfect domain knowledge. Our formulation is agnostic
    to the chosen machine learning model, is presented in both continuous and discrete-time
    settings, and is compatible both with model errors that exhibit substantial memory
    and errors that are memoryless. We will present formulations and experiments to
    examine data-driven point-wise and distributional estimates of differential equations
    in these settings.
- poster: 139789
  presenter: "Lewis, Joshua"
  email: "684147700547ade0"
  title: |-
    MS29: Fractional Schrodinger Equation: Eigenfunctions with Physical Potentials,
    and Fractionally-Enhanced Quantum Tunneling
  abstract: "Fractional evolution equations lack generally accessible and well-converged codes excepting anomalous diffusion.  A particular equation of strong interest to the growing intersection of applied mathematics and quantum information science and technology is the fractional Schrödinger equation, which describes sub-and super-dispersive behavior of quantum wavefunctions induced by multiscale media.  We derive a computationally efficient sixth-order split-step numerical method to converge the eigenfunctions of the FSE to arbitrary numerical precision for arbitrary fractional order derivative.  We demonstrate applications of this code to machine precision for classic quantum problems such as the finite well and harmonic oscillator, which take surprising twists due to the non-local nature of the fractional derivative. For example, the evanescent wave tails in the finite well take a Mittag-Leffer-like form which decay much slower than the well-known exponential from integer-order derivative wave theories, enhancing penetration into the barrier and therefore quantum tunneling rates.  We call this effect fractionally enhanced quantum tunneling.  This work includes an open source code for communities from quantum experimentalists to applied mathematicians to easily and efficiently explore the solutions of the fractional Schrödinger equation in a wide variety of practical potentials for potential realization in quantum tunneling enhancement and other quantum applications."
- poster: 140168
  presenter: "Li, Chang"
  email: "53d4b39992e18ca1"
  title: "MS2: Improved Curvature Gaps for Stochastic Block Model Graphs"
  abstract: |-
    Discrete notions of curvature have recently been used to study the structural
    properties of networks from a new perspective, modeling properties such as geodesic
    dispersion and volume growth that are not directly captured by spectral analysis.
    Clustering algorithms based on graph curvatures rely on the property that edges
    within communities tend to be positively curved, while bridges between communities
    tend to be negatively curved. In the current work, we improve bounds on the "curvature
    gap", which quantifies the separation between these two edge types, by analyzing
    the distributional limits of Ollivier's Ricci curvature in stochastic blockmodel
    graphs for these two kinds of edges.
- poster: 139600
  presenter: "Li, Shiying"
  email: "5094acea3b03356"
  title: "MS42: Measure Transfer Via Stochastic Slicing and Matching"
  abstract: |-
    Transporting and estimating probability measures are fundamental tasks
    in various generative modeling methods like normalizing flows. An equally crucial
    aspect is having a suitable metric to gauge model performance and guide algorithm
    design, particularly for scalability. Our focus lies on an iterative slice-matching
    approach initially introduced by Pitie et al. for transferring color statistics.
    While these techniques have proven effective in various data science applications,
    a comprehensive understanding of their convergence properties remains elusive.
    Our primary aim is to establish an almost sure convergence, shedding light on
    the connections with stochastic gradient descent in the context of the sliced-Wasserstein
    distance. We also explore recovery and stability  under specific structural assumptions
    about the measures. This is joint work with Caroline Moosmueller.
- poster: 139636
  presenter: "Li, Chunyan"
  email: "c36b18f7e54cb259"
  title: |-
    Energy Dissipation Rate-Based Sampling: An Adaptive Refinement Strategy
    for Solving Thermodynamically Consistent Models Using Pinns
  abstract: |-
    We introduce Energy Dissipation Rate-based Sampling (EDRS), an innovative
    adaptive refinement technique for Physics-Informed Neural Networks (PINNs) aimed
    at significantly enhancing their efficacy in tackling thermodynamically consistent
    models. The core is the strategic utilization of energy dissipation rate density
    as a key metric for selectively resampling critical collocation points, thereby
    refining the model's accuracy. EDRS notably outperforms the traditional residual-based
    adaptive refinement approach, demonstrating a sixfold improvement in relative
    mean square error for the Allen-Cahn equation.  Leveraging the inherent mesh-free
    nature of PINNs, we deploy neural networks to adeptly approximate the solutions
    of thermodynamically consistent phase field models with dynamic boundary conditions
    in arbitrary domains.  Our primary aim is to meticulously investigate how dynamic
    boundary conditions influence the overall dynamics of the bulk material.  We conduct
    thorough simulations across both disk and ellipse-shaped domains, analyzing how
    static versus dynamic boundary conditions distinctly affect the system's behavior.  This
    study not only underscores the potential of EDRS in enhancing the computational
    performance of PINNs but also enriches our comprehension of the pivotal role dynamic
    boundary conditions play in influencing the dynamic behavior of thermodynamic
    systems, offering valuable insights for future studies in computational physics
    and engineering.
- poster: 140542
  presenter: "Li, Xingjie Helen"
  email: "2a0cf79427173069"
  title: "MS42: Robust and Efficient Differentiation for Regularized Optimal Transport"
  abstract: |-
    In this work, we consider the computation of derivatives for a bi-level
    optimization problem using regularized optimal transport (OT) distance, with applications
    including shuffled regression. We focus on the entropy-regularized OT distance
    and present an analytical expression for the second derivatives (Hessian) with
    respect to samples.  Computing the Hessian poses significant challenges in terms
    of numerical stability and large memory consumption, especially for large-scale
    computations. To address the numerical instability arising from the ill-conditioned
    nature of the problem, we propose a theoretically guided stabilization method.
    We also derived a quantity that can serve as a surrogate to check the accuracy
    of the numerical approximation.  Our numerical algorithm and the chosen hyperparameters
    are based on rigorous analysis of the properties of problems. We examine the efficiency
    and accuracy of the computed Hessian for various data sets. Finally, we apply
    the proposed algorithm to parameter estimation for shuffled regression of mixed
    Gaussians and 3D Point Cloud Registration of the MobilNet10 dataset.
- poster: 140256
  presenter: "Li, Xiang"
  email: "ba30b112c8b6e57a"
  title: "MS10: Decoupled Data Consistency with Diffusion Purification for Image Restoration"
  abstract: |-
    Diffusion models have recently gained traction as a powerful class of
    deep generative priors, excelling in a wide range of image restoration tasks due
    to their exceptional ability to model data distributions. To solve image restoration
    problems, many existing techniques achieve data consistency by incorporating additional
    likelihood gradient steps into the reverse sampling process of diffusion models.
    However, the additional gradient steps pose a challenge for real-world practical
    applications as they incur a large computational overhead, thereby increasing
    inference time. They also present additional difficulties when using accelerated
    diffusion model samplers, as the number of data consistency steps is limited by
    the number of reverse sampling steps. In this work, we propose a novel diffusion-based
    image restoration solver that addresses these issues by decoupling the reverse
    process from the data consistency steps. Our method involves alternating between
    a reconstruction phase to maintain data consistency and a refinement phase that
    enforces the prior via diffusion purification. Our approach demonstrates versatility,
    making it highly adaptable for efficient problem-solving in latent space. Additionally,
    it reduces the necessity for numerous sampling steps through the integration of
    consistency models.
- poster: 140070
  presenter: "Li, Wei"
  email: "9d5b19c8f622e278"
  title: "Deep Operator Learning for Modeling Li-Ion Batteries"
  abstract: |-
    The widespread adoption of Li-ion batteries highlights their importance
    in modern energy storage systems. However, their complex multi-physics and multi-scale
    nature poses significant challenges for accurate modeling and computation.  One
    of the primary challenges in Li-ion battery modeling is finding the balance between
    accuracy and efficiency. Detailed models encompassing all physics require high
    computational resources, while simplified models compromise accuracy.  Deep operator
    learning offers a promising solution by integrating machine learning with scientific
    computing.  In this poster, we demonstrate the efficacy of this approach with
    two examples. Firstly, we propose a physics-informed deep operator neural network
    (Phase-Field DeepONet) for fast simulations of pattern formation governed by gradient
    flows of free-energy functionals. This is achieved by employing the DeepONet as
    a time-stepper to approximate the dynamic response of a phase field and introducing
    the free energy into the training loss.  Secondly, we develop a DeepONet-based
    computation framework for a homogenized model for all-solid-state batteries. This
    can accelerate the simulation by replacing the conventional time-consuming numerical
    methods for solving the mass transport equation and the reaction kinetics equation
    with DeepONets.
- poster: 140095
  presenter: "Li, Jr-Shin"
  email: "4a1608b60f0a5bfd"
  title: |-
    Filtrated Reinforcement Learning for Infinite-Dimensional Systems Using Moment
    Kernelization
  abstract: |-
    Despite the prosperity of research into reinforcement learning (RL), methods
    for designing optimal control policies for high-dimensional deterministic systems
    remain underdeveloped. The primary challenge lies in the curse of dimensionality,
    which results in high computational costs and low learning accuracy. To fill this
    literature gap, in this work, we study reinforcement learning problems for control
    systems defined on an infinite-dimensional function space. This class of systems,
    referred to as the ensemble system, arises in diverse scientific and application
    domains, ranging from quantum science and robotics to neuroscience. We develop
    a moment kernel transform, which maps an ensemble system to an associated moment
    system defined on a reproducing kernel Hilbert space (RKHS) consisting of infinite
    sequences. The RKHS structure enables the approximation of an ensemble system
    using a finite-dimensional truncated moment system. We further reveal a filtration
    structure formed by a sequence of truncated moment systems with increasing truncation
    orders. Using this structure, we show that the sequences of control policies and
    value functions learned from the sequence of systems converge to those of the
    ensemble system. This gives rise to a filtrated RL algorithm with a convergence
    guarantee. The computational efficiency and performance of the proposed algorithm
    are demonstrated using emerging applications in quantum science and robotics.
- poster: 140145
  presenter: "Li, Yuchen"
  email: "213dc7ade027521"
  title: |-
    MS89: Transformers Are Uninterpretable with Myopic Methods: a Case Study
    with Bounded Dyck Grammars
  abstract: "Interpretability methods aim to understand the algorithm implemented by a trained model (e.g., a Transofmer) by examining various aspects of the model, such as the weight matrices or the attention patterns. In this work, through a combination of theoretical results and carefully controlled experiments on synthetic data, we take a critical view of methods that exclusively focus on individual parts of the model, rather than consider the network as a whole. We consider a simple synthetic setup of learning a (bounded) Dyck language. Theoretically, we show that the set of models that (exactly or approximately) solve this task satisfy a structural characterization derived from ideas in formal languages (the pumping lemma). We use this characterization to show that the set of optima is qualitatively rich; in particular, the attention pattern of a single layer can be “nearly randomized', while preserving the functionality of the network. We also show via extensive experiments that these constructions are not merely a theoretical artifact: even after severely constraining the architecture of the model, vastly different solutions can be reached via standard training. Thus, interpretability claims based on inspecting individual heads or weight matrices in the Transformer can be misleading."
- poster: 140247
  presenter: "Li, Yuchen"
  email: "213dc7ade027521"
  title: "MS59: How Do Transformers Learn Topic Structure: Towards a Mechanistic Understanding"
  abstract: "While the successes of transformers across many domains are indisputable, accurate understanding of the learning mechanics is still largely lacking. Their capabilities have been probed on benchmarks which include a variety of structured and reasoning tasks—but mathematical understanding is lagging substantially behind. Recent lines of work have begun studying representational aspects of this question: that is, the size/depth/complexity of attention-based networks to perform certain tasks. However, there is no guarantee the learning dynamics will converge to the constructions proposed. In our paper, we provide fine-grained mechanistic understanding of how transformers learn “semantic structure”, understood as capturing co-occurrence structure of words. Precisely, we show, through a combination of mathematical analysis and experiments on Wikipedia data and synthetic data modeled by Latent Dirichlet Allocation (LDA), that the embedding layer and the self-attention layer encode the topical structure. In the former case, this manifests as higher average inner product of embeddings between same-topic words. In the latter, it manifests as higher average pairwise attention between same-topic words. The mathematical results involve several assumptions to make the analysis tractable, which we verify on data, and might be of independent interest as well."
- poster: 139562
  presenter: "Li, Yuchen"
  email: "e6e0ccd048abc395"
  title: "MS14: Stochastic Block Majorization-Minimization Methods on Riemannian Manifolds"
  abstract: |-
    Block majorization-minimization (BMM) is a simple iterative algorithm
    for nonconvex optimization that sequentially minimizes a majorizing surrogate
    of the objective function in each block coordinate while the other block coordinates
    are held fixed. Recently, the Riemannian counterpart of BMM has been studied,
    where each block is a Riemannian manifold.  In this project, we consider an online
    version of Riemannian BMM, which proceeds by sampling new data points and recursively
    minimizing the empirical loss function. We analyze the convergence and complexity
    of the proposed algorithm and provide numerical experiments to validate its effectiveness.
    This poster is associated with the mini-symposium entitled Probabilistic Methods
    in Machine Learning and Complex Systems.
- poster: 139898
  presenter: "Li, Yao"
  email: "43552a77254a6318"
  title: "MS36: Deep Neural Network Solver for Fokker-Planck Eigenfunctions"
  abstract: |-
    Many coupled neuronal oscillators are modeled by high-dimensional stochastic
    differential equations. It is well known that eigenvalues and eigenfunctions of
    the infinitesimal generator of the stochastic differential equation play important
    roles in stochastic stability, sensitivity analysis, and model reductions. However,
    eigenvalue problems are difficult to compute, particularly in higher dimensional
    cases. In this poster, I will explain how to modify the neural network Fokker-Planck
    solver for the eigenvalue and eigenfunction problem. The key idea is to use a
    nonlinear regression to find a low-resolution Monte Carlo solution of eigenfunctions.
    Then the eigenfunction can be solved by a PINN-like neural network.
- poster: 139594
  presenter: "Li, Yuchen"
  email: "e6e0ccd048abc395"
  title: |-
    MS49: Convergence and Complexity Guarantee for Inexact First-Order Riemannian
    Optimization Algorithms
  abstract: |-
    We are interested in analyzing inexact Riemannian gradient descent (RGD)
    where Riemannian gradients and retractions are inexactly (and cheaply) computed.
    Our focus is on understanding when inexact RGD converges and what is the complexity
    in the general nonconvex and constrained setting. We answer these questions in
    a general framework of tangential Block Majorization-Minimization (tBMM). We establish
    that tBMM converges to an $\epsilon$-stationary point within $O(\epsilon^{-2})$
    iterations. Under a mild assumption, the results still hold when the subproblem
    is solved inexactly in each iteration provided the total optimality gap is bounded.
    Our general analysis applies to a wide range of classical algorithms with Riemannian
    constraints including inexact RGD and proximal gradient method on Stiefel manifolds.
    We numerically validate that tBMM converges faster than the classical algorithm
    when applied to low-rank matrix recovery, nonnegative tensor decomposition, and
    constrained matrix factorization. This poster is associated with the section entitled
    "Efficient and robust optimization techniques for structured data learning" (80608).
- poster: 139683
  presenter: "Li, Tao"
  email: "1d3e0d28e0dab65d"
  title: |-
    Automated Security Response Through Conjectural Online Learning under Information
    Asymmetry
  abstract: |-
    Stochastic games arise in many complex socio-technical systems, particularly
    in security contexts, where the defender and the attacker interact under asymmetric
    information feedback. Existing computational  methods for asymmetric information
    stochastic games (AISGs) are primarily offline and can not adapt to equilibrium
    deviations. The resulting defense strategies are inadequate when facing online
    nonstationary attacks. To address these limitations, we propose conjectural online
    learning (COL), an online method for generic AISGs. COL uses a forecaster-actor-critic
    (FAC) architecture, where subjective forecasts are used to conjecture the opponents'
    strategies within a lookahead horizon and Bayesian learning is used to calibrate
    the conjectures. To adapt strategies to nonstationary environments, COL uses online
    rollout with cost function approximation (actor-critic). We prove that the conjectures
    produced by COL are asymptotically consistent with the information feedback in
    the sense of a relaxed Bayesian consistency. We also prove that the empirical
    strategy profile induced by COL converges to the Berk-Nash equilibrium. We evaluate
    our method in a simulated IT infrastructure through an advanced persistent threat
    use case. COL produces effective security strategies adapting to a changing environment
    and enjoys faster convergence than current reinforcement learning techniques.
- poster: 139794
  presenter: "Li, Sixu"
  email: "1d6ba3553bb9f173"
  title: |-
    MS27: Bilevel-Fedcbo: Robust Clustered Federated Learning Through Bilevel
    Consensus-Based Optimization
  abstract: "Federated learning is an important framework in modern machine learning that seeks to integrate the training of learning models from multiple users, each user with their own local data set, in a way that is sensitive to the users’ data privacy and to communication cost constraints. In clustered federated learning, one assumes an additional unknown group structure among users, and the goal is to train models that are useful for each group, rather than training a single global model for all users. We propose a novel solution to the problem of clustered federated learning that is inspired by ideas in consensus-based optimization (CBO). Our new CBO-type method is based on a system of interacting particles that is oblivious to group memberships. Our algorithm is accompanied by theoretical justification that is illustrated by real data experiments. Motivated from an additional point of concern in federated learning: the vulnerability of federated learning protocols to “backdoor” adversarial attacks, we further introduce a modified, improved particle system with enhanced robustness properties that, at an abstract level, can be interpreted as a bi-level optimization algorithm based on interacting particle dynamics. This poster is based on joint works with Nicolás García Trillos, Konstantin Riedl, and Yuhua Zhu."
- poster: 139792
  presenter: "Liang, Ying"
  email: "a4b5076456bc0cd7"
  title: "MS70: Data-Assisted Algorithms for Inverse Random Source Problems"
  abstract: "Inverse source scattering problems are essential in various fields, including antenna synthesis, medical imaging, and earthquake monitoring. In many applications, it is necessary to consider uncertainties in the model, and such problems are known as stochastic inverse problems. Traditional methods require a large number of realizations and information on medium coefficients to achieve accurate reconstruction for inverse random source problems. To address this issue, we propose a data-assisted approach that uses boundary measurement data to reconstruct the statistical properties of the random source with fewer realizations. We compare the performance of different data-driven algorithms under this framework to enhance the initial approximation obtained from integral equations. Our numerical experiments demonstrate that the data-assisted approach achieves better reconstruction with only 1/10 of the realizations required by traditional methods. Among the various Image-to-Image translation algorithms that we tested, the pix2pix method outperforms others in reconstructing well-separated inclusions with accurate positions. Our proposed approach results in stable reconstruction with respect to the observation data noise."
- poster: 139733
  presenter: "Liang, Siming"
  email: "5977f30bd160082b"
  title: |-
    MS23: Ensemble Score Filter for Tracking Turbulent Atmosphere Dynamics -
    Generative Ai in Data Assimilation
  abstract: |-
    In this study, we introduced a stable and highly efficient implementation
    of the Ensemble Score Filter (EnSF) for sequential data assimilation (DA) with
    geophysical systems. As a diffusion model-based generative AI approach for DA,
    the score filter can effectively store the information of filtering density in
    the score model, and the EnSF adopts an ensemble approximation scheme to efficiently
    approximate the filtering density scores. To showcase its advantageous performance,
    we compared EnSF with a benchmark Local Ensemble Transform Kalman Filter (LETKF)
    in DA for a surface quasi-geostrophic (SQG) model, and we have demonstrated the
    superior performance of EnSF over LETKF in scenarios involving incomplete knowledge
    of the state dynamical model and nonlinear/partial observations.
- poster: 139865
  presenter: "Liao, Wenjing"
  email: "51b59913f7a0adad"
  title: "MS11: Exploiting Low-Dimensional Data Structures in Deep Learning"
  abstract: |-
    In the past decade, deep learning has made astonishing breakthroughs in
    various real-world applications. It is a common belief that deep neural networks
    are good at learning various geometric structures hidden in data sets. One of
    the central interests in deep learning theory is to understand why deep neural
    networks are successful, and how they utilize low-dimensional data structures.
    In this talk, I will present some statistical learning theory of deep neural networks
    where data are concentrated on or near a low-dimensional manifold. The learning
    tasks include regression, classification, feature representation and operator
    learning. When data are sampled on a low-dimensional manifold, the sample complexity
    crucially depends on the intrinsic dimension of the manifold instead of the ambient
    dimension of the data. These results demonstrate that deep neural networks are
    adaptive to low-dimensional geometric structures of data sets.
- poster: 139874
  presenter: "Liao, Wenjing"
  email: "51b59913f7a0adad"
  title: |-
    MS40: A Manifold Two-Sample Test Study: Integral Probability Metric with
    Neural Networks
  abstract: ""
- poster: 139659
  presenter: "Lim, Tongseok"
  email: "78228d74584a685f"
  title: "Maximizing Marginal Variance in Martingales for Unsupervised Learning"
  abstract: |-
    We present an unsupervised statistical learning approach that investigates
    all martingale couplings with one variable and one fixed marginal. In this framework,
    we propose that the variable marginal distribution, which maximizes variance,
    serves as a solution to the learning objective. We illustrate the applicability
    of this approach in a variety of unsupervised learning contexts, such as data
    clustering and principal curve and surface inference. We establish the existence
    of solutions to our optimization scheme and provide consistency result. Additionally,
    we show that a specific instance of our method is equivalent to classical principal
    component analysis (PCA), implying that our approach generalizes PCA.
- poster: 140266
  presenter: "Lin, Hao"
  email: "7cf820f5710f8944"
  title: "Major Or Minor"
  abstract: |-
    This project aimed to determine whether a song was major or minor with
    machine learning. To achieve this, we used a large dataset to have training data
    and testing data. I used about 80% of the data to train and 20% to test. This
    helps the model to be adequately ready to classify songs into their respective
    categories. I used data such as energy, loudness, and tempo to help the model
    determine a possible category. I used several different machine learning algorithms
    to determine the final classification. These models included a decision tree,
    knn learning, and neural network. The highest performing algorithm being the decision
    tree with a classification accuracy of 72%. In conclusion while the results could
    vary, a machine learning model can classify songs based on a major or minor key
    fairly accurately.
- poster: 139719
  presenter: "Lindberg, Julia"
  email: "d632b288e0a73e1d"
  title: "MS78: Real Circles Tangent to Three Conics."
  abstract: |-
    In this paper we study circles tangent to conics. We show there are generically
    184 complex circles tangent to three conics in the plane and we characterize the
    real discriminant of the corresponding polynomial system. We give an explicit
    example of 3 conics with 136 real circles tangent to them. We conjecture that
    136 is the maximal number of real circles. Furthermore, we implement a hill-climbing
    algorithm to find instances of conics with many real circles, and we introduce
    a machine learning model that, given three real conics, predicts the number of
    circles tangent to these three conics.
- poster: 140151
  presenter: "Liu, Shu                    "
  email: "9b8ca9a716bdd1b5"
  title: "MS61: Parametrized Wasserstein Hamiltonian Flow"
  abstract: "In this work, we propose a numerical method to compute the Wasserstein Hamiltonian flow (WHF), which is a Hamiltonian system on the probability density manifold. Many well-known PDE systems can be reformulated as WHFs such as the Schrödinger equation. We use parameterized functions as push-forward maps to characterize the solution of WHF, and convert the PDE to a finite-dimensional ODE system, which is a Hamiltonian system in the phase space of the parameter manifold. We establish theoretical error bounds for the continuous time approximation in Wasserstein metric. For the numerical implementation, neural networks are used as push-forward maps. We design an effective symplectic scheme to solve the derived Hamiltonian ODE system so that the method preserves desirable quantities such as Hamiltonian. The computation is done by a fully deterministic symplectic integrator without any neural network training. The proposed algorithm is a sampling-based approach that scales well to higher dimensional problems. In addition, the method also provides an alternative connection between the Lagrangian and Eulerian perspectives of the original WHF through the parameterized ODE dynamics."
- poster: 139681
  presenter: "Liu, Shikun"
  email: "4daf1abd97d58e40"
  title: "MS77: Pairwise Alignment Improves Graph Domain Adaptation"
  abstract: |-
    Graph-based methods, pivotal for label inference  over interconnected
    objects in many real-world  applications, often encounter generalization challenges,
    if the graph used for model training differs significantly from the graph used
    for testing.  This work delves into Graph Domain Adaptation  (GDA) to address
    the unique complexities of distribution shifts over graph data, where interconnected
    data points experience shifts in features,  labels, and in particular, connecting
    patterns. We  propose a novel, theoretically principled method,  Pairwise Alignment
    (Pair-Align) to counter graph  structure shift by mitigating conditional structure  shift
    (CSS) and label shift (LS). Pair-Align uses  edge weights to recalibrate the influence
    among  neighboring nodes to handle CSS and adjusts the  classification loss with
    label weights to handle LS.  Our method demonstrates superior performance  in
    real-world applications, including node classification with region shift in social
    networks, and  the pileup mitigation task in particle colliding experiments. For
    the first application, we also curate  the largest dataset by far for GDA studies.
    Our  method shows strong performance in synthetic  and other existing benchmark
    datasets.
- poster: 139834
  presenter: "Liu, Lizuo"
  email: "ee28048454fa06ee"
  title: |-
    MS43: DeepPropNet - A Recursive Deep Neural Network Propagator for Learning
    Evolutionary Pde Operators
  abstract: |-
    In this poster, we propose a deep neural network ap-  proximation to the
    evolution operator for time dependent PDE sys-  tems over long time period by
    recursively using one single neural  network propagator, in the form of POD-DeepONet
    with built-in  causality feature, for a small time interval. The trained DeepPropNet
    of moderate size is shown to give accurate prediction of wave solutions over the
    whole time interval.
- poster: 140074
  presenter: "Liu, Siting"
  email: "9e0dc37f2f2918bd"
  title: |-
    MS3: Wasserstein Proximal Operators Describe Score-Based Generative Models
    and Resolve Memorization
  abstract: |-
    We focus on the fundamental mathematical structure of score-based generative
    models (SGMs). We formulate SGMs in terms of the Wasserstein proximal operator
    (WPO) and demonstrate that, via mean-field games (MFGs), the WPO formulation reveals
    mathematical structure that describes the inductive bias of diffusion and score-based
    models. In particular, MFGs yield optimality conditions in the form of a pair
    of coupled PDEs: a forward-controlled Fokker-Planck (FP) equation, and a backward
    Hamilton-Jacobi-Bellman (HJB) equation. Via a Cole-Hopf transformation and taking
    advantage of the fact that the cross-entropy can be related to a linear functional
    of the density, we show that the HJB equation is an uncontrolled FP equation.
    Next, with the mathematical structure at hand, we present an interpretable kernel-based
    model for the score function which dramatically improves the performance of SGMs
    in terms of training samples and training time. The WPO-informed kernel model
    is explicitly constructed to avoid the recently studied memorization effects of
    score-based generative models. The mathematical form of the new kernel-based models
    in combination with the use of the terminal condition of the MFG reveals new explanations
    for the manifold learning and generalization properties of SGMs, and provides
    a resolution to their memorization effects. Our mathematically informed kernel-based
    model suggests new scalable bespoke neural network architectures for high-dimensional
    applications.
- poster: 139782
  presenter: "Liu, Chaoyue"
  email: "7ac216dbcbb2122b"
  title: |-
    MS59: Non-Linear Activation Soothes Ntk Conditioning for Wide Neural Networks:
    a Study in the Relu Case
  abstract: |-
    Non-linear activation functions are well known to improve the expressivity
    of neural networks, which is the main reason of their wide implementation in neural
    networks. In this work, we showcase a new and interesting property of certain
    non-linear activations, focusing on the most popular example of its kind -- Rectified
    Linear Unit (ReLU). By comparing the cases with and without this non-linear activation,
    we show that the ReLU has the following effects: (a) better data separation, i.e.,  a
    larger angle separation for similar data in the feature space of model gradient,
    and (b) better NTK conditioning, i.e.,  a smaller condition number of neural tangent
    kernel (NTK). Furthermore, we show that the ReLU network depth (i.e., with more
    ReLU activation operations) further magnifies these effects. Note that, without
    the non-linear activation, i.e., in a linear neural network, the data separation
    and NTK condition number always remain the same as in the case of a linear model,
    regardless of the network depth.     Our results imply that  ReLU activation,
    as well as the depth of ReLU network, helps improve the worst-case convergence
    rate of GD, which is closely related to the NTK condition number.
- poster: 139869
  presenter: "Liu, Hao"
  email: "38a38e5038680d9"
  title: |-
    MS40: Exploring Low-Dimensional Data Structures by Neural Networks with
    Application on Operator Learning
  abstract: ""
- poster: 139608
  presenter: "Liu, Jialin"
  email: "c7558e5d0571e4ed"
  title: |-
    MS20: Associated Poster: Towards Constituting Mathematical Structures for
    Learning to Optimize
  abstract: |-
    Learning to Optimize (L2O), a technique that utilizes machine learning
    to learn an optimization algorithm automatically from data, has gained arising
    attention in recent years. A generic L2O approach parameterizes the iterative
    update rule and learns the update direction as a black-box network. While the
    generic approach is widely applicable, the learned model can overfit and may not
    generalize well to out-of-distribution test sets. In this paper, we derive the
    basic mathematical conditions that successful update rules commonly satisfy. Consequently,
    we propose a novel L2O model with a mathematics-inspired structure that is broadly
    applicable and generalized well to out-of-distribution problems. Numerical simulations
    validate our theoretical findings and demonstrate the superior empirical performance
    of the proposed L2O model.
- poster: 139903
  presenter: "Liu, Yicheng"
  email: "426075aeb7733c6c"
  title: |-
    MS45: Deep Neural Network Solver for the Wkb Approach to the Fokker-Planck
    Equation
  abstract: |-
    Many biological systems are modeled by stochastic differential equations,
    with probability density functions governed by the Fokker-Planck equation. Although
    there are many recent advancements in the numerical solution of the Fokker-Planck
    equation, solving the weak noise Fokker-Planck equation remains challenging due
    to its highly concentrated solutions. We propose a deep-WKB framework for solving
    singular Fokker-Planck equations. Two neural networks are used to solve the Freidlin-Wentzell
    quasi-potential function and the pre-factor function separately. Then, two neural
    networks are re-trained for the Fokker-Planck equation itself.
- poster: 139618
  presenter: "Liu, Yuxuan"
  email: "c9d71bc10c7fcf0d"
  title: |-
    MS31: PROSE: Predicting Multiple Operators and Symbolic Expressions Using
    Multimodal Transformers
  abstract: |-
    Approximating nonlinear differential equations using a neural network
    provides a robust and efficient tool for various scientific computing tasks. Observing
    that families of differential equations often share key characteristics, we seek
    one network representation across a wide range of equations. Our multimodal approach,
    called Predicting Multiple Operators and Symbolic Expressions (PROSE), is capable
    of constructing multiple operators and governing equations simultaneously through
    a novel fusion structure. In particular, PROSE solves differential equations,
    predicts future states, and generates the underlying equations of motion by incorporating
    symbolic "words" through a language model. Experiments with 25600 equations show
    that PROSE benefits from its multimodal nature, resulting in robust generalization
    (e.g. noisy observations, equation misspecification, and data imbalance) supported
    by comparison and ablation studies. PROSE provides a new operator learning framework
    incorporating multimodal input/output and language models for solving forward
    and model-discovery problems related to differential equations.
- poster: 140011
  presenter: "Liu, Yanfang"
  email: "45849ba2cf689db8"
  title: |-
    MS58: Supervised Learning of Flow Map for Modeling Stochastic Dynamical
    Systems
  abstract: |-
    We present a supervised learning technique to accurately learn unknown
    stochastic dynamical systems from their trajectory data. Our diffusion-based generative
    model is a training-free score estimation method that leverages the Monte Carlo
    estimator to directly approximate the score function. With the estimated score
    function, labeled data can be generated by solving  an ordinary differential equation
    (ODE), corresponding to the reverse-time stochastic differential equation (SDE).
    The generated labeled data allows us to employ a simple fully connected neural
    network to learn the generative model in the supervised manner. Our proposed method
    is efficient in terms of training time and demonstrates high accuracy, as proven
    by a range of numerical experiments on stochastic systems.
- poster: 140557
  presenter: "Liu, Zhexuan"
  email: "36258ba132116cf0"
  title: |-
    MS75: Assessing and Improving Accountability of Stochastic Neighbor Embedding
    Methods: A Map-Continuity Perspective
  abstract: |-
    Visualizing high-dimensional data is an important routine for understanding
    biomedical data and interpreting deep learning models. Neighbor embedding methods
    such as t-SNE and UMAP are popular visualization methods, but recent studies suggest
    that they are likely to produce visual artifacts and lead to incorrect scientific
    conclusions. We find that, perhaps disturbingly, the embedding map displays discontinuity
    across many datasets. We identify two roots of observed discontinuity---namely
    overconfidence-inducing discontinuity and fracture-inducing discontinuity---that
    lead to overconfident clusters and spurious structures. Based on a statistical
    technique known as leave-one-out, we propose two metrics to remedy these issues:
    perturbation score and singularity score. Experiments on datasets across several
    domains show that the two metrics identify dubious embedding points, aid hyperparameter
    selection, and improve visualization diagnostics.
- poster: 139581
  presenter: "Llosa, Carlos"
  email: "7627a2957b8a59a"
  title: |-
    On a Latent-Variable Formulation of the Poisson Canonical Polyadic Tensor
    Model
  abstract: |-
    The Poisson Canonical Polyadic (PCP) tensor model is a popular tool for
    the analysis of high-dimensional tensors composed elementwise of counts, but many
    of its statistical properties are unknown due to a loglikelihood that is challenging
    to differentiate.  We revisit the PCP tensor model as a latent variable model,
    and leverage the missing information principle to obtain the Fisher information
    matrix, which can be used to (1) gauge the well-posedness or identifiability of
    the model, (2) derive second-order Fisher scoring algorithms for maximum likelihood
    estimation and to (3) derive Cramer-Rao lower bounds on the variance of unbiased
    estimators. We also re-discover existing non-negative matrix and tensor factorization
    algorithms within our latent-variable framework as expectation-maximization algorithms,
    and study our derived properties in extensive simulation studies.
- poster: 140271
  presenter: "Lodge, Tiffany"
  email: "500cb47a3d90aeaa"
  title: |-
    MS25: Interpretable Inference of Drift and Control Vector Fields of Stochastic
    Dynamical Systems
  abstract: |-
    In this presentation, we will discuss a novel approach for inferring drift
    and control vector fields of a dynamic system independently represented by a stochastic
    differential equation. First, we propose an optimal perturbation scheme to generate
    rich data online for identifying the control vector field. In the second step,
    we formulate the learning problems to unambiguously approximate the vector field
    without passively relying on the persistency of the excitation condition. We leverage
    the structural and dynamic properties of the autonomous agents to develop the
    perturbation scheme for data-driven inference. The optimal number of data points
    for the inference is determined online to meet the required approximation accuracy.
    The learning problem is formulated using the pseudospectral method, specifically
    the Chebyshev polynomial of the second kind. The convergence of the parameter
    identification scheme is guaranteed using the Lyapunov approach in probability.
    Finally, simulation results will demonstrate the effectiveness of our interpretable
    inference scheme.
- poster: 139585
  presenter: "Lok, Jackie"
  email: "588c637d55880bed"
  title: |-
    MS67: A Subspace Constrained Randomized Kaczmarz Method for Structure or
    External Knowledge Exploitation
  abstract: |-
    We study a version of the randomized Kaczmarz algorithm for solving linear
    systems where the iterates are confined to the solution space of a selected subsystem.
    We show that the subspace constraint leads to an accelerated convergence rate,
    especially when the system has approximately low-rank structure. On Gaussian-like
    random data, we show that it results in a form of dimension reduction that effectively
    increases the aspect ratio of the system. Furthermore, this method serves as a
    building block for a second, quantile-based algorithm for solving linear systems
    with arbitrary sparse corruptions, which is able to efficiently utilize external
    knowledge about corruption-free equations and achieve convergence in difficult
    settings, such as in almost-square systems. Numerical experiments on synthetic
    and real-world data support our theoretical results and demonstrate the validity
    of the proposed methods for even more general data models than guaranteed by the
    theory.
- poster: 139623
  presenter: "Lopez, Oscar"
  email: "984fda7fb8780462"
  title: "The Average Spectrum Norm for Tensor Completion Analysis"
  abstract: |-
    We introduce the average spectrum norm to study sample complexity of tensor
    completion problems based on the canonical polyadic decomposition (CPD). Our novel
    approach significantly reduces the provable sample rate for CPD-based noisy tensor
    completion, providing the best bounds to date on the number of observed noisy
    entries required to produce an arbitrarily accurate estimate of an underlying
    mean value tensor. Under Poisson and Bernoulli multivariate distributions, we
    show that an $N$-way CPD rank-$R$ parametric tensor $\boldsymbol{\mathscr{M}}\in\mathbb{R}^{I\times
    \cdots\times I}$ generating noisy observations can be approximated by large likelihood
    estimators from $\mathcal{O}(IR^2\log^{N+2}(I))$ revealed entries. Furthermore,
    under nonnegative and orthogonal versions of the CPD we improve the result to
    depend linearly on the rank, achieving the near-optimal rate $\mathcal{O}(IR\log^{N+2}(I))$.
- poster: 140302
  presenter: "Lopez-Castaño, Daniel"
  email: "18d0e4905ed37d6c"
  title: "MS83: Wasserstein Factor Analysis"
  abstract: ""
- poster: 139881
  presenter: "Lou, Mengqi"
  email: "9d1d96053d4eeec"
  title: |-
    MS83: Hyperparameter Tuning via Trajectory Predictions: Stochastic Prox-linear
    Methods in Matrix Sensing
  abstract: |-
    Motivated by the desire to understand stochastic algorithms for nonconvex
    optimization that are robust to their hyperparameter choices, we analyze a mini-batched
    prox-linear iterative algorithm for the problem of recovering an unknown rank-1
    matrix from rank-1 Gaussian measurements corrupted by noise. We derive a deterministic
    recursion that predicts the error of this method and show, using a non-asymptotic
    framework, that this prediction is accurate for any batch-size and a large range
    of step-sizes. In particular, our analysis reveals that this method, though stochastic,
    converges linearly from a local initialization with a fixed step-size to a statistical
    error floor. Our analysis also exposes how the batch-size, step-size, and noise
    level affect the (linear) convergence rate and the eventual statistical estimation
    error, and we demonstrate how to use our deterministic predictions to perform
    hyperparameter tuning (e.g. step-size and batch-size selection) without ever running
    the method. On a technical level, our analysis is enabled in part by showing that
    the fluctuations of the empirical iterates around our deterministic predictions
    scale with the error of the previous iterate.
- poster: 139748
  presenter: "Lu, Lu"
  email: "3c297c5153661e5a"
  title: "MS43: One-Shot Learning for Solution Operators of Partial Differential Equations"
  abstract: |-
    Learning and solving governing equations of a physical system, represented
    by partial differential equations (PDEs), from data is a central challenge in
    a variety of areas of science and engineering. Current methods require either
    some prior knowledge (e.g., candidate PDE terms) to discover the PDE form, or
    they need a large dataset to learn a surrogate model of the PDE solution operator.
    Here, we propose the first solution operator learning method that only requires
    one PDE solution, i.e., one-shot learning. We first decompose the entire computational
    domain into small domains, where we learn a local solution operator, and then
    we find the solution of a new input function via mesh-based fixed-point iteration
    (FPI), meshfree local-solution-operator informed neural network (LOINN) or local-solution-operator
    informed neural network with correction (cLOINN). We tested our method on 7 different
    PDEs, including linear or nonlinear PDEs, PDEs defined on complex geometries,
    and PDE systems. Our method demonstrates effectiveness and generalization capabilities
    across these varied scenarios.
- poster: 140229
  presenter: "Lu, Xiaoyi"
  email: "f8fdb8d0731f9275"
  title: |-
    MS31: Accelerating Radiation Calculations for Fire Safety Applications Using
    DeepONet
  abstract: |-
    Computational Fluid Dynamics (CFD) simulations of industrial fires model
    and simulate all the complex physical phenomena in fires, including combustion,
    heat transfer, pyrolysis, turbulence, and water suppression. These CFD simulations
    are computationally expensive, and a moderate-to-large scale simulation takes
    several days to over two months. One major performance bottleneck is in solving
    radiative transport equations (RTEs) which contributes up to 90% of total computational
    time. This work leverages deep operator networks (DeepONets) to develop a fast
    surrogate to accelerate radiation calculations. The proposed model is trained
    to learn the nonlinear mapping between radiative absorption and emission and radiative
    intensity, which corresponds to RTE solutions. We show that the trained model
    approximates the solution operator of the RTE and can provide accurate radiation
    predictions at an order-of-magnitude faster rate than traditional numerical models.
    Special session: Recent Advances of Operator Learning and Foundation-Model-Assisted
    Multi-Operator Learning  Organizers: Zecheng Zhang, and Lu Lu
- poster: 139924
  presenter: "Lun, Zhixin"
  email: "5fe7277af08cd930"
  title: |-
    A General Approach for Imputation of Non-Normal Continuous Data Based on
    Copula Transformation
  abstract: |-
    Dealing with missing data problems for skewed data is a difficult task
    especially since most of the imputation and data augmentation methodologies assume
    multivariate normality. The performance of imputation and hence the accuracy of
    inference on parameters become questionable when the violation of the above assumption
    occurs. One approach to solve the normality violation is to apply normalizing
    transformation prior to the imputation phase. However, this approach may introduce
    new problems such as altering the dependence structure among random variables.
    We present a general purpose multiple imputation approach based on Copula transformation.
    The approach is used to effectively transform any continuous multivariate non-normal
    data to multivariate normal, thereby allowing the imputation using standard normality-based
    techniques. The method then allows to conveniently back transform the data into
    original space. Real data sets are used to illustrate the techniques. We then
    compare the performance of our Copula-based method with other traditional normality
    based multiple imputation approaches through extensive simulated and real non-normal
    multivariate datasets. We demonstrate that this method significantly mitigates
    the problem and hence the practice of making the blind assumption of multivariate
    normality for non-normal multivariate data under the assumption that data are
    missing at different mechanisms.
- poster: 139880
  presenter: "Luner, Alan"
  email: "bb70349b357a8c37"
  title: "MS83: On Averaging and Extrapolation for Gradient Descent"
  abstract: |-
    This work considers the effect of averaging, and more generally extrapolation,
    of the iterates of gradient descent in smooth convex optimization. After running
    the method, rather than reporting the final iterate, one can report either a convex
    combination of the iterates (averaging) or a generic combination of the iterates
    (extrapolation). For several common stepsize sequences, including recently developed
    accelerated periodically long stepsize schemes, we show averaging cannot improve
    gradient descent's worst-case performance and is, in fact, strictly worse than
    simply returning the last iterate. In contrast, we prove a conceptually simple
    and computationally cheap extrapolation scheme strictly improves the worst-case
    convergence rate: when initialized at the origin, reporting $(1+1/\sqrt{16Nlog(N)})x_N$
    rather than xN improves the best possible worst-case performance by the same amount
    as conducting $O(\sqrt{N/log(N)})$ more gradient steps. Our analysis and characterizations
    of the best-possible convergence guarantees are computer-aided, using performance
    estimation problems. Numerically, we find similar (small) benefits from such simple
    extrapolation for a range of gradient methods.
- poster: 140187
  presenter: "Luo, Dingcheng"
  email: "d3cc872b5ff89c2"
  title: "MS5: Dimension Reduction for Derivative Informed Operator Learning"
  abstract: |-
    Neural operators---neural network approximations of mappings between function
    spaces---have shown promise as computationally efficient approximations of PDE
    solution operators, and can be deployed as surrogates in many query-intensive
    tasks such as optimization and uncertainty quantification, bayesian inverse problems,
    optimal experimental design.  Conventional operator learning typically focuses
    on the accuracy of the evaluations, i.e. accuracy of the predicted PDE solutions
    over some input distribution.  However, the derivative of the solution operator
    also plays an important role in the accuracy and efficiency of algorithms for
    many of the tasks described above.  In this work, we present theoretical analysis
    and universal approximation results for neural operator in Sobolev classes over
    Gaussian measures. That is, we quantify both the function and derivative approximation
    errors for input-output maps between infinite dimensional spaces.  Specifically,
    we consider reduced basis architectures, and how compare different choices of
    dimension reduction, including principal component analysis as well as derivative
    informed dimension reduction strategies, affect the approximation errors.
- poster: 139653
  presenter: "Ma, Anna"
  email: "922173577079fbb9"
  title: "MS13: Iterative Approaches for Tensor Linear Systems"
  abstract: |-
    In this poster presentation, we introduce an iterative method for approximating
    the solution of large-scale multi-linear systems, represented in the form A*X=B
    under the tensor t-product. Unlike previously proposed randomized iterative strategies,
    such as the tensor randomized Kaczmarz method (row slice sketching) or the tensor
    Gauss-Seidel method (column slice sketching), which are natural extensions of
    their matrix counterparts, our approach delves into a distinct scenario utilizing
    frontal slice sketching. In particular, we explore a context where frontal slices,
    such as video frames, arrive sequentially over time, and access to only one frontal
    slice at any given moment is available. We will present our novel approach, shedding
    light on its applicability and potential benefits in approximating solutions to
    large-scale multi-linear systems.
- poster: 139920
  presenter: "Ma, Xiaohang"
  email: "45806e9eebf16b53"
  title: "MS82: Deep Optimal Control Approach to Path Estimation of Nonlinear Filtering"
  abstract: |-
    We developed a novel deep learning based numerical solution of the nonlinear
    filtering problem.       The approach begins by establishing an estimation measure
    via a controlled Markov process and then measuring the discrepancy between the
    posterior distribution of the nonlinear filter and the controlled measure using
    Kullback-Leibler (KL) divergence.        This framework leads to a variational
    problem for addressing the nonlinear filtering. Practically, the variational objective
    can often be articulated as the sum of a running cost and a terminal cost with
    respect to the controlled Markov process.        To solve this reformulated stochastic
    control problem, we introduce a training scheme for Deep FBSDE algorithm, which
    utilizes artificial neural networks to derive the optimal controller.         We
    numerically demonstrate the scalability and efficiency of our method in addressing
    the nonlinear filtering problem across both finite and infinite state space signal
    processes.
- poster: 140099
  presenter: "Ma, Ming"
  email: "d4afe73195a7a380"
  title: |-
    MS19: Medical Image Segmentation Using the Equivariance under Diffeomorphisms
    Framework
  abstract: |-
    Medical image segmentation plays a crucial role in medical image analysis,
    computer-aided detection and diagnosis, treatment planning, and etc. However,
    it is still challenging to obtain an accurate segmentation result due to irregularity
    of organ contours or lack of labeled dataset. In this work, we formulate an important
    property for image segmentation: equivariance under diffeomorphisms, that is,
    the segmentation results are independent of the small diffeomorphic deformations.
    Based on this property, we propose a novel Equivariance under Diffeomorphism (ED)
    framework for medical image segmentation using the optimal transport maps. Experiments
    are carried out to evaluate the proposed method on two publicly available datasets,
    including a colon dataset and a hepatic vessels dataset. Results show that the
    proposed method outperforms the existing method in terms of two metrics Jaccard
    and Dice, respectively.
- poster: 140260
  presenter: "Macchio, Gregory"
  email: "1bec6afe636a0adf"
  title: "Neural Networks for Nonlinear Dynamics: The Importance of Initial Transients"
  abstract: "The numerical simulation of high-dimensional nonlinear systems often demands extensive computational resources, spanning hours, days, or even weeks. To alleviate this computational burden, we may construct a reduced-order model capable of describing the essential input-output behavior of the original system. One technique for reduced-order modeling involves approximating each chart of the system's slow manifold with an autoencoder neural network. Subsequently, the dynamics on each chart's image are approximated by a neural ordinary differential equation. Once trained, the reduced-order model becomes operational. However, this method relies solely on post-transient data, limiting its effectiveness in modeling trajectories that originate off the slow manifold. To overcome this limitation, we propose an extended learning curriculum that incorporates a new training stage capable of modeling fast transients. Initially, we select a sampled trajectory point near the slow manifold. Then, utilizing that point as an initial condition, we simulate the reduced-order model backward in time. Finally, we utilize the backward time predictions as targets for a neural network responsible for encoding fast transients. To illustrate our approach, we construct two-dimensional reduced-order models for the complex Ginzburg–Landau equation in its supercritical regime and a Kolmogorov flow with a traveling wave solution."
- poster: 140097
  presenter: "Madhavan, Madhusudan"
  email: "cd72bf02135822b6"
  title: |-
    Optimal Experimental Design for Control Problems Governed by Pde-Constrained
    Bayesian Inverse Problems
  abstract: |-
    We propose a framework for Control-Oriented Optimal Experimental Design
    (OED) of linear PDE-constrained Bayesian inverse problems. In particular, we focus
    on optimal control problems that are parameterized by the solution of an inverse
    problem relying on experimental data gathered from sensors. While classical Bayesian
    OED techniques provide sensor placements that minimize the posterior uncertainty
    in the inversion parameter, these designs may not be ideal at minimizing uncertainty
    in the optimal control and the resulting final state. As experimental resources
    are often limited by cost or feasibility, one needs to prioritize the designs
    that reduce the uncertainty in the final goal. Hence, we propose design criteria
    and fast computational methods for finding sensor placements that minimize the
    end-goal uncertainty in optimal control. We present illustrative numerical experiments
    in the context of a convective heat transfer application, in which we reconstruct
    an ambient heat source term based on initial state measurements and, with the
    reconstructed dynamics, control a stationary source to attain a target state within
    a given period of time.
- poster: 140114
  presenter: "Madondo, Malvern"
  email: "d757ab7422656e10"
  title: |-
    MS21: Amortized Control of Pathological Neuronal Dynamics under Parametric
    Variations
  abstract: |-
    Neuronal dynamics, governed by the Hodgkin-Huxley (HH) equations, exhibit
    complex behaviors influenced by various parameters. Controlling these dynamics,
    particularly in pathological states, is challenging due to inherent electrophysiological
    variability and uncertainty in these parameters. Drawing inspiration from numerical
    approaches for optimal control problems with uncertain parameters, we consider
    a two-stage approach for tackling this problem. This involves offline amortization
    of the control solution over a wide parameter range and online adaptation of the
    control policy to specific settings. We leverage the system dynamics, objective
    function, and the Hamilton-Jacobi-Bellman equation to learn the value function
    and employ neural networks to represent policies for managing high-dimensional
    states and parameters. This approach enables us to devise closed-loop control
    policies enabling real-time adaptation to new or unseen scenarios without extensive
    retraining. Numerical experiments demonstrate the efficacy and robustness of our
    approach in managing perturbations within HH system dynamics.
- poster: 139990
  presenter: "Maheshwari, Aditya"
  email: "d7bfff76d413d8ab"
  title: "Prediction of Natural Disaster Data Using Fractional Count Processes"
  abstract: |-
    Modelling wildfire events has been studied in the literature using the
    Poisson process, which essentially assumes the independence of wildfire events.
    In this talk, we use the fractional Poisson process to model the wildfire occurrences
    in California between June 2019 - April 2023 and predict the wildfire events that
    explains the underlying memory between these events. We introduce  method of moments
    and maximum likelihood estimate approaches to estimate the parameters of the fractional
    Poisson process, which is an alternative to the method proposed by Cahoy (2010).
    We obtain the estimates of the fractional parameter as 0.8,  proving that the
    wildfire events are dependent. The proposed model has reduced prediction error
    by 90% compared to the classical Poisson process model.
- poster: 140571
  presenter: "Maia, Pedro"
  email: "d3568db7cc06afda"
  title: "Data-Driven Solutions for Neuroscience Challenges"
  abstract: "In this talk, we will explore a few challenges in computational neurology that we addressed with data-driven techniques. We have applied inverse-problem techniques to identify the origins of neurodegeneration and leveraged deep learning approaches in medical imaging to gain better diagnostic insights, particularly for Alzheimer’s and Parkinson's diseases. Finally, we will demonstrate how we can accurately reconstruct Local Field Potential time series from rodents using the Eigensystem Realization Algorithm (ERA). This technique from the field of system identification allowed us to identify a high-dimensional linear dynamical system governing the observed data. The complexity of the time series, typically reflected in the dimensionality of the inferred system, provides a measure of brain complexity over time. These techniques hold potential for broader application across various electrophysiological recording modalities."
- poster: 139645
  presenter: "Mandujano Reyes, Juan Francisco"
  email: "d4a1425f1754ea25"
  title: |-
    Spatiotemporal Causal Inference with Mechanistic Ecological Models: Evaluating
    Targeted Culling on Chronic Wasting Disease Dynamics in Cervids
  abstract: |-
    Spatiotemporal (ST) causal inference is needed to detect the effect of
    interventions on indirectly measured epidemiological outcomes going beyond studying
    ST correlations. Chronic wasting disease (CWD) causes neurological degeneration
    and death to white-tailed deer in Wisconsin. Targeted culling (TC) involves removing
    deer after traditional hunting seasons in high CWD prevalence areas. Evaluating
    the causal effect of TC in the spread and growth of CWD is an unresolved research
    and CWD management question that can guide surveillance efforts. Reaction-diffusion
    partial differential equations (PDEs) can mechanistically model underlying ST
    dynamics of wildlife diseases allowing us to make inference in unobserved epidemiological
    quantities. These models indirectly regress ST covariates on diffusion and growth
    rates parameterizing such PDEs obtaining associational conclusions. In this work,
    we develop an innovative method to obtain causal estimators for TC interventions
    on CWD epidemiological processes using inverse-probability-of-treatment-weighting
    by means of marginal structural models embedded in the PDE fitting process. Moreover
    we establish a novel scheme for sensitivity analysis under unmeasured confounder
    for testing the significance of the effect on indirectly measured epidemiological
    outcomes. Our methods can be used to study ST interventions in epidemiological
    evolution of infectious diseases helping to mitigate public health implications
    and disease burden
- poster: 139913
  presenter: "Manzonelli, Nico"
  email: "2028b1b33c1cc818"
  title: "MS46: Membership Inference Attacks and Privacy in Topic Modeling"
  abstract: |-
    Recent research shows that large language models are susceptible to privacy
    attacks that infer aspects of the training data. However, it is unclear if simpler
    generative models, like topic models, share similar vulnerabilities. In this work,
    we propose an attack against topic models that can confidently identify members
    of the training data in Latent Dirichlet Allocation. Our results suggest that
    the privacy risks associated with generative modeling are not restricted to large
    neural models. Additionally, to mitigate these vulnerabilities, we explore differentially
    private (DP) topic modeling. We propose a framework for private topic modeling
    that incorporates DP vocabulary selection as a pre-processing step, and show that
    it improves privacy while having limited effects on practical utility.
- poster: 139669
  presenter: "Mao, Haitao"
  email: "f877dd0256dbea5f"
  title: "MS77: Principle-Guided Graph Foundation Model Design"
  abstract: |-
    Graph Foundation Model outperforms vanilla GNNs which benefits from larger
    training scale with positive transferability. Nonetheless, the principle for how
    to achieve such positive transfer remains unclear. In this poster, I will present
    a graph vocabulary perspective to find the basic units in the graph domain, transferable
    across different tasks and datasets. Such vocabulary design should be guided by
    existing principles from network analysis, graph expressiveness, and network stability
    perspectives.
- poster: 140538
  presenter: "Marini, Michela"
  email: "c7524ff460865f8e"
  title: |-
    MS55: Predictive Analysis of Opioid-Addiction Behavior Using Morphological
    Analysis of Astrocytes
  abstract: "Astrocytes, a subtype of glial cells with a complex star-shaped structure, are active players in many aspects of the physiology of the Central Nervous System (CNS). They reflect their diverse abilities and functions on their complex morphology and high heterogeneity. As a result, quantifying their morphological alterations is of great importance for understanding astrocytes’ role within the CNS in both the healthy and disease states. To provide a flexible computational platform for the analysis of images of astrocytes, we developed a software pipeline that automatically detects astrocytes and then extracts and analyzes single-cell morphological profiles. We applied this pipeline for the analysis of fluorescent images of astrocytes in the Nucleus Accumbens (NAc) – a brain region associated with the reward system and, hence, of critical importance for the study of behavioral changes related to substance of abuse disorders. We applied two levels of analysis including (i) single-cell classification based on supervised learning and (ii) computation of morphological distances, defined in terms of Earth Mover’s Distance, between astrocyte subpopulations. Results of our study show that (i) single-cell morphological characteristics are predictive of astrocyte location within the NAc; (ii) single-cell morphological characteristics are predictive of behavioral responses to heroin induced withdrawal and relapse; (iii) single-cell morphological alterations are highly specific, in the sense that alterations observed in the heroin model are distinctly different from those observed in the sucrose control model. Our pipeline contributes to understanding astrocytes’ function within the CNS and their critical role in addiction-related behaviors, highlighting the importance of statistical and geometrical modeling in identifying changes in astrocyte structure that may indicate alterations in their functional capacities."
- poster: 140321
  presenter: "Masserano, Luca"
  email: "41b5002670f54009"
  title: "MS87: Trustworthy Scientific Inference from Generative Models"
  abstract: |-
    Scientific inference often involves using data to infer internal parameters
    that determine a complex physical phenomenon. This data usually comes in the form
    of a labeled set collected either i) from a mechanistic model that implicitly
    encodes the likelihood function, or more generally from a statistical model that
    cannot be evaluated; or ii) from observational studies where labels can be measured
    with high precision. Both of these are Likelihood-Free Inference (LFI) settings,
    where scientists are increasingly leveraging machine learning methods, such as
    neural density estimators and generative models, to infer parameters of interest
    given a new observation from the same intractable likelihood model. However, high-posterior
    density (HPD) regions derived from neural density estimators do not necessarily
    have a high probability of including the true parameter of interest, even if the
    posterior is well-estimated and the labeled dataset has the same distribution
    as the target. In this talk, I will present new LFI methodology and algorithms
    to leverage neural density estimators and produce confidence regions of parameters
    of interest that have (i) nominal frequentist coverage for any value of the true
    unknown parameters, and (ii) small average area (yielding high constraining power)
    if the prior is well-specified. I will illustrate our methods on examples from
    astronomy and high-energy physics, and discuss where we stand and what challenges
    still remain.
- poster: 140018
  presenter: "Mathangadeera, Praveeni"
  email: "c5aa4e900eaef8ff"
  title: |-
    Sensitivity and Ml Analysis of a Response of Permafrost Soil to Surface Temperature
    Variations in Variable Topography.
  abstract: |-
    We consider a computational model for energy equation in permafrost soils
    and its response to surface temperature model  [Ling, Zhang' 2003] involving
    n=9 environmental parameters including the albedo of surface (lake, snow, vegetation,
    wildfire-affected vegetation).  We study the sensitivity of the solutions using
    Sobol indices sensitivity framework,  and set up a ML regression model: these
    allow to assess the robustness of our computational model as well as to understand
    the uncertainty associated with the parameters and the model itself. Our simulations
    and analyses help to determine the response of the soils in the Arctic to the
    changing climate and to assess the reliability of the model.
- poster: 140402
  presenter: "Mcfadden, Francesca"
  email: "e3b723390366645b"
  title: "Machine Learning Competency Measures"
  abstract: |-
    Current approaches to machine model application involve use of model confidence
    in decision making. The goal of model competence estimation is to provide the
    end user with more than model confidence, and to calibrate trust in a specific
    decision with a metric for how likely it is that the input is truly outside the
    prediction space of the algorithm. Confidence estimation along with out of distribution
    indicators are both factored in to provide a measure of model prediction competence
    as an indicator of the predictive capability of a model for a current input.
- poster: 140008
  presenter: "Medlin, Karen"
  email: "542b4a322e951b29"
  title: "MS68: Classifying Imbalanced Data"
  abstract: "“Imbalanced” describes any dataset in which there is one class of data represented by significantly less samples than another – called the “minority” class. A common type of data found in many fields including toxicology/drug discovery, medical diagnostics, fraud detection, and nuclear physics, to name a few, imbalanced data poses problems of bias to machine learning models. Traditional machine learning models were built with the assumption of balanced data; therefore, they learn in favor of the majority class well represented in the dataset. Common approaches rebalance the data or rebalance the model. Of the former category, we present a novel algorithm, “MUBO: Majority Undersampling with Bilevel Optimization,” that leverages a bilevel optimization formulation to optimize training data and overcome model bias and, ultimately, improve the classification of imbalanced data. Experiments with benchmark imbalanced datasets show that MUBO results in F1 score increases of up to 11% over the classic SMOTE method and up to 8% over the state-of-the-art SMOTified-GAN method."
- poster: 139841
  presenter: "Medri, Ivan"
  email: "d7e5804cee6d6df0"
  title: |-
    MS41: Direct Methods for Classification of Time Signals with Separation
    Guarantees Using Linear Optimal Transport Embeddings
  abstract: ""
- poster: 139959
  presenter: "Melnichenko, Max"
  email: "9e81f48f6435cc5c"
  title: |-
    MS54: Novel Randomized Algorithms for Low and Full-Rank Factorizations and
    Their Implementations in Randlapack
  abstract: "We present a pair of novel randomized algorithms: a scheme for performing QR with column pivoting and a method for performing a partial SVD.   The first algorithm, called ICQRRP, carefully uses randomized sketching to accelerate both pivot decisions for the input matrix and the process of decomposing the pivoted matrix via Cholesky QR. ICQRRP is applicable to matrices of any aspect ratio.  The second algorithm, called ABRIK, improves partial SVD runtime by performing a few multiplications with large blocks of random vectors.  Furthermore, ABRIK overcomes the limitation of the power iteration-based Randomized SVD, related to matrices with a slowly decaying spectrum.   We implement the algorithms in RandLAPACK by calling into RandBLAS and vendor-provided BLAS/LAPACK libraries. Experiments with these implementations were performed on an Intel Xeon Gold 6245 CPU. ICQRRP demonstrates two orders-of-magnitude speedup relative to LAPACK’s standard function for QRCP and superior performance to the state-of-the-art alternative randomized QRCP scheme; ABRIK exhibits an order-of-magnitude speedup over the alternative partial SVD solvers."
- poster: 139991
  presenter: "Mendoza, Renier"
  email: "b491bf4de42c2782"
  title: |-
    A Hybrid of Half-Quadratic Minimization Algorithm and Method of K-means for
    Edge Detection of Texts
  abstract: |-
    We propose an improved edge-detection method for text images. First,
    we formulate an optimization problem for image denoising. The minimization problem,
    which utilizes a nonlinear regularization term, is solved using a half-quadratic
    minimization technique. In this approach, we transform the nonlinear objective
    function into two simpler subproblems: one being a quadratic problem, while the
    other has an explicit solution. The optimality condition of the quadratic problem
    results in an elliptic partial differential equation, which is solved using a
    Finite Element Method. After denoising the image, we employ the method of $K$-means
    to separate the text from the image. The final step involves utilizing any edge
    detection algorithm to obtain the edges of the text images. We implement our proposed
    approach on images containing text with varying levels and types of noise and
    compare our results with other edge detection algorithms.
- poster: 139970
  presenter: "Meng, Tingwei"
  email: "a3e5d585263c0b15"
  title: |-
    MS43: Solving Optimal Execution Problems in Financial Mathematics Via in-Context
    Operator Networks
  abstract: |-
    Computing optimal order execution strategies is a fundamental problem
    in financial mathematics. The optimal execution strategy balances the trade-off
    between the cost of executing the order and market risk. A prominent approach
    proposed by practitioners is the propagator model, which employs an optimal control
    framework to derive these strategies. Yet, enhancing the speed of computing these
    strategies remains a vital area of research. Concurrently, in-context learning
    has proven effective in addressing large-scale practical problems. Within the
    domain of scientific computing, In-Context Operator Networks (ICONs) have emerged.
    These networks facilitate the learning of operators by merging offline pre-training
    with online inference, where the operator is approximated using a limited dataset
    provided as context. This approach eliminates the need for retraining when new
    contexts arise, offering benefits like rapid inference and reduced data demands.
    In our study, we apply ICONs to tackle optimal execution problems. Utilizing price
    dynamics as the context, ICONs establish a correlation between market prices and
    execution strategies. Subsequently, the optimal strategy is determined by solving
    the optimal control problem using the operator learned through this process.
- poster: 139969
  presenter: "Meng, Tingwei"
  email: "a3e5d585263c0b15"
  title: |-
    MS22: HJ-Sampler: a Bayesian Sampler for Inverse Problems of a Stochastic
    Process by Leveraging Hamilton-Jacobi PDEs and Score-Based Generative Models
  abstract: |-
    The interplay between stochastic processes and optimal control has been
    extensively explored in the literature. With the recent surge in diffusion models,
    stochastic processes have increasingly been applied to sample generation. This
    paper builds on the log transform, known as Cole-Hopf transform in Brownian motion
    contexts, and extends it to a more abstract framework that involves a linear operator.
    Within this framework, we found that the well-known relationship between the Cole-Hopf
    transform and optimal transport is a particular case where the linear operator
    is the generator of a stochastic process. We also introduce a novel scenario where
    the linear operator is the adjoint of the generator, linking to Bayesian inference
    under specific initial and terminal conditions. Leveraging this theoretical foundation,
    we develop an algorithm, named HJ-sampler, for Bayesian inference for inverse
    problems of a stochastic differential equation with given terminal observations.
    The HJ-sampler involves two stages: solving viscous Hamilton-Jacobi (HJ) partial
    differential equations (PDEs) and sampling from the associated stochastic optimal
    control problem. Our proposed algorithm naturally allows for flexibility in selecting
    the numerical solver for viscous HJ PDEs. We introduce two variants of the solver:
    the Riccati-HJ-sampler, based on the Riccati method, and the SGM-HJ-sampler, which
    utilizes diffusion models. Numerical examples demonstrate the effectiveness of
    our proposed methods.
- poster: 140037
  presenter: "Meyer, Raphael"
  email: "4f4dd471d8e50a48"
  title: "MS54: The Girard-Hutchinson Estimator Is Bad at Kronecker Trace Estimation"
  abstract: |-
    We study the problem of estimating the trace of a matrix $A$ that can
    only be access via Kronecker-matrix-vector products. That is, we can compute $Ax$
    for any vector $x$ that has Kronecker structure. In particular, we study how Hutchinson's
    Estimator performs in this setting, proving tight rates for the number of matrix-vector
    products this estimator needs to find a relative error approximation to the trace
    of $A$. We find an exact expression for the variance of this estimator, show this
    is unavoidably exponential, and conclude with evidence that a much faster non-Hutchinson
    algorithm may exist.
- poster: 140522
  presenter: "Miller, Kevin"
  email: "3f495ca94085d4f8"
  title: "MS92: Dirichlet Active Learning: Exploration and Exploitation Guarantees"
  abstract: "This work introduces Dirichlet Active Learning (DiAL), a Bayesian-inspired approach to the design of active learning algorithms. Our framework models feature-conditional class probabilities as a Dirichlet random field and lends observational strength between similar features in order to calibrate the random field. This random field can then be utilized in learning tasks: in particular, we can use current estimates of mean and variance to conduct classification and active learning in the context where labeled data is scarce. We demonstrate the applicability of this model to low-label rate graph learning by constructing “propagation operators' based upon the graph Laplacian, and offer computational studies demonstrating the method's competitiveness with the state of the art. Finally, we provide rigorous guarantees regarding the ability of this approach to ensure both exploration and exploitation, expressed respectively in terms of cluster exploration and increased attention to decision boundaries."
- poster: 140299
  presenter: "Mitra, Siddharth"
  email: "fc25e1838186e882"
  title: "MS76: On Independent Samples Along the Langevin Diffusion and Algorithm"
  abstract: |-
    Markov chains are widely used to sample from probability distributions,
    and their mixing time tells us how long we need to wait before the law of the
    iterates is close to the stationary distribution. This poster discusses the "independence
    time" of a Markov chain, which tells us how long we need to wait before the iterates
    have small mutual information with the initial random variable. It shows the independence
    time for the Langevin diffusion along with discrete time implementations of the
    diffusion, and shows that for strongly-log concave targets, the mutual information
    goes to 0 exponentially fast. These convergence rates are tight and the results
    are proven using strong data processing inequalities and regularity properties
    of these Markov chains. It's based on joint work with Jiaming Liang and Andre
    Wibisono.
- poster: 139877
  presenter: "Mojamder, Md Amran Hossan"
  email: "94d069647836cbd6"
  title: "Machine Learning Parametrization of Sub-Grid Fluxes in 1D Shallow Water Equations"
  abstract: |-
    In this work, we consider the 1D shallow-water equations (SWE) and use
    machine learning to estimate the effect of unresolved sub-grid degrees of freedom.
    To this end, we first consider a fine-mesh finite-volume discretization of the
    SWE and define coarse variables as averages in space. Next, we use Machine Learning
    to estimate effective fluxes on a coarse mesh. Moreover, we also use flux limiters
    to ensure consistency of the effective coarse-mesh model. We consider statistical
    steady-state regimes and use numerical simulations to demonstrate agreement between
    fine-mesh and coarse-mesh discretization in several parameter regimes, including
    regimes not included in the training dataset.
- poster: 139764
  presenter: "Mora, Carlos"
  email: "ab05619b9d78870d"
  title: "A Gaussian Process Approach for Operator Approximation Via Physics and Data"
  abstract: |-
    Numerous machine learning (ML) methods have recently emerged as a promising
    alternative to traditional numerical methods for solving partial differential
    equations (PDEs). In a more ambitious attempt, such methods extend beyond solving
    individual instances of PDEs and aim to approximate operators that establish mappings
    between infinite-dimensional spaces. This non-trivial extension enables on-the-fly
    approximation of PDE solution given unseen initial and boundary conditions, domain
    geometries, and input parameters. Most ML-based approaches for learning operators
    in PDEs rely on neural networks and kernel methods such as Gaussian Processes
    (GPs) have scarcely been explored in this direction. To fill this gap, in this
    presentation we introduce a novel GP-based approach for operator learning. We
    leverage the Kronecker product structure of the covariance matrix to enhance the
    computational efficiency and scalability of the approach without resorting to
    low-rank approximations. In addition, our method seamlessly integrates physics
    and data and strictly satisfies the boundary and initial conditions for unseen
    samples. Through a host of comprehensive benchmarks, we demonstrate the superiority
    of our framework over state-of-the-art methods, especially in data-scarce scenarios,
    which makes it particularly attractive for real-world applications.
- poster: 140297
  presenter: "Morris, Rachel"
  email: "ba44aa8c3accfd57"
  title: "MS92: Performance Guarantees for Probabilistic Adversarial Learning"
  abstract: "Recent work has shown that neural networks trained to maximize classification accuracy can be tricked by well-targeted adversarial attacks – even though their effects may be imperceptible to the human eye. This has sparked many new approaches to classification which include an adversary in the training process: such an adversary can improve robustness and generalization properties, at the cost of decreased accuracy and increased training time. This poster will discuss a range of adversarial training models, with a special focus on probabilistic methods which sample from nearby data points. In particular, the theoretical results provide a rigorous comparison with the standard Bayes’ classification problem and important steps towards understanding the regularity of minimizers of the adversarial training problem using techniques from geometric measure theory."
- poster: 140197
  presenter: "Myers, Jeremy"
  email: "ba47190b63f4d7b4"
  title: "An Online, Sampling-Based Convergence Criterion for Symmetric Matrices"
  abstract: |-
    When a matrix is streamed or if both its dimensions are enormous, it may
    not be possible to store  the entire matrix in memory.  When dimensionality reduction
    is required, Incremental SVD (iSVD)  circumvents the need for costly data storage.  However,
    computing residual information that is necessary for early termination in iterative
    solvers  requires access to all of the matrix, which is not possible.  Additionally,
    we often observe empirically that the solver tolerance requested for each window
    does not guarantee that the   residuals of the final solution will satisfy the
    same tolerance.  In this work, we demonstrate how sampling the matrix  at runtime
    can be used to accurately estimate the residual error that  would be achievable
    if the matrix were fully available.  With an estimate of the residual error, we
    develop a sampling-based convergence  criterion for an iterative eigenmethod.
    We provide numerical evidence on challenging datasets, both dense and sparse,   that
    our convergence criterion reduces cumulative costs as the matrix is streamed.  We
    utilize this criterion only  for symmetric matrices since the right singular vectors
    approximated  by iSVD coincide with the eigenvectors of a symmetric matrix.
- poster: 140178
  presenter: "Nag, Protik"
  email: "21662b1d420b9a03"
  title: |-
    MS25: Expressive and Flexible Simulation of Information Spread Strategies
    in Social Networks Using Planning
  abstract: |-
    In the digital age, understanding the dynamics of information spread and
    opinion formation within networks is paramount. This research introduces an innovative
    framework that combines the principles of opinion dynamics with the strategic
    capabilities of Automated Planning and reinforcement learning. We have developed
    a menu of numeric PDDL as well as neural network-based reinforcement learning
    planners tailored for opinion dynamics networks. Our tool empowers users to visualize
    intricate networks, simulate the evolution of opinions, and strategically influence
    that evolution to achieve specific outcomes. By harnessing planning techniques,
    our framework offers a nuanced approach to devise sequences of actions tailored
    to transition a network from its current opinion landscape to a desired state.
    This holistic approach provides insights into the intricate interplay of individual
    nodes within a network and paves the way for targeted interventions. Furthermore,
    the tool facilitates human-AI collaboration, enabling users to not only understand
    information spread but also devise practical strategies to mitigate potential
    harmful outcomes arising from it.
- poster: 139816
  presenter: "Navarro, Madeline"
  email: "8d0e224efacc4f47"
  title: |-
    MS77: Graphmad: Graph Mixup for Data Augmentation Using Data-Driven Convex
    Clustering
  abstract: |-
    We develop a novel data-driven nonlinear mixup mechanism for graph data
    augmentation and present different mixup functions for sample pairs and their
    labels. Mixup is a data augmentation method to create new training data by linearly
    interpolating between pairs of data samples and their labels. Mixup of graph data
    is challenging since the interpolation between graphs of potentially different
    sizes is an ill-posed operation. Hence, a promising approach for graph mixup is
    to first project the graphs onto a common latent feature space and then explore
    linear and nonlinear mixup strategies in this latent space. In this context, we
    propose to (i) project graphs onto the latent space of continuous random graph
    models known as graphons, (ii) leverage convex clustering in this latent space
    to generate nonlinear data-driven mixup functions, and (iii) investigate the use
    of different mixup functions for labels and data samples. We evaluate our graph
    data augmentation performance on benchmark datasets and demonstrate that nonlinear
    data-driven mixup functions can significantly improve graph classification.
- poster: 139722
  presenter: "Negrini, Elisa"
  email: "55506f421a19f949"
  title: "MS26: Applications of No-Collision Transportation Maps in Manifold Learning"
  abstract: |-
    In this work, we investigate applications of no-collision transportation
    maps introduced by Nurbekyan et al. in 2020 in manifold learning for image data.
    Recently, there has been a surge in applying transportation-based distances and
    features for data representing motion-like or deformation-like phenomena. Indeed,
    comparing intensities at fixed locations often does not reveal the data structure.
    No-collision maps and distances developed in [L. Nurbekyan, A. Iannantuono, and
    A. M. Oberman, J. Sci. Comput., 82 (2020), 45] are sensitive to geometric features
    similar to optimal transportation (OT) maps but much cheaper to compute due to
    the absence of optimization. In this work, we prove that no-collision distances
    provide an isometry between translations (respectively, dilations) of a single
    probability measure and the translation (respectively, dilation) vectors equipped
    with a Euclidean distance. Furthermore, we prove that no-collision transportation
    maps, as well as OT and linearized OT maps, do not in general provide an isometry
    for rotations. The numerical experiments confirm our theoretical findings and
    show that no-collision distances achieve similar or better performance on several
    manifold learning tasks compared to other OT and Euclidean-based methods at a
    fraction of the computational cost.
- poster: 139838
  presenter: "Newton, Rachel"
  email: "a15953b11b895813"
  title: "MS49: Manifold Optimization for Data Driven Reduced-Order Modeling"
  abstract: |-
    The focus of this poster is on data-driven reduced-order modeling.  We
    assume a high fidelity, discrete-time model is available for simulation.  The
    simulator allows state and output trajectories to be collected for any specified
    initial condition and input signal.  An optimal reduced-order model (ROM) requires:
    (i) the selection of a lower dimensional subspace for the state of the ROM, and
    (ii) an optimal reduced-order state-space model (evolving on the lower dimensional
    subspace). A common heuristic is to: (i) select the lower-order subspace using
    proper orthogonal decomposition (POD), and (ii) use least-squares to fit the reduced-order
    state-space model on the POD subspace. We demonstrate the potential deficiencies
    of this heuristic via two simple examples. In order to address these deficiencies,
    we propose a novel method to optimize the choice of subspace using the Grassmann
    manifold.  Finally, we show that our proposed manifold optimization empirically
    outperforms the POD heuristic on the two motivating examples and a planar wind
    farm model.  This submission is a part of the minisymposium on "Efficient and
    robust optimization techniques for structured data learning" (80608).
- poster: 139664
  presenter: "Nguyen, Duc Toan"
  email: "b6fc4d8a838c14bb"
  title: "Towards Tuning-Free Minimum-Volume Nonnegative Matrix Factorization"
  abstract: |-
    Nonnegative Matrix Factorization (NMF) has emerged as a versatile and
    powerful tool in diverse domains, including machine learning, data mining, and
    signal processing. Among various NMF approaches, minimum-volume NMF stands out
    as a prominent method to extract indentifiable solutions. In 2019, Leplat et al.
    proposed a version of minimum-volume NMF that can handle the rank-deficiency case.
    However, this work did not address an important issue of selecting an appropriate
    $\lambda$ parameter to adapt to varying noise levels, resulting in suboptimal
    performance. Furthermore, the attained errors failed to achieve machine precision,
    which limits the method's practical applicability. To overcome these limitations,
    our research aims to investigate the relationship between $\lambda$ and the noise
    level through a series of well-designed experiments. Additionally, we present
    a novel Majorization-Minimization (MM) variant for minimum-volume NMF. By employing
    the MM philosophy, we optimize a relaxed surrogate function, enhancing the optimization
    process and yielding improved results compared to the traditional hard function.
    Our experimental findings demonstrate the efficacy of the proposed method, showcasing
    errors that consistently fall below machine precision.
- poster: 139890
  presenter: "Nguyen, Hai"
  email: "9e8cd255273c62ed"
  title: |-
    MS90: Model-Constrained AutoEncoder Approach for Solving Forward and Inverse
    Problems
  abstract: |-
    Obtaining fast and accurate solvers for partial differential equations
    (PDEs) and inverse problems is crucial across a broad spectrum of engineering
    disciplines. We introduce a novel methodology that employs a model-constrained
    autoencoder to learn both the forward surrogate and inverse mappings for PDE-constrained
    inverse problems simultaneously. This approach is particularly effective in low-data
    regimes, where purely data-driven machine-learning methods tend to overfit due
    to the absence of underlying physical laws. We validate the generalization, accuracy,
    and effectiveness of our model through its application to the 2D Poisson equation,
    2D Burgers' equation, and 2D Navier-Stokes equation. Our results demonstrate substantial
    enhancements in computational efficiency and solution fidelity, marking a significant
    advancement in the field.
- poster: 140101
  presenter: "Nguyen, Hoang"
  email: "3b11f55bed1a662f"
  title: |-
    MS61: Wasserstein Convergence Guarantees for a General Class of Score-Based
    Generative Models
  abstract: |-
    Score-based generative models (SGMs) is a recent class of deep generative
    models with state-of-the-art performance in many applications. In this presentation,
    we establish convergence guarantees for a general class of SGMs in 2-Wasserstein
    distance, assuming accurate score estimates and smooth log-concave data distribution.
    We specialize our result to several concrete SGMs with specific choices of forward
    processes modeled by stochastic differential equations, and obtain an upper bound
    on the iteration complexity for each model, which demonstrates the impacts of
    different choices of the forward processes. We also provide a lower bound when
    the data distribution is Gaussian. Numerically, we experiment SGMs with different
    forward processes, some of which are our newly proposed methods, for unconditional
    image generation on CIFAR-10. We find that the experimental results are in good
    agreement with our theoretical predictions on the iteration complexity, and the
    models with our newly proposed forward processes can outperform existing models.
- poster: 140245
  presenter: "Nnyaba, Ukamaka"
  email: "95fd9e2a987f7c29"
  title: "An Optimal Multivariate Functional Classifier"
  abstract: |-
    The task of classification using multiple functional covariates has become
    increasingly relevant over the past years with the growing sizes of data and technological
    advancements allowing to collect data over different domains, from one-dimensional
    signals to two-dimensional images. Despite the growing importance of this task,
    there are not many approaches available to make use of functional variables from
    different domains to perform this classification task. We therefore contribute
    to this line of research by (i) extending the functional neural network (FNN)
    approach (initially developed for regression problems) to the classification task;
    (ii) proving the minimax optimality of the proposed approach; (iii) investigating
    the behavior of the approach in presence of functional variables from different
    domains (i.e. a combination of one- and two-dimensional functional variables);
    and (iv) highlighting the good comparative performance of this approach through
    simulation and real-world data studies. The proposed method, that we call Multivariate
    Functional Classifier (MFC), complements the few existing methods for the classification
    of multivariate functional data and does not require the functional Gaussian assumption.
- poster: 140098
  presenter: "Nockolds, William"
  email: "dca800df226975b1"
  title: |-
    MS52: Machine Learning for Explicitly Solving Stiff, Parameterized Ordinary
    Differential Equations
  abstract: |-
    Stiff ordinary differential equations present distinct challenges in
    various engineering disciplines. Specifically, these types of problems cannot
    be solved with traditional explicit time integration methods. Rather, costly implicit
    methods must be used to find solutions. Additionally, the timescale separation
    of various elements of the solution in stiff problems are handled poorly with
    modern machine learning techniques. Methods, such as Neural ODE (NODE), have proven
    to be insufficient for learning stiff systems. This work builds upon the NODE
    framework, to create surrogate models for stiff problems which can be solved using
    explicit time integration methods. Additionally, the approach can be extended
    to include additional input parameters, in the same fashion as the Parameterized
    Neural Ordinary Differential Equation approach proposed in [Lee, Parish, Parameterized
    Neural Ordinary Differential Equations: Applications to Computational Physics
    Problems, 2021]. This poster will demonstrate a new surrogate modelling approach
    that allows for the use of explicit methods to model stiff ordinary differential
    equations which have input-parameter-varying dynamics.
- poster: 140046
  presenter: "Nurbekyan, Levon"
  email: "a943c357e1e557bc"
  title: "MS93: Regularity and Gradients of Optimal Transportation Based Loss Functions"
  abstract: |-
    Optimal transportation distances become ever more popular in learning
    problems involving probability measures. In a typical setting, one has a loss
    function that measures an optimal transportation distance between a parametric
    model and data. Thus, gradient-based optimization methods require differentiability
    analysis and computations of gradients of such loss functions. Here, we provide
    such analysis and discuss gradient formulas. In particular, we find a new sufficient
    condition for the differentiability via the geometry of optimal transport plans
    and prove qualitative approximation results for the gradient formula.
- poster: 139914
  presenter: "Nurbekyan, Levon"
  email: "a943c357e1e557bc"
  title: "MS50: No-Collision Transportation Maps"
  abstract: |-
    We present a new class of transportation maps inspired by the no-collision
    properties of optimal transportation maps. In particular, we discuss the basic
    principles pertaining to such maps and some theoretical properties such as existence
    and regularity. Furthermore, we examine their relation with optimal transportation
    maps and discuss some manifold learning applications where no-collision maps perform
    at the level of optimal transportation maps or better at a cheaper computational
    cost. Finally, we discuss the drawbacks and challenges pertaining to no-collision
    maps and possible roads ahead.
- poster: 140025
  presenter: "O’Neill, Riley"
  email: "f8529aac0bd49600"
  title: "MS60: Learnable Weighting For Dijkstra’s Algorithm Via Graph Neural Networks for Gps Trajectory Registration"
  abstract: ""
- poster: 139935
  presenter: "Oberai, Assad"
  email: "8dc66e44890f2de7"
  title: "MS17: Conditional Generative Models in Data Assimilation"
  abstract: |-
    In this poster we consider the probabilistic data assimilation problem
    and describe how conditional generative models can be used to approximate the
    Bayes filter. The advantages of this approach include the ability to work with
    nonlinear dynamics, nonlinear measurement operators, and non-Gaussian approximations
    for model error and measurement noise.
- poster: 139829
  presenter: "Ogueda, Alonso"
  email: "de58d8347da859eb"
  title: |-
    MS45: Applications of Disease Informed Neural Networks That Includes Human
    Behavior
  abstract: |-
    In this work, we present modeling and simulation of disease dynamics through
    Physics-Informed Neural Networks (PINNS) and its application to real data modeled
    using non-linear system of differential equations. Specifically, we apply PINNS
    to predict the behavior of diseases described by modified compartmental models
    that include parameters and variables associated with the governing system describing
    the dynamics of the disease. Through benchmark problems, we will show that our
    model validates real-data and demonstrate how PINNs can predict optimal parameters
    for a given dataset.
- poster: 139117
  presenter: "Okunola, Toluwani"
  email: "ebc833a65b9625a6"
  title: |-
    MS69: Dynamic Inverse Problems: Efficient Methods for Dynamic Image Reconstruction
    with Motion Estimation
  abstract: |-
    Large-scale dynamic inverse problems are typically ill-posed and suffer
    from complexity of the model constraints and large dimensionality of the parameters.
    A common approach to overcome ill-posedness is through regularization that aims
    to add constraints on the desired parameters, typically on space by sparsity of
    the coefficients or the gradient and with constraints on the temporal dimension.
    In this work, we propose a new method that incorporates a model for the temporal
    dimension by estimating the motion of the objects alongside solving the regularized  problems.
    In particular, we consider the optical flow model as part of the regularization
    that simultaneously estimates the motion and provides an approximation for the
    desired image. To overcome the large dimensionality of the parameters, we combine
    our approach with a generalized Krylov subspace method for computational efficiency.  To
    illustrate the effectiveness of the prescribed approach, we present numerical
    experiments from computerized tomography and image deblurring applications.
- poster: 140150
  presenter: "Ongie, Gregory"
  email: "bcf1b3025c5f9dbd"
  title: |-
    MS8: Performance Guarantees for Solving Linear Inverse Problems with Implicit
    Neural Representations
  abstract: |-
    Implicit neural representations (INRs) have emerged as a powerful tool
    inverse for solving inverse problems in computer vision and computational imaging.
    Rather than representing images as a discrete collection of pixels, INRs represent
    images as a continuous domain function via a neural network taking spatial coordinates
    as inputs. However, unlike discrete image representations, little is known about
    the sample complexity of estimating images using INRs in the context of linear
    inverse problems. Towards this end, we derive necessary and sufficient conditions
    under which an image is exactly recoverable from its low-pass Fourier coefficients
    when fitting a two-layer (i.e., single hidden-layer) INR with a Fourier features
    layer. In particular, we relate the sample complexity to the minimum effective
    width needed to realize the ground truth image as an INR.
- poster: 140200
  presenter: "Orangi-Fard, Negar"
  email: "8fbaf2e090336558"
  title: "Real Time Dyspnea Assessment"
  abstract: |-
    Breathlessness, medically known as dyspnea, is a multifaceted symptom
    and a significant predictor of survival rates among patients dealing with traumatic
    injury or respiratory ailments. Dyspnea typically signals an underlying serious
    condition and necessitates prompt intervention to mitigate the risk of respiratory
    failure or fatality. Presently, the assessment of dyspnea relies on patients self-reporting
    their perceived level of breathing difficulty through the Borg Dyspnea Score.
    This research has shed light on the correlation between prefrontal cortex activity
    and subjective experiences of dyspnea such as breathlessness or labored breathing.
    Leveraging this understanding, we have devised a method to quantify dyspnea by
    integrating prefrontal cortex processing, offering an objective measure of dyspnea.
    This approach aims to assist clinicians in the diagnosis and management of respiratory
    conditions and injuries more effectively.  In our study, we collected objective
    dyspnea scores (ODS) along with subject-reported Borg scores from 25 patients
    with chronic obstructive pulmonary disease (COPD) and 3 healthy volunteers. The
    results of this study demonstrates that the ODS system can distinguish categories
    of clinically relevant dyspnea and breathing effort with a high degree of accuracy.
- poster: 139637
  presenter: "Orlova, Elena"
  email: "61fe8929dd409130"
  title: "MS14: Deep Stochastic Mechanics"
  abstract: "This work introduces a novel deep-learning-based approach for numerical simulation of a time-evolving Schrödinger equation inspired by stochastic mechanics and generative diffusion models. Unlike existing approaches, which exhibit computational complexity that scales exponentially in the problem dimension, our method allows us to adapt to the latent low-dimensional structure of the wave function by sampling from the Markovian diffusion. Depending on the latent dimension, our method may have far lower computational complexity in higher dimensions. Numerical simulations verify our theoretical findings and show a significant advantage of our method compared to other deep-learning-based approaches used for quantum mechanics. This poster is associated with the minisymposium entitled Probabilistic Methods in Machine Learning and Complex Systems."
- poster: 140315
  presenter: "Orozco, Rafael"
  email: "81aa59a72e6601f4"
  title: |-
    MS53: Unsupervised Simulation Based Inference for Amortized Bayesian Inverse
    Problems Without Prior Samples
  abstract: |-
    Advances in modern generative models have made Simulation-Based Inference
    (SBI) a vital tool across numerous scientific disciplines. However, a significant
    limitation of SBI is its dependency on prior samples, which may not always be
    available. For instance, in subsurface imaging, it is impractical to obtain prior
    samples directly from the Earth. To address this challenge, we have developed
    extensions to SBI that facilitate its application even when prior samples are
    unavailable. Our primary contribution is a novel reformulation of the non-amortized
    backward Kullback-Leibler divergence typically used in variational inference for
    learning posteriors. Specifically, we incorporate an expectation over observations,
    which enables two key capabilities: (i) effective amortization of the posterior
    approximation across a family of observations, and (ii) the ability to learn useful
    features from a training dataset, thereby enabling posterior learning without
    direct access to prior samples. We validate our approach through a stylized test,
    demonstrating that our method can accurately sample from an inverse problem with
    a known posterior. Furthermore, we showcase a practical implementation on a large-scale
    subsurface imaging problem that requires solving costly partial differential equations
    (PDEs).
- poster: 139153
  presenter: "Oshinubi, Kayode"
  email: "22000e2b965a05d2"
  title: |-
    Enhancing Covid-19 Endemic/Epidemic Forecasting Precision Through the Integration
    of Deterministic Model, Machine Learning, Daily Data, and Cycle Threshold Values
  abstract: |-
    In this study, we propose an overview of the COVID-19 situation using
    both deterministic and machine learning models on the transmission dynamics of
    the COVID-19 pandemic. This is achieved by integrating a hybrid model with daily
    empirical data and cycle threshold (Ct) values from different countries. We partitioned
    this dataset into before and after vaccination to understand the influence of
    vaccination on disease dynamics. We used the deterministic model to present some
    mathematical analyses. Also, the model was fitted to real data to validate the
    deterministic model's precision. Furthermore, we used a machine learning model
    to validate our model by performing principal component analysis (PCA) to predict
    the evolution of the spread of the COVID-19 outbreak on some statistical predictor
    indicators from time series modeling on a 14-day moving window for detecting which
    of these indicators capture the dynamics of the disease spread across the epidemic
    curve. The results of the PCA, the calculated index of dispersion, the fitted
    deterministic model, and the deterministic model simulation are all in agreement
    with the dynamics of the disease before and after vaccination started. Conclusively,
    our approach has been able to capture the dynamics of the pandemic at different
    phases of the disease outbreak, and the result presented will be useful to understand
    the evolution of the disease and future and emerging epidemics.
- poster: 139656
  presenter: "Ou, Weigutian"
  email: "44522e6f9bf4b5a3"
  title: "Covering Numbers of Relu Networks"
  abstract: |-
    Covering numbers of ReLU network families are important as they allow  to
    quantify the fundamental limits in function approximation and  generalization
    in the context of non-parametric regression. Previous studies have reported upper
    bounds on covering numbers for specific network architectures and covering ball
    radii, which in turn allow to lower-bound the function approximation error and
    upper-bound the generalization error in regression problems. The results presented
    in this poster complete this picture by deriving comprehensive upper bounds in
    general settings, i.e., for various types of network architectures and all covering
    ball radii.  More importantly,  we also establish lower bounds that are tight
    w.r.t. the upper bounds we obtain.  The resulting complete characterization of
    covering numbers allows us to characterize the behavior of network compression,
    network quantization, network approximation, and non-parametric regression with
    ReLU networks. In particular, we show in a constructive manner that fully connected
    ReLU networks with base-2 quantized weights and independent choices of width and
    depth, can approximate 1-Lipschitz functions in a memory-optimal manner. We also
    establish that the generalization error in non-parametric regression using ReLU
    networks achieves the optimal rate of convergence.
- poster: 140549
  presenter: "Oulhaj, Ziyad"
  email: "84a263af28194baf"
  title: "MS6: Differentiable Mapper for Topological Data Representation"
  abstract: |-
    Unsupervised data representation and visualization using tools from topology
    is an active and growing field of Topological Data Analysis (TDA) and data science.  Its
    most prominent line of work is based on the so-called Mapper graph, which is a
    combinatorial graph whose topological structures (connected components, branches,
    loops) are in correspondence with those of the data itself.  While highly generic
    and applicable, its use has been hampered so far by the manual tuning of its many
    parameters. Among these, a crucial one is the so-called filter: it is a continuous
    function whose variations on the data set are the main ingredient for building
    the Mapper representation. However, there is currently no method for tuning the
    filter itself. In this work, we build on a recently proposed optimization framework
    incorporating topology to provide the first filter optimization scheme for Mapper
    graphs. In order to achieve this, we propose a relaxed and more general version
    of the Mapper graph, whose convergence properties are investigated.
- poster: 140105
  presenter: "Pan, Yunian"
  email: "2455efddce4ed6ae"
  title: "On the Variational Interpretation of Mirror Play in Monotone Games"
  abstract: |-
    Mirror play (MP) is a well-accepted primal-dual multi-agent learning algorithm
    where all agents simultaneously implement mirror descent in a distributed fashion.
    The advantage of MP over vanilla gradient play lies in its usage of mirror maps
    that better exploit the geometry of decision domains. Despite extensive literature
    dedicated to the asymptotic convergence of MP to equilibrium, the understanding
    of the finite-time behavior of MP before reaching equilibrium is still rudimentary.
    To facilitate the study of MP's non-equilibrium performance, this work establishes
    an equivalence between MP's finite-time primal-dual path (mirror path) in monotone
    games and the closed-loop Nash equilibrium path of a finite-horizon differential
    game, referred to as mirror differential game (MDG). Our construction of MDG rests
    on the Brezis-Ekeland variational principle, and the stage cost functional for
    MDG is Fenchel coupling between MP's iterates and associated gradient updates.
    The variational interpretation of mirror path in static games as the equilibrium
    path in MDG holds in deterministic and stochastic cases. Such a variational interpretation
    translates the non-equilibrium studies of learning dynamics into a more tractable
    equilibrium analysis of dynamic games, as demonstrated in a case study on the
    Cournot game, where MP dynamics corresponds to a linear quadratic game.
- poster: 139644
  presenter: "Pan, Yang"
  email: "3d0e0468e7eaea45"
  title: |-
    Metric Entropy Limits on Recurrent Neural Network Learning for Lipschitz
    Fading Memory Systems
  abstract: |-
    Neural networks have attracted significant attention due to their ability to approximate almost any function arbitrarily well. The Kolmogorov-Donoho rate-distortion theory, as introduced in "[Elbrächter et al., Deep neural network approximation theory, 2021]", quantifies the information-efficiency in terms of the number of bits needed to specify the approximating neural network relative to a fundamental limit. This limit is based on the concept of metric entropy which essentially measures the complexity of the approximation problem. We extend this theory to the approximation of sequence-to-sequence maps using recurrent neural networks (RNNs). Specifically, we consider Lipschitz fading memory (LFM) systems, which model dynamical systems that gradually forget long-past inputs. We develop tools to quantify the complexity of these systems in terms of metric entropy and propose a general construction to approximate them using RNNs. We show that RNNs can approximate LFM systems in a metric-entropy optimal manner in the case of exponentially decaying memory and polynomially decaying memory.
- poster: 140544
  presenter: "Pandey, Biraj"
  email: "d690177754149e7a"
  title: |-
    MS42: Approximation Rates for Minimum Integral Probability Metric Generative
    Models
  abstract: |-
    We present a thorough error analysis for generative models trained to
    sample from arbitrary data distributions. We derive a priori error bounds based
    on the model complexity and the size of the training dataset, specifically for
    models optimized using statistical divergences. Our theoretical framework is applicable
    to losses of the integral probability metric type, such as Maximum Mean Discrepancy
    and Wasserstein distance, within both neural network and kernel method-based models.
    Additionally, we provide a comprehensive set of numerical examples that validate
    our theoretical error bounds, demonstrating their consistency with experimental
    results.
- poster: 140201
  presenter: "Park, Jeongjin"
  email: "8589d2bfc2773d45"
  title: "MS93: Learning Statistically Accurate Chaotic Models with Neural ODEs"
  abstract: |-
    To address the inherent limitations of Neural Ordinary Differential Equations
    (Neural ODEs) such as instability or limited expressiveness, recent works have
    proposed various methods, including innovative adjoint techniques and approaches
    focusing on latent space representations. Nonetheless, can neural dynamical models
    learn the true dynamics and, in particular, reproduce associated statistical measures?
    Our initial findings indicate that the usual generalization error of Neural ODEs
    is not reflective of how well the governing dynamics is learned. In this work,
    we define notions of generalization that are more appropriate for ergodic systems,
    such as ergodic chaotic systems and stochastic systems, utilizing metrics derived
    from dynamical systems theory. Additionally, we introduce and analyze a regularization
    technique aimed at enabling Neural ODEs to learn invariant measures.  Our analysis
    provides sufficient conditions for when neural network parameterizations can accurately
    learn statistical properties of dynamical systems.  Our empirical results and
    analysis encompass a wide variety of ergodic systems and neural network parameterizations,
    including via Fourier neural layers, ResNets and RNNs. By combining shadowing
    theory from dynamical systems with statistical learning, our work advances the
    foundations of effective neural network modeling of complex dynamics.
- poster: 139790
  presenter: "Park, Hyoung Suk"
  email: "216d96753af9e01"
  title: |-
    A Novel Implicit Neural Representation-Based Metal Artifact Reduction in
    Dental CBCT
  abstract: |-
    Dental cone-beam computed tomography (CBCT) has gained popularity as a
    cost-effective and low-radiation alternative in dental clinics. However, its clinical
    utility is partly limited by artifacts related to metallic objects in the images.
    Recently, implicit neural representations (INRs), particularly through multi-layer
    perceptrons, have shown significant potential for dental CBCT reconstructions.
    However, these studies have not specifically addressed the challenges associated
    with metal artifact reduction (MAR). Based on a mathematical analysis of the physical
    interactions between polychromatic X-ray beams and metal objects, we present an
    INR-based MAR method that generates two distinct and informative tomographic images.
    The first image represents the monochromatic attenuation distribution at a specific
    energy level, while the second captures the nonlinear beam-hardening effect due
    to the polychromatic nature of X-ray beams. We also discuss the advantages of
    our proposed method compared to conventional model-based and deep learning-based
    MAR approaches.
- poster: 140265
  presenter: "Parkinson, Suzanna"
  email: "a03278f90d416eba"
  title: |-
    MS8: Effect of Linear Layers in ReLu Networks: Learning Single-/Multiple-Index
    Models
  abstract: |-
    Why do deeper neural networks tend to outperform shallow ones? We study
    the inductive bias of neural networks in the simplified case where most layers
    have a linear activation. Despite these models having the same capacity at different
    depths, they do not all have the same representation cost. Specifically, minimizing
    the $\ell_2$ penalty when training a neural network with many linear layers followed
    by a single ReLu layer using weight decay is equivalent to a function-space penalty
    that encourages the network to select a function with low mixed variation. That
    is, the function has limited variation in directions orthogonal to a low-dimensional
    subspace. This means that the trained model will approximately be a single- or
    multiple- index model. Our experiments show that when this active subspace structure
    exists in the data, adding linear layers can improve generalization and result
    in a network that is well-aligned with the true active subspace.
- poster: 140555
  presenter: "Patel, Ravi"
  email: "ba261cc6eb8c3d06"
  title: |-
    MS30: A Novel Ensemble Approach to Uncertainty Quantification in Graph Neural
    Operators
  abstract: |-
    Neural operators are a recently developed generalization of regression
    to mappings between functions. They promise to drastically reduce expensive numerical
    integration of PDEs to fast evaluations of mappings between functional states
    of a system, i.e., surrogate and reduced-order modeling. Neural operators have
    already found applications in several areas such as modeling sea ice, combustion,
    and atmospheric physics. However, even in the ideal case where neural operators
    recover the training data, there will be uncertainty in interpolatory and extrapolatory
    regimes. A useful model is one that is transparent about its shortcomings. Single
    point estimates, hide the broad range of models that reasonably fit training data
    but present vastly differing predictions. Operator learned models with uncertainty
    quantification (UQ) would expose users of the models to the full range of surrogate
    predictions. In this talk, we introduce a novel ensemble approach to UQ in a graph
    neural operator and compare it to a more standard variational Bayes approach in
    synthetic benchmarks.
- poster: 140319
  presenter: "Patil, Pratik"
  email: "7efdde5388be69d6"
  title: "MS87: Optimal Ridge Regularization for Out-of-Distribution Prediction"
  abstract: |-
    We study the behavior of optimal ridge regularization and optimal ridge
    risk for out-of-distribution prediction, where the test distribution deviates
    arbitrarily from the train distribution. We establish general conditions that
    determine the sign of the optimal regularization level under covariate and regression
    shifts. These conditions capture the alignment between the covariance and signal
    structures in the train and test data and reveal stark differences compared to
    the in-distribution setting. For example, a negative regularization level can
    be optimal under covariate shift or regression shift, even when the training features
    are isotropic or the design is underparameterized. Furthermore, we prove that
    the optimally tuned risk is monotonic in the data aspect ratio, even in the out-of-distribution
    setting and when optimizing over negative regularization levels. In general, our
    results do not make any modeling assumptions for the train or the test distributions,
    except for moment bounds, and allow for arbitrary shifts and the widest possible
    range of (negative) regularization levels.
- poster: 140560
  presenter: "Patterson, Evan"
  email: "67323b39ec73bfa8"
  title: "MS12: The Synthetic Theory of Probability and Statistics"
  abstract: |-
    Probability theory and statistics are usually conceived analytically,
    grounded in the standard measure-theoretic foundation. In recent years, significant
    progress has been made toward a synthetic account of probability and statistics
    that treats many of the common definitions and theorems in an axiomatic and purely
    algebraic setting. The synthetic theory is based on Markov categories and other
    category-theoretic ideas. Besides widening the scope of probability theory to
    nonstandard settings, the synthetic theory can be used to give a formal semantics
    to probabilistic programs, a structuralist account of statistical models and the
    relationships between them, and a language for specifying statistical models richer
    than the familiar graphical models. In this talk, we give an introduction to the
    synthetic theory of probability and statistics.
- poster: 140559
  presenter: "Paul, Sounak"
  email: "2359672ef6e98dcb"
  title: "MS80: Learning Cryo-Em Structures Via the Method of Moments (Poster)"
  abstract: |-
    We extend our recent work by introducing deep neural network priors to
    learn the moment inversion map for cryo-EM. Our neural networks output the volumes
    along the distribution of rotations and shift variance, with moments being the
    input. We also generalize the neural network-based reconstruction algorithm to
    perform the reconstruction even with shifts in the data, which we demonstrate
    on simulated and biological volumes.
- poster: 139708
  presenter: "Paul, Allen"
  email: "3843271a0f3a8b18"
  title: "Sparse Nystrom Approximation of Currents, Varifolds and Normal Cycles"
  abstract: |-
    Comparing and computing distances between shapes is a fundamental task
    in computer vision, geometric learning, and computational anatomy. When building
    models of shape variation, one requires a fidelity metric on shapes for model
    fitting. Ideally shape data is available in parametric correspondence. In applications,
    it is far from the case, with shape data acquired with inconsistent parametrizations
    and resolutions across a dataset. For submanifolds of $\mathbb{R}^{d}$ one may
    compare shapes in a geometric manner without correspondence, using concepts from
    geometric measure theory (GMT), such as currents, varifolds and normal-cycles.
    Shapes are embedded into the dual of a Hilbert Space of differential forms, and
    compared through the dual metric on this space, independent of parametrization.
    However, practical metric computation is costly. Comparison between triangulations
    with $M,N$ triangles respectively, scales as $\mathcal{O}(MN)$; undesirable for
    practical statistical shape modelling (SSM) when $M,N > 10^{5}$. In this work,
    we derive a randomized algorithm for compression of the GMT representations of
    shapes, using the Nystrom approximation. Our method is significantly faster than
    existing compression methods and has theoretical convergence bounds. Post compression,
    one can compute distances and gradients in closed form between shapes in $\mathcal{O}(mn)$,
    with $m,n < < \mathrm{min}(M,N)$.  The compressions are shown to be useful for
    down-line geometric learning tasks.
- poster: 140061
  presenter: "Pessoa, Pedro"
  email: "7b2f63daf98ac3fb"
  title: |-
    Normalizing Flows for Simulation-Based Inference of Non-Markovian and Multiscale
    Stochastic Processes
  abstract: |-
    Inverse modeling of arbitrary stochastic processes remains an open challenge.
    When it is possible, inverse modeling often requires problem-specific tools due
    to incalculable or intractably costly likelihoods. Meanwhile, given a model and
    parameters, forward simulation is usually straightforward, computationally cheap,
    and often parallelizable. Recent developments in likelihood-free or simulation-based
    inference leverage neural network models, particularly normalizing flows, provide
    a method to efficiently approximate likelihoods based only on realizations of
    the underlying stochastic process (samplable from forward simulations). Here,
    we apply normalizing flows to address easy to  simulate dynamics for which likelihood
    calculation has been previously challenging, including: non-Markovian protein
    production kinetics across cell division, and a large state space model of multi-stage
    gene expression. In both cases, we see that normalizing flows can accurately approximate
    likelihoods across different measurement ranges, making it ideal for multi-scale
    processes.
- poster: 139027
  presenter: "Pickarski, Adam"
  email: "1cad251a0c2fa618"
  title: "MS4: Continuum Limits for Dimension Reduction"
  abstract: |-
    Dimension reduction is a common preprocessing step in many data science
    pipelines, which enables data visualization, feature construction, and greater
    intuition and interpretability of datasets. However, basic properties of many
    dimension reduction algorithms, such as statistical consistency, efficiency of
    parametrization, and structural properties of the learned embeddings, are not
    well-understood. This presentation discusses basic properties of the population-level
    dimension reduction problem from a variational viewpoint: results include Monge-type
    existence results, parametric representation results, and a counterintuitive result
    about the fact that in several natural cases the learned embeddings always contain
    discontinuities. These results raise questions about inherent biases in dimension
    reduction techniques, and in particular about the validity of the scientific conclusions
    drawn from interpretations of cluster plots in applications. For example, in genomics,
    artificial clustering could erroneously separate essential genetic subpopulations
    into different groups, potentially resulting in ineffective or even harmful treatments.
- poster: 140550
  presenter: "Piekenbrock, Matt"
  email: "324444fc7ef18e11"
  title: "MS6: Spectral Families of Persistent Rank Invariants"
  abstract: |-
    Using the fact that the persistent rank invariant determines the persistence
    diagram and vice versa, we introduce a framework for constructing families of
    continuous relaxations of the persistent rank invariant for persistence modules
    indexed over the real line. Like the rank invariant, these families obey inclusion-exclusion,
    are derived from simplicial boundary operators, and encode all the information
    needed to construct a persistence diagram. Unlike the rank invariant, these spectrally-derived
    families enjoy a number of stability and continuity properties typically reserved
    for persistence diagrams, such as smoothness and differentiability. By leveraging
    its relationship with combinatorial Laplacian operators, we find the non-harmonic
    spectra of our proposed relaxation encode valuable geometric information about
    the underlying space, prompting several avenues for geometric data analysis. Moreover,
    as trace-class operators, we also find these Laplacian operators can be efficiently
    approximated using just O(n) space with a randomized algorithm based on the stochastic
    Lanczos quadrature method, suggesting the potential for improving the scalability
    of the persistence computation. We investigate the utility of our relaxation with
    applications in topological data analysis and machine learning, such as parameter
    optimization, shape classification, and mesh simplification.
- poster: 140159
  presenter: "Pocivavsek, Luka"
  email: "c52168edc03c187"
  title: "Evolution of Aortic Geometry Modeled with Morphoelastic Growth"
  abstract: |-
    Growth is ubiquitous in biological tissues. While growth is normal during
    development, it is often a hallmark of or response to disease in adults. We study
    the evolution of the largest blood vessel in the human body, the aorta. Looking
    at a cohort of 150 normal people ranging from 1 years to 93 years old, we calculate
    the shape operator on the aortic surface and perform a Gauss mapping of each surface
    to the unit sphere. This allows us to study changes in shape independently of
    changes in size. Using constant rate isotropic growth, where the volume of all
    finite elements increases proportionally to the growth tensor, we can trace out
    the trajectory of normal aortic growth. A second cohort of patients, with diseased
    aneurysmal or dissected aortas, was shown to geometrically diverge from the shape-preserving
    normal growth. Diseased aortic shape evolution is highly sensitive to the initial
    geometry. Furthermore, the correct shape evolution cannot be captured with a single
    growth law. Globally the shape evolution of diseased aortas is shown to be similar
    to the development of amplitude fluctuations on a sphere that can be modeled using
    spatially heterogenous growth. Aortic surfaces are sub-divided into patches of
    relatively constant Gaussian curvature. Each individual patch grows with a varying
    growth rate coupled to the mean Gaussian curvature within the patch, this strategy
    allows us to reproduce shape evolution of diseased aortas.
- poster: 139561
  presenter: "Powell, William"
  email: "aed6a47121cec12b"
  title: "MS14: Stochastic Optimization with Arbitrary Recurrent Data Sampling"
  abstract: |-
    For obtaining optimal first-order convergence guarantees for stochastic
    optimization, it is necessary to use a recurrent data sampling algorithm that
    samples every data point with sufficient frequency. Most commonly used data sampling
    algorithms (e.g., i.i.d., MCMC, random reshuffling) are indeed recurrent under
    mild assumptions. In this work, we show that for a particular class of stochastic
    optimization algorithms, we do not need any further property (e.g., independence,
    exponential mixing, and reshuffling) beyond recurrence in data sampling to guarantee
    optimal rate of first-order convergence. Namely, using regularized versions of
    Minimization by Incremental Surrogate Optimization (MISO), we show that for non-convex
    and possibly non-smooth objective functions with constraints, the expected optimality
    gap converges at an optimal rate $O(n^{-1/2})$ under general recurrent sampling
    schemes. Furthermore, the implied constant depends explicitly on the 'speed of
    recurrence', measured by the expected amount of time to visit a data point, either
    averaged ('target time') or supremized ('hitting time') over the target locations.
    We discuss applications of our general framework to decentralized optimization
    and distributed non-negative matrix factorization. This poster is associated with
    the section "Efficient and robust optimization techniques for structured data
    learning" (80608).
- poster: 140548
  presenter: "Prinster, Drew"
  email: "5994061819463f4a"
  title: |-
    MS15: Conformal Validity Guarantees Exist for Any Data Distribution (and
    How to Find Them)
  abstract: "As artificial intelligence (AI) / machine learning (ML) gain widespread adoption,  practitioners are increasingly seeking means to quantify and control the risk these  systems incur. This challenge is especially salient when such systems have autonomy  to collect their own data, such as in black-box optimization and active learning, where  their actions induce sequential feedback-loop shifts in the data distribution. Conformal  prediction is a promising approach to uncertainty and risk quantification, but prior  variants' validity guarantees have assumed some form of “quasi-exchangeability' on  the data distribution, thereby excluding many types of sequential shifts. In this paper we  prove that conformal prediction can theoretically be extended to any joint data  distribution, not just exchangeable or quasi-exchangeable ones. Although the most  general case is exceedingly impractical to compute, for concrete practical applications  we outline a procedure for deriving specific conformal algorithms for any data  distribution, and we use this procedure to derive tractable algorithms for a series of  AI/ML-agent-induced covariate shifts. We evaluate the proposed algorithms empirically  on synthetic black-box optimization and active learning tasks."
- poster: 139652
  presenter: "Pritchard, Nathaniel"
  email: "8dca51bddd4b829b"
  title: "MS13: Scalable Sketching Through Shared Preconditioning"
  abstract: |-
    Solving large-scale data science problems requires the development of new large-scale linear solvers. One such linear solver is sketching methods (e.g. Randomized Block Kaczmarz and Randomized Block Coordinate Descent). These methods “sketch" the data system to generate iterative, easy-to-compute updates to a solution. By working with sketches, these methods often allow for more efficient memory operations, thus faster performance for large-scale problems. We can improve performance by generating updates using Krylov solvers rather than exact solvers. However, making this substitution requires efficient-to-compute preconditioners. This poster will present theoretical and experimental results that leverage the similarity of the “sketches" to form a common preconditioner that we compute for one system and apply to all others.  We further demonstrate that these modified sketching methods substantially outperform state-of-the-art methods on large linear systems.
- poster: 139781
  presenter: "Propp, Adrienne"
  email: "9f4c236b727905ce"
  title: "MS30: Learning Dirichlet-to-Neumann Maps for Graphs with Gaussian Processes"
  abstract: |-
    We present a novel method for learning Dirichlet-to-Neumann maps on graphs
    using Gaussian processes, focusing on cases where data obeys some conservation
    constraint. We aim to construct a data-driven surrogate model and uncertainty
    estimates for an entire graph, even when data is only available for a subset of
    nodes and edges. Our approach combines Gaussian processes and discrete exterior
    calculus to infer a relationship between node and edge values. By optimizing over
    the reproducing kernel Hilbert space (RKHS) norm and applying a maximum likelihood
    estimation (MLE) penalty on kernel complexity, we ensure that our surrogate models
    adhere to conservation laws without excessive complexity. We demonstrate the effectiveness
    of our method on various datasets, highlighting its potential for scientific applications
    with limited data.
- poster: 140156
  presenter: "Pugar, Joseph"
  email: "4987b506150ff604"
  title: "Thoracic Aortic Geometry in Scale Space for Enhanced Clinical Decision-Making"
  abstract: "The scale and resolution of anatomy extracted from medical CT images is paramount in the study and advancement of clinical decision-making tools. Furthermore, the quality and signal that can be computed from an image goes beyond trivial 1D size metrics and can introduce compounding error in higher dimensional features such as anatomical shape. Recent computational methods have been presented which quantify the shape of the aorta and successfully use a tandem size-shape feature space to increase the accuracy in classifying the impending success of surgery from pre-operative imaging data. The most promising of the metrics is the fluctuation in total integrated Gaussian curvature (dK) over a surface mesh model of the aortic external surface. However, a significant tradeoff has been identified between noise reduction and shape signal preservation within the scale space parameters; namely smoothing intensity, meshing density, and the partitioning performed in the computation of (dK). The presented is an analysis into identifying the regions of high signal strength within a 300+ aortic dissection patient dataset – implying a critical resolution scale at which quantified shape variations are linked to aortic intervention outcomes. Additionally, these results allow the benefiting clinical prediction models to contain inherent confidence intervals sourced from the image properties rather than the statistics of a potentially biased or skewed patient training population."
- poster: 139634
  presenter: "Qaraad, Mohammed"
  email: "7a95810490300f4e"
  title: |-
    A Variant Equilibrium Optimizer Approach for Dimensionality Reduction in
    Cancer Microarray Data
  abstract: |-
    In cancer research, identifying relevant gene subsets from high-dimensional
    microarray data is crucial for accurate classification and understanding of underlying
    biological mechanisms. This work proposes a novel variant of the Equilibrium Optimizer
    (EO) algorithm tailored for gene subset selection in cancer microarray data analysis.
    Our approach integrates a rank-based mechanism for parameter updates and incorporates
    an adaptive exploration factor, enhancing the algorithm's exploration-exploitation
    balance. Furthermore, we strategically deploy the logistic map to augment the
    performance of the Enhanced Local Escaping Operator (eLEO) within the original
    EO framework. The efficacy of our proposed algorithm is evaluated by conducting
    experiments on standardized test functions from the IEEE Congress on Evolutionary
    Computation (CEC) 2021 competition, which serves as a benchmark for assessing
    optimization algorithms. The proposed algorithm is also applied to real-world
    cancer microarray datasets to demonstrate its practical utility in improving classification
    accuracy by selecting optimal gene subsets and reducing dimensionality. The experimental
    results showcase the competitiveness of the proposed variant EO against cutting-edge
    metaheuristics, affirming its efficacy in tackling high-dimensional optimization
    problems and its potential for advancing cancer research through improved data
    analysis techniques.
- poster: 140307
  presenter: "Qi, Di"
  email: "b9c408ef27af81df"
  title: |-
    MS24: A Statistical-Stochastic Reduced-Order Model for Multiscale Turbulent
    Systems
  abstract: |-
    The capability of using imperfect stochastic and statistical reduced-order
    models to capture key statistical features in multiscale nonlinear dynamical systems
    is investigated. A systematic framework is proposed using a high-order statistical
    closure enabling accurate prediction of leading-order statistical moments and
    probability density functions in multiscale complex turbulent systems. A new efficient
    ensemble forecast algorithm is developed dealing with the nonlinear multiscale
    coupling mechanism as a characteristic feature in high-dimensional turbulent systems.
    To address challenges associated with closely coupled spatio-temporal scales in
    turbulent states and expensive large ensemble simulation for high-dimensional
    complex systems, we introduce efficient computational strategies using the so-called
    random batch method. It is demonstrated that crucial principal statistical quantities
    in the most important large scales can be captured efficiently with accuracy using
    the new reduced-order model in various dynamical regimes of the flow field with
    distinct statistical structures. Finally, the proposed model is applied for a
    wide range of problems in uncertainty quantification, data assimilation, and control.
- poster: 139886
  presenter: "Qian, Haoyang"
  email: "4b0779010b5b1e6c"
  title: |-
    Explosive Polarization-Depolarization in Opinion Dynamics with Nonlinear
    Incidence
  abstract: |-
    Polarization significantly influences societal divisions across economic,
    political, religious, or ideological lines. Understanding these mechanisms is
    key to devising strategies to mitigate such divisions and promote depolarization.
    Our study examines how nonlinear incidence in opinion dynamics affects polarization
    within structured communities. The model:  \begin{equation}  \dot{x}_i = - \gamma
    x_i + \beta (1 - x_i)  \sum_{k=1}^{N} A_{ik} x_k (1 + \alpha x_k^{p-1}),  \end{equation}  where
    \(x_i\) measures individual opinion intensity, \(\gamma\) the resilience rate,
    \(\beta\) the change rate, \(\alpha\) quantifies the nonlinear incidence influence,
    with \(p\) as the exponent, and \(A\) representing the adjacency matrix. Setting
    \(\alpha=0\) initially simplifies to a linear model, but at \(\alpha=1\), the
    system undergoes a first-order phase transition , i.e., explosive polarization,
    respectively depolarization,the latter resilient to \(\gamma\) changes. We developed
    a degree-based model, \(\langle \dot{x} \rangle = (\tilde{\beta} - \gamma) \langle
    x \rangle + \tilde{\beta} (\alpha - 1) \langle x \rangle^2 - \alpha \tilde{\beta}
    \langle x \rangle^3\), representing average opinion. Continuity at critical points
    is crucial for predicting polarization patterns, resembling the critical eigenvector
    in continuous transitions.
- poster: 139725
  presenter: "Qian, Elizabeth"
  email: "23ab6f0908539d65"
  title: "MS53: The Fundamental Subspaces of Ensemble Kalman Inversion"
  abstract: "The Ensemble Kalman Inversion (EKI) method re-interprets the ensemble Kalman filter as an adjoint/derivative-free iterative method for solving least squares problems, particularly in the context of inverse problems. The method has a history of use in applications in the geosciences, and was first investigated more mathematically in [Iglesias, Law, & Stuart 2013]. This initial work laid the foundation for a decade of subsequent developments of both variations of the method (e.g., by adapting different Kalman filtering methods to the inversion setting) as well as theoretical analyses proving rates of convergence of EKI in the linear setting (largely using ODE theory and stochastic calculus to analyze continuous-time limits of the iterative method). This poster aims to provide a broadly accessible introduction to EKI and its linear theory. In particular, we present a new analysis approach that analyzes the discrete iteration directly, relying only on basic knowledge of probability and linear algebra. Our analysis naturally gives rise to an elegant interpretation of the method’s properties through the lens of “Fundamental Subspaces” of EKI, in the style of the fundamental subspaces of linear algebra. Numerical experiments illustrate our results."
- poster: 139687
  presenter: "Qiu, Yuan"
  email: "d1cb1f9c773bf0ca"
  title: "MS31: Derivative-enhanced Deep Operator Network"
  abstract: |-
    Deep operator networks (DeepONets), a class of neural operators that learn
    mappings between function spaces, have recently been developed as surrogate models
    for parametric partial differential equations (PDEs). In this work we propose
    a derivative-enhanced deep operator network (DE-DeepONet), which leverages the
    derivative information to enhance the prediction accuracy, and provide a more
    accurate approximation of the derivatives, especially when the training data are
    limited. DE-DeepONet incorporates dimension reduction of input into DeepONet and
    includes two types of derivative labels in the loss function for training, that
    is, the directional derivatives of the output function with respect to the input
    function and the gradient of the output function with respect to the physical
    domain variables. We test DE-DeepONet on three different equations with increasing
    complexity to demonstrate its effectiveness compared to the vanilla DeepONet.
- poster: 140199
  presenter: "Radhakrishnan, Adityanarayanan"
  email: "2cedb34ed62c1bd2"
  title: |-
    MS8: Mechanism for Feature Learning in Neural Networks and Backpropagation-Free
    Machine Learning Models
  abstract: |-
    Understanding how neural networks learn features, or relevant patterns
    in data, for prediction is necessary for their reliable use in technological and
    scientific applications. We propose a unifying mechanism that characterizes feature
    learning in neural network architectures. Namely, we show that features learned
    by neural networks are captured by a statistical operator known as the average
    gradient outer product (AGOP). Empirically, we show that the AGOP captures features
    across a broad class of network architectures including convolutional networks
    and large language models. Moreover, we use AGOP to enable feature learning in
    general machine learning models through an algorithm we call Recursive Feature
    Machine (RFM). Overall, this line of work advances our fundamental understanding
    of how neural networks extract features from data, leading to the development
    of novel, interpretable, and effective models for use in scientific applications.
- poster: 140182
  presenter: "Raghavan, Krishnan"
  email: "aff80a1be391414b"
  title: "MS25: The Effect of Uncertainty on Machine Learning Through Optimal Control"
  abstract: |-
    A typical learning process of a neural network can be described by the interaction between the data, the weights and the model architecture. In a standard regime, this interaction involves updating the weights of the neural network model with respect to a series of data batches for a fixed architecture. Multitude of decisions such as the number of layers, the activation function, learning rate, etc; are made during this process of learning. As these decisions are made apriori in the standard setup, there is an inherent uncertainty in the learning process due to the discrepancy between these decisions and the nature of the data. In this talk, we will demonstrate that this uncertainty could be quantified and further leveraged to improve performance in a neural network training process. Towards this end, we will enunciate a novel optimal control-driven mathematical formulation where the dynamics of learning are represented by a differential equation. Subsequently, we will elucidate, “how to quantify this uncertainty during learning?" and provide insights into the effect of the this uncertainty. We will end this talk with insights on “how to use uncertainty to correct the behavior of the graph neural network while performing anomaly detection?"
- poster: 140218
  presenter: "Raj, Sunny"
  email: "8847f2c25caf80cf"
  title: |-
    Low-Cost Robotic Arms and Large Language Models for Autonomous Scientific
    Laboratories
  abstract: |-
    The autonomous scientific laboratories paradigm envisions autonomous robots
    performing trial-and-error scientific experiments. The motivation for this paradigm
    is to remove the uncertainties present due to human experimenters, where a significant
    portion of the experiments performed by them are not reproducible. The workflow
    execution interface (WEI) library from Argonne National Laboratory provides a
    way of integrating multiple devices and sensors for executing complex scientific
    workflows. However, existing experiments rely on expensive robots and sensors
    for performing precise experiments and are thus not good educational tools for
    upcoming engineers and researchers. To solve this issue, we propose to use the
    low-cost Arduino Braccio robot and a web camera to perform a trial-and-error object-moving
    experiment. The interface to the experiment is a fine-tuned Llama 3 large language
    model (LLM) that takes instructions in English and then converts them into Python
    code for robot movement. To simulate real-world experiments, where a number of
    them might fail, the sizes of the objects have been carefully chosen so that the
    robot has some difficulty in picking up and placing the object, and the experiments
    fail around 75% of the time. The goal of this work is to demonstrate autonomous
    scientific experiments using low-cost equipment and sensors to provide more accessible
    training opportunities to students and professionals.
- poster: 140228
  presenter: "Ramasamy Ramamurthy, Sreenivasan"
  email: "cdc1c924ae184524"
  title: "Mitigating Hallucinations in Large Language Models Using Meta-Cognition"
  abstract: |-
    Large language models (LLMs) can produce human-like, coherent, and contextually
    appropriate responses to queries. Despite significant advancements in LLMs, they
    are susceptible to generating incorrect information, posing challenges for their
    use in safety-critical applications such as autonomous systems. This work investigates
    the feasibility of leveraging meta-cognition strategies to mitigate hallucinations
    in LLMs while preserving acquired knowledge. By integrating self-awareness and
    error detection mechanisms into LLM architectures, we aim to evaluate the effectiveness
    of meta-cognition in identifying and suppressing hallucination phenomena, which
    often result in the generation of erroneous sequences.
- poster: 140208
  presenter: "Ramesh, Nikhil"
  email: "7086899800ed7f72"
  title: "Learning Effective Memory Kernels For Coarse-Grained Simulations"
  abstract: |-
    Accurately capturing the dynamical behavior of real physical systems usually
    require Non-Markovian models to account for memory effects. The Generalized Langevin
    Equation (GLE) provides a robust framework for building such models.  However,
    the main bottleneck in using the GLE is determining the form and parameters of
    the memory kernel. Here we propose a new approach to circumvent this by using
    a Bayesian framework to extract these parameters from experimental or simulation
    data. To introduce our method, we first focus on a simple case of a one-dimensional
    free particle with an exponentially decaying memory kernel. Synthetic data is
    generated through numerical integration of the GLE by mapping the system onto
    an equivalent, higher dimensional Markovian system. Using the numerical update
    rule and by eliminating all unphysical variables introduced in the mapping, we
    derive an expression for the likelihood. With a suitable prior, we show that we
    can successfully sample the decay rate of the corresponding memory kernel for
    the synthetic data. To establish the practical applicability of our method, we
    also generalize our likelihood derivation for the case of a coarse-grained model
    of a protein and demonstrate its efficacy in predicting the memory kernel decay
    rates of different residues in the protein.
- poster: 139765
  presenter: "Ray, Deep"
  email: "7149b09caa64f860"
  title: "MS17: Physics-Driven Conditional GAN to Solve Inverse Problems"
  abstract: |-
    Recently, a novel conditional Wasserstein GAN was proposed (Ray et al.,
    2023) to solve physics-based inverse problems. Training this generative model
    required samples from the joint distribution of the inferred and measured fields.
    Thus, the GAN implicitly leans the underlying "physics" via the dataset itself.
    In this poster, we propose an alternate approach for training the conditional
    GAN to solve PDE-based inverse problems, where the PDE structure is carefully
    integrated into the training objective function. Such a framework is capable of
    reducing the data complexity, which can play a critical role in applications where
    it is not possible to generate a lot of labelled training samples.
- poster: 140094
  presenter: "Ren, Yinuo"
  email: "19e43b52fb735f84"
  title: "MS33: Confronting Singularities in Diffusion-Based Generative Modelling"
  abstract: |-
    Recently there have been ongoing efforts to apply diffusion-based generative
    modeling and sampling to physical and chemical systems. One of the challenges
    in these applications is the singularities that may present in the learned score
    function. We aim to investigate the impact of these singularities on the performance
    of diffusion-based algorithms and provide new insights that may alleviate the
    issue.
- poster: 140263
  presenter: "Rheaume, Evan"
  email: "dd5482170e8067dd"
  title: "Using Phase Space Perturbations for Defibrillation in Cardiac Tissue"
  abstract: "A voltage wave produced by the cascade of cardiac cells’ action potentials triggers the contraction of the heart. These voltage waves can be characterized by four level sets: the excitation and refractory fronts and backs. When the level sets intersect in physical space, they form a phase singularity which is characteristic of the tip of a spiral wave. When spiral waves occur in the heart, they sustain fibrillation (an abnormal and fast, irregular heartbeat). Previous studies show that a small stimulus across the refractory wave back allows for the instantaneous movement of the spiral wave’s tip across physical space which allows for termination of small spiral waves. Here we use the FitzHugh-Nagumo model to show that the elimination of any complex distribution of spiral waves (defibrillation) is accomplishable at any point in time and phase space by an appropriate perturbation performed in phase space. Singularities of spiral waves are located at the center of the limit cycle. To defibrillate, the stimulus requires a minimum region in phase space and the stimulus must move any values in this region across the phase singularity. Such a stimulus is able to eradicate all singularities and return the system to a steady state. This is a different kind of defibrillation than previously shown. These stimulations designed in phase space allow for the instantaneous termination of all spiral waves by removing the phase singularities in physical space."
- poster: 139803
  presenter: "Riegler, Erwin"
  email: "767e207ce61a1eba"
  title: "Generating Singular Distributions Through Neural Networks"
  abstract: |-
    There is a large body of work on neural networks as function approximators, either in the context of regression or  classification. In recent years another use for neural networks has emerged in  generative AI, namely for generating high-dimensional probability distributions. It was shown in “High-dimensional distribution generation through deep neural networks" by Perekrestenko et al., 2021  that the set of probability distributions supported on the unit cube in $\mathbb{R}^d$ can be approximated by  push-forwards of the uniform distribution on the unit interval through a set of neural networks of cardinality $2^{C \varepsilon^{-d}\log^2(\varepsilon)}$  to achieve an approximation error of  $\varepsilon$. In this poster, we consider the class of   probability distributions that are supported on (countable) $m$-rectifiable sets, i.e., on (countable unions of) Lipschitz images of compact sets in $\mathbb{R}^m$.  We show that such distributions can be approximated by  push-forwards of the uniform distribution on the unit interval through a set of neural networks of cardinality $2^{C \varepsilon^{-m}\log^2(\varepsilon)}$  to achieve an approximation error of  $\varepsilon$. Notably, the ambient space dimension $d$ can be replaced by the Hausdorff dimension $m$ of the rectifiable set. This result is striking as in practice the dimension of the data set can be much smaller than the dimension of the ambient space.
- poster: 140166
  presenter: "Roberts, Graham"
  email: "74e420945df95aad"
  title: |-
    MS52: Implicit Symbolic Regression for Data-Driven Discovery of Conservation
    Laws
  abstract: |-
    Discovery of conservation laws is an important challenge for data-driven
    learning of dynamical systems. Conservation laws correspond to implicit equations
    satisfied by dynamic trajectories. Symbolic regression is a key method for extracting
    concise mathematical expressions to model relationships in data. However, existing
    symbolic regression methods are primarily designed for explicit functions, and
    encounter challenges when attempting to learn implicit equations. Methods for
    recovering non-trivial implicit equations typically depend on numerical gradients,
    which require larger amounts of densely sampled training data. We develop a new
    and simple-to-use method for symbolic regression that enables learning of implicit
    equations, even with noisy training datasets, which to the best of our knowledge
    has not been widely feasible previously. Our method is based on the formulation
    of a probabilistic fitness function using the Kullback-Leibler divergence, and
    is compatible with existing symbolic regression algorithms. We demonstrate our
    method on a series of benchmark implicit equations, and use it to recover conserved
    quantities in dynamical systems.
- poster: 140073
  presenter: "Robertson, Sawyer"
  email: "6b07d1a46aec995d"
  title: |-
    MS50: All You Need Is Resistance: On the Equivalence of Effective Resistance
    and Certain Optimal Transport Problems on Graphs
  abstract: |-
    The fields of effective resistance and optimal transport on graphs are
    filled with rich connections to combinatorics, geometry, machine learning, and
    beyond. In this article we put forth a bold claim: that the two fields should
    be understood as one and the same, up to a choice of p. We make this claim precise
    by introducing the parameterized family of p-Beckmann distances for probability
    measures on graphs and relate them sharply to certain Wasserstein distances. Then,
    we break open a suite of results including explicit connections to optimal stopping
    times and random walks on graphs, graph Sobolev spaces, and a Benamou-Brenier
    type formula for 2-Beckmann distance. We further explore empirical implications
    in the world of unsupervised learning for graph data and propose further study
    of the usage of these metrics where Wasserstein distance may produce computational
    bottlenecks.
- poster: 139729
  presenter: "Rodriguez, Jose"
  email: "d8108f454c0d7f02"
  title: |-
    MS78: Applications of Singularity Theory in Applied Algebraic Geometry and
    Algebraic Statistics
  abstract: |-
    We survey recent applications of topology and singularity theory in the
    study  of the algebraic complexity of concrete optimization problems in applied
    algebraic geometry and algebraic statistics
- poster: 140093
  presenter: "Rosenfeld, Joel"
  email: "525763ab60ac0853"
  title: "Operator Approximations for Inverse Problems"
  abstract: |-
    This poster presents a framework for resolving inverse problems through
    the use of operator approximations over vector valued RKHSs. This generalizes
    Koopman based methods for data driven methods in dynamical systems by laying out
    the general requirements to resolve an inverse problem through an operator approximation.
    We will present three different examples of this method in action.
- poster: 139627
  presenter: "Rosso, Haley"
  email: "9a2219989ec059cc"
  title: |-
    MS1: Comparison of Weight-Parameterization Methods in Neural ODEs for Surrogate
    Modeling
  abstract: |-
    Training continuous limits of residual neural networks (ResNets), commonly
    called neural ordinary differential equations (ODEs), requires determining the
    neural network weights that define the dynamics.   As the weights in ResNets vary
    from layer to layer, it is natural to model the weights as a function of time
    and apply techniques from optimal control to perform the training.  Although there
    are some non-parametric approaches,  the weights are usually parameterized resulting
    in an apparent trade-off between the expressiveness and the number of parameters
    of the model.   In this paper, we compare different polynomial parameterizations
    and highlight the importance of choosing the basis function on supervised machine
    learning tasks motivated by PDE surrogate modeling tasks.  In this application,
    we aim at achieving high accuracy with as few parameters as possible.   Across
    various types of neural ODEs and two different optimization algorithms our results
    indicate that restricting the weights to a small dimensional subspace can yield
    accurate surrogates that generalize well and are efficient to store and evaluate.
    However, we also observe that the choice of basis functions can have dramatic
    impact on the achieved accuracy.
- poster: 139943
  presenter: "Ruiz, Luana"
  email: "f1049f9f31787695"
  title: "MS63: A Spectral Analysis of Graph Neural Networks on Dense and Sparse Graphs"
  abstract: |-
    In this work we propose a random graph model that can produce graphs at
    different levels of sparsity. We analyze how sparsity affects the graph spectra,
    and thus the performance of graph neural networks (GNNs) in node classification
    on dense and sparse graphs. We compare GNNs with spectral methods known to provide
    consistent estimators for community detection on dense graphs, a closely related
    task. We show that GNNs can outperform spectral methods on sparse graphs, and
    illustrate these results with numerical examples on both synthetic and real graphs.
- poster: 139625
  presenter: "Ruthotto, Lars"
  email: "3888f93f0a2b0bf4"
  title: "MS1: Neural ODE/SDE Training in Mixed and Low-Precision"
  abstract: |-
    Continuous-dime deep learning can be cast as optimal control of ordinary,
    partial, or stochastic differential equations. Hence, learning can become computationally
    expensive regarding the number of operations (e.g., required for time-stepping)
    and memory (e.g., to enable gradient computations). One way to lower the computational
    costs is to resort to lower precision floating or even fixed point arithmetic.
    This poster will show how to effectively combine the precisions in different parts
    of the training algorithms to facilitate learning. We will show results for supervised
    learning, optimal control, and generative models involving various differential
    equations and discretizations.
- poster: 139894
  presenter: "Sadria, Mehrshad"
  email: "84f738eff9c58559"
  title: |-
    MS34: FateNet: An Integration of Dynamical Systems and Deep Learning for
    Cell Fate Prediction
  abstract: |-
    Understanding cellular decision-making, particularly its timing and impact
    on the biological system such as tissue health and function, is a fundamental
    challenge in biology and medicine. Existing methods for inferring fate decisions
    and cellular state dynamics from single-cell RNA sequencing data lack precision
    regarding decision points and broader tissue implications. Addressing this gap,
    we present FateNet, a computational approach integrating dynamical systems theory
    and deep learning to probe the cell decision-making process using scRNA-seq data.
    By leveraging information about normal forms and scaling behavior near tipping
    points common to many dynamical systems, FateNet accurately predicts cell decision
    occurrence and offers qualitative insights into the new state of the biological
    system. Also, through in-silico perturbation experiments, FateNet identifies key
    genes and pathways governing the differentiation process in hematopoiesis. Validated
    using different scRNA-seq data, FateNet emerges as a user-friendly and valuable
    tool for predicting critical points in biological processes, providing insights
    into complex trajectories
- poster: 140001
  presenter: "Sapkota, Yogesh"
  email: "a58af350bc3f9c40"
  title: |-
    MS47: A Network Science Approach to Understanding Ev Charging Accessibility
    in the Texas Triangle
  abstract: "A robust electric vehicle (EV) charging infrastructure is essential for the widespread adoption of electric vehicles and the transition towards sustainable energy. Understanding the spatial distribution of existing infrastructures is crucial for the strategic expansion and futureproofing of the burgeoning EV market. This research presents a detailed analysis of EV charging stations within the Texas Triangle, which includes the state's four major metropolitan areas—Austin, Dallas, Houston, and San Antonio. The comprehensive dataset utilized for constructing and analyzing the network graph was sourced from the Alternative Fuels Data Center, which provides extensive information on EV charging stations. We employed a graph-based method to model the distribution of charging stations, effectively capturing the complex spatial dynamics intuitively and computationally efficiently. Additionally, this study examines how specific attributes of EV stations, such as operational hours, pricing, and accessibility, influence the network's capacity to meet regional charging demands. Graph neural networks (GNNs) were utilized to accurately model the spatial distribution of charging stations, optimized using the Adam optimizer. We also explored various complementarity metrics to assess the spatial distribution of the charging station network. Finally, we conducted a temporal analysis to examine how the network evolved."
- poster: 139904
  presenter: "Saravanan, Vijayalakshmi"
  email: "4b4ce9d8024748c7"
  title: "AI-Driven Dimensionality Reduction for Scientific Computing with HPC"
  abstract: |-
    The extensive adoption of high-performance computing across diverse scientific
    domains such as high-energy physics, chemistry, and bioinformatics has resulted
    in a substantial increase in data volumes. To tackle this influx, experimental
    facilities are increasingly turning to edge data reduction techniques. For instance,
    detectors and simulations in fields like combustion and fusion generate substantial
    data, necessitating efficient reduction methods. This abstract presents a novel
    approach to addressing the challenges posed by high-dimensional data in scientific
    research, particularly in real-time analysis scenarios. It emphasizes the significance
    of dimensionality reduction techniques in reducing complex data streams while
    preserving critical information. The proposed methodology harnesses artificial
    intelligence (AI) algorithms such as autoencoders, principal component analysis
    (PCA), and t-distributed stochastic neighbor embedding (t-SNE) to effectively
    reduce dimensionality without compromising essential insights. By integrating
    these AI-powered dimensionality reduction techniques into real-time scientific
    streaming pipelines, researchers can adeptly handle huge datasets, enabling rapid
    extraction of meaningful patterns. This approach holds promise for advancing our
    understanding of complex phenomena across various scientific domains in real-time.
    We validate and assess our proposed approach using specific scientific use cases
    such as Open Catalyst and NWChem.
- poster: 139929
  presenter: "Schneider, Fabian"
  email: "3328b232f9b0c710"
  title: |-
    Reducing the cost of posterior sampling in linear inverse problems via task-dependent
    score learning
  abstract: |-
    Score-based diffusion models (SDMs) offer a powerful way to sample from
    the posterior distribution in various Bayesian inverse problems. Traditional methods
    rely on multiple evaluations of the forward mapping to generate each posterior
    sample, which can be computationally demanding. Our research focuses on linear
    inverse problems, particularly in fields like medical imaging, where the forward
    mapping is expensive and frequent posterior sampling is necessary. We introduce
    a novel approach that eliminates the need for forward mapping evaluations during
    posterior sampling. By shifting the computational effort to an offline task, we
    train the score of a specific diffusion-like random process. This training is
    task-dependent, requiring knowledge of the forward mapping but not the measurement
    data. The conditional score for the posterior is then derived from the auxiliary
    score through affine transformations, ensuring no introduction of error. Our findings
    generalize to infinite-dimensional diffusion models and are supported by rigorous
    numerical analysis and experiments. This method can significantly reduce computational
    costs, making it highly beneficial for applications demanding frequent and efficient
    posterior sampling.
- poster: 139836
  presenter: "Schonsheck, Nikolas"
  email: "b4a7b60218258054"
  title: "MS79: Hebbian Learning of Cyclic Features of Neural Code"
  abstract: |-
    Cyclic structures are a class of mesoscale features ubiquitous in both
    experimental stimuli and the activity of neural populations encoding them. Important
    examples include encoding of head direction, grid cells in spatial navigation,
    and orientation tuning in visual cortex. The central question of our present work
    is: how does the brain faithfully transmit cyclic structures between regions?
    Is this a generic feature of neural circuits, or must this be learned? If so,
    how? While cyclic structures are difficult to detect and analyze with classical
    methods, tools from algebraic topology have proven to be particularly effective
    in understanding cyclic structures. Recently, work of Yoon et al. develops a topological
    framework to match cyclic coding patterns in distinct populations that encode
    the same information. We leverage this framework to show that, beginning with
    a random initialization, Hebbian learning robustly supports the propagation of
    cyclic structures through feedforward networks. Moreover, the efficacy of this
    learning mechanism can be modulated by the relative sizes of the input and output
    neural populations, suggesting that high-dimensional "unpackings" are necessary
    to propagate complex geometry in the context of naive neural connectivity. This
    is joint work with Chad Giusti and is supported by the Air Force Office of Scientific
    Research under award FA9550-21-1-0266.
- poster: 139849
  presenter: "Schotthoefer, Steffen"
  email: "ab95ce1502db2ff9"
  title: "MS84: Distributed Machine Learning Strategies Using Dynamical Low-Rank Approximation"
  abstract: |-
    Computational cost on client machines and communication latency are central
    challenges for federated learning, where the cost of communicating the whole weight
    matrices and training the entire model on resource-constraint edge devices is
    prohibitively expensive.  This work introduces Federated dynamical low-rank training
    (FeDLRT), a technique for improving the computing and communication efficiency
    of federated learning in training neural networks across decentralized devices.  Drawing
    from dynamical low-rank approximation, FeDLRT enables provably robust optimization
    using only a low-rank factorization on the clients and server of a federated learning
    setup.  FeDLRT offers several key benefits. It focuses on automatic rank adaptation
    of the low-rank factors of the weight matrices, a feature that significantly reduces
    communication and computational costs while maintaining model performance. Additionally,
    it incorporates variance reduction techniques and provides theoretical proof of
    global loss descent in the federated learning setting. Numerical experiments further
    demonstrate the effectiveness of FeDLRT in enhancing the efficiency and scalability
    of federated learning for neural networks.
- poster: 140191
  presenter: "Schwerdtner, Paul"
  email: "84c783ecd868ef94"
  title: |-
    MS7: Greedy Construction of Quadratic Manifolds for Nonlinear Dimensionality
    Reduction and Nonlinear Model Reduction
  abstract: |-
    Dimensionality reduction on quadratic manifolds augments linear approximations
    with quadratic correction terms. Previous works rely on linear approximations
    given by projections onto the first few leading principal components of the training
    data; however, linear approximations in subspaces spanned by the leading principal
    components alone can miss information that are necessary for the quadratic correction
    terms to be efficient. In this work, we propose a greedy method that constructs
    subspaces from leading as well as later principal components so that the corresponding
    linear approximations can be corrected most efficiently with quadratic terms.    Properties
    of the greedily constructed manifolds allow applying linear algebra reformulations
    so that the greedy method scales to data points with millions of dimensions. Numerical
    experiments demonstrate that an orders of magnitude higher accuracy is achieved
    with the greedily constructed quadratic manifolds compared to manifolds that are
    based on the leading principal components alone.
- poster: 140158
  presenter: "Scott, Mitchell"
  email: "fe13ec4432bb1d5f"
  title: "MS94: Acceleration Methods for Scientific and Data Science Applications"
  abstract: |-
    Recent years have seen increasing interest in general-purpose 'acceleration'
    methods that improve the convergence rate of fixed point iterations. Anderson
    Acceleration (AA) is a well-known example of such a method. In this poster, we
    introduce a new class of nonlinear acceleration algorithms based on extending
    conjugate residual-type procedures for linear equations. Our main algorithm exhibits
    similarities with both Anderson Acceleration and inexact Newton methods. Experimental
    results demonstrate the algorithm's effectiveness across diverse applications,
    including simulation and deep learning.
- poster: 140017
  presenter: "Sgattoni, Cristina"
  email: "7315af958a127876"
  title: |-
    MS53: An Innovative and Rapid Physics-Informed Data-Driven Method for Atmospheric
    Radiative Transfer Inversion
  abstract: |-
    FORUM (Far-infrared Outgoing Radiation Understanding and Monitoring)
    is the ninth Earth Explorer satellite mission chosen by the European Space Agency
    in 2019. It will provide interferometric measurements in the Far-InfraRed (FIR)
    spectrum, constituting 50% of Earth's outgoing longwave flux emitted by our planet
    into space. Accurate Top Of the Atmosphere measurements in the FIR are crucial
    for improving climate models. Existing instruments are insufficient, necessitating
    new computational techniques.   In early mission stages, an End-to-End Simulator
    was developed to demonstrate proof-of-concept and assess the impact of instrument
    characteristics on reconstructed atmospheric properties. From a mathematical perspective,
    two challenges arise: the radiative transfer equation (direct problem) and its
    inversion (retrieval problem). Both problems can be addressed through a full-physics
    method, particularly applying the Optimal Estimation approach, a specialized Tikhonov
    regularization scheme based on Bayesian formulation. However, full-physics methods'
    computational demands hinder Near Real-Time data analysis. Faster models are essential
    for next-gen satellites measuring hundreds of spectra per minute and climatology
    models simulating years of global-scale radiative transfer. To expedite solutions
    for both problems, a hybrid approach is employed, combining data-driven operator
    learning using the Moore-Penrose pseudoinverse with neural-network-based Tikhonov
    regularization.
- poster: 140403
  presenter: "Shams, Ehsan"
  email: "df9c8c40cfba57e1"
  title: |-
    Navigating the Exploitation-Exploration Challenge in Genetic Algorithms:
    A Novel Human-Centered Approach
  abstract: "Genetic Algorithms are potent tools for solving complex computational challenges.However, one persistent challenge for algorithm designers is striking the delicate balance between exploitation and exploration, essential for effective search navigation. In this presentation, a new interactive approach for handling the exploitation-exploration tradeoff dynamics within the search process of Genetic algorithms is proposed.Unlike the conventional approach, the search process will not be compromised of a single-phase nor the decision-maker tuning efforts will be distributed among the algorithm’s traditional parameters such as defining new crossover and mutation operators internal to the algorithm to influence its search navigation. Instead, a human-centered two-phase search process, compromised of a global search phase followed by a local search phase will be utilized. In this framework, the designer plays the central role in directing the algorithm's search navigation through the focused tuning efforts of a new search space size control parameter external to the algorithm which proves itself to be the most dominant parameter in-effect to the algorithm’s navigation process.   The broad applicability of this approach holds the key to unlocking unexplored optimization capabilities across the class of evolutionary algorithms, suggesting a potentially transformative step in the field. We demonstrate the power of this new approach on well-known benchmark problems in optimization."
- poster: 140079
  presenter: "Shamshad, Marrium"
  email: "806a85efee10a987"
  title: |-
    A Functional Network Criterion for Identifying Seizure Onset Zones from Ieeg
    Recordings
  abstract: |-
    Epilepsy, affecting millions worldwide, poses significant challenges to
    patients and healthcare providers due to its debilitating nature and limited treatment
    options.   The critical clinical goal for drug-resistant epilepsy lies in the
    localization of the initiation site of the seizure (i.e., the seizure focus).  It
    is this localization that allows for successful surgical intervention.  Surgical
    interventions targeting the epileptogenic zone are crucial for those unresponsive
    to medications, but the lack of precise biomarkers hampers surgical efficacy.  To
    address these difficulties we conduct a retrospective study of ictal iEEG data
    by reconstructing the functional network using Granger causal analysis.  We study
    the role of functional brain networks in controlling the onset of epileptic seizures
    and how their unfolding dynamics localize the seizure onset zone.  Here we show
    an effective method for the identification of the seizure onset zone by analyzing
    the temporal derivative of standard graph theoretic measures.   We find that the
    temporal derivative of node degree is particularly effective at identifying candidate
    nodes of the seizure focus of Engel class I cases.  This method offers an innovative
    clinical tool that promises real-world benefit.
- poster: 139641
  presenter: "Sharma, Tanvi"
  email: "1cead9d158d5189b"
  title: |-
    Exploring Covid-Related Relationship Extraction: Contrasting Data Sources
    and Analyzing Misinformation
  abstract: |-
    The COVID-19 pandemic presented an unparalleled challenge to global healthcare
    systems. A central issue revolves around the urgent need to swiftly amass critical
    biological and medical knowledge concerning the disease, its treatment, and containment.
    Remarkably, text data remains an underutilized resource in this context. In this
    paper, we delve into the extraction of COVID-related relations using transformer-based
    language models, including Bidirectional Encoder Representations from Transformers
    (BERT) and DistilBERT. Our analysis scrutinizes the performance of five language
    models, comparing information from both PubMed and Reddit, and assessing their
    ability to make novel predictions, including the detection of "misinformation."
    Key findings reveal that, despite inherent differences, both PubMed and Reddit
    data contain remarkably similar information, suggesting that Reddit can serve
    as a valuable resource for rapidly acquiring information during times of crisis.
    Furthermore, our results demonstrate that language models can unveil previously
    unseen entities and relations, a crucial aspect in identifying instances of misinformation.
- poster: 139737
  presenter: "Sharon, Nir"
  email: "c5ac584a56694a1c"
  title: "MS95: Refining Particle Picking in Cryo-Em"
  abstract: |-
    Single-particle cryo-electron microscopy (cryo-EM) is a powerful technique
    for determining high-resolution protein structures in their near-native states.
    In cryo-EM, the experimental data consists of projection images, often called
    micrographs, each containing numerous individual protein projections known as
    particles. Due to the low signal-to-noise ratio (SNR) typically exhibited by micrographs,
    many particles are required to achieve a three-dimensional (3D) protein reconstruction
    at near-atomic resolution. Identifying the tomographic projections contained in
    each micrograph is known as particle picking. Many semi-automatic and automatic
    methods have been developed to address this problem. In this poster, we propose
    adding a new step to the computational pipeline, which refines the output of a
    particle picker and facilitates the conventional following steps of 2D classification
    and 3D recovery by centering the picked images and eliminating outliers. The poster
    is based on joint work with Ayelet Heimowitz (Ariel University) and Lev Kapnulin
    (Tel Aviv University).
- poster: 140430
  presenter: "Shaw, Sage"
  email: "e56baa9811370a98"
  title: "MS36: Radial Basis Function Methods for Neural Field Models"
  abstract: |-
    Neural field models are non-linear systems of integro-differential equations
    intended to model large-scale neural activity. There is growing interest in identifying
    efficient and accurate schemes for simulating neural field models as they can
    capture activity dynamics that spread across wide swathes of tissues and that
    reflect highly complex neural architecture. Recently, a framework has been put
    forth for analyzing neural field solvers (Avitable 2023) that separates the error
    due to the numerical representation of the solution (projection) and the error
    due to approximating the integral operator (quadrature). In this poster, we demonstrate
    using Radial Basis Function (RBF) interpolation and quadrature methods to combine
    and simplify this error analysis and to create efficient, robust, and high-order-accurate
    neural field solvers.
- poster: 139624
  presenter: "Shen, Dixin"
  email: "c0807eab40fbf5c9"
  title: |-
    A Regularized Cox Hierarchical Model for Incorporating Annotation Information
    in Predictive Omic Studies
  abstract: "Background: Associated with high-dimensional omics data there are often “meta-features” such as pathways and functional annotations that can be informative for predicting an outcome of interest. We developed a regularized hierarchical framework for integrating meta-features, with the goal of improving prediction and feature selection performance with time-to-event outcomes.    Methods: A hierarchical framework is deployed to incorporate meta-features. Regularization is applied to the omic features as well as the meta-features so that high-dimensional data can be handled at both levels. The proposed hierarchical Cox model can be efficiently fitted by a combination of iterative reweighted least squares and cyclic coordinate descent.    Results: In a simulation study we show that when the external meta-features are informative, the regularized hierarchical model can substantially improve prediction performance over standard regularized Cox regression. Importantly, when the external meta-features are uninformative, the prediction performance based on the regularized hierarchical model is on par with standard regularized Cox regression, indicating robustness of the framework. We illustrate the proposed model with applications to breast cancer and melanoma survival based on gene expression profiles, which show the improvement in prediction performance by applying meta-features, as well as the discovery of important omic feature sets with sparse regularization at meta-feature level."
- poster: 140157
  presenter: "Shenouda, Joseph"
  email: "78507f1733b9d57d"
  title: |-
    MS8: Variation Spaces for Multi-Output Neural Networks: Insights on Multi-Task
    Learning and DNN Compression
  abstract: |-
    This paper introduces a novel theoretical framework for the analysis of
    vector-valued neural networks through the development of vector-valued variation
    spaces, a new class of reproducing kernel Banach spaces. These spaces emerge from
    studying the regularization effect of weight decay in training networks with activations
    like the rectified linear unit (ReLU). This framework offers a deeper understanding
    of multi-output networks and their function-space characteristics. A key contribution
    of this work is a representer theorem for the vector-valued variation spaces.
    This representer theorem establishes that shallow vector-valued neural networks
    are the solutions to data-fitting problems over these infinite-dimensional spaces,
    where the network widths are bounded by the square of the number of training data.
    This observation reveals that the norm associated with these vector-valued variation
    spaces encourages the learning of features that are useful for multiple tasks,
    shedding new light on multi-task learning with neural networks. Finally, this
    paper develops a connection between weight-decay regularization and the multi-task
    lasso problem. This connection leads to novel bounds for layer widths in deep
    networks that depend on the intrinsic dimensions of the training data representations.
    This insight not only deepens the understanding of the deep network architectural
    requirements, but also yields a simple convex optimization method for deep neural
    network compression.
- poster: 140276
  presenter: "Shi, Wen"
  email: "939790b4272681cd"
  title: |-
    MS36: An Automated Method for Spike Ripples Localization in Human Intracranial
    Recordings
  abstract: |-
    Spike ripples, the combination of epileptiform spikes and ripples, have
    been proposed as a reliable biomarker for the epileptogenic zone. However, detection
    of spike ripples requires a fast and reliable automated method to identify these
    infrequent, transient events. To that end, we combined two previously developed
    spike ripple detection strategies for use in noninvasive scalp electroencephalogram
    (EEG) recordings: a feature-based algorithm applied to time series data, and a
    convolutional neural network (CNN) applied to spectrogram images. To apply this
    method to invasive intracranial data, we first retrained the CNN detector on hand-marked
    events. We then created an intracranial detector that combined both the feature-based
    and CNN strategies. We validated the resulting detector on the hand-marked events
    using a leave-one-out cross-validation procedure. We find that, while the intracranial
    spike ripple detector identified less than half of hand-marked events (sensitivity,
    42%), it had high precision (78.5%) and a low false positive rate (6.7%), resulting
    in balanced performance (F1 = 0.68). Applying the intracranial spike ripple detector
    to 109 subjects collected as part of a multicenter, international study, we find
    that most spike ripples were removed in subjects with good surgical outcome for
    epilepsy (p < 0.001), and that automatically detected spike ripples have improved
    specificity for epileptogenic tissue compared to alternative electrographic biomarkers.
- poster: 140010
  presenter: "Shi, Yichuan"
  email: "60a4cfae504236bc"
  title: |-
    MS62: Dealing Doubt: Unveiling Threat Models in Gradient Inversion Attacks
    under Federated Learning
  abstract: |-
    Federated Learning (FL) has emerged as a leading paradigm for decentralized,
    privacy preserving machine learning training. However, recent research on gradient
    inversion attacks (GIAs) have shown that gradient updates in FL can leak information
    on private training samples. While existing surveys on GIAs have focused on the
    honest-but-curious server threat model, there is a dearth of research categorizing
    attacks under the realistic and far more privacy-infringing cases of malicious
    servers and clients. Here we present a survey and novel taxonomy of GIAs that
    emphasize FL threat models, particularly that of malicious servers and clients.
    We first formally define GIAs and contrast conventional attacks with the malicious
    attacker. We then summarize existing honest-but-curious attack strategies, corresponding
    defenses, and evaluation metrics. Critically, we dive into attacks with malicious
    servers and clients to highlight how they break existing FL defenses, focusing
    specifically on reconstruction methods, target model architectures, target data,
    and evaluation metrics. Lastly, we discuss open problems and future research directions.
- poster: 140202
  presenter: "Shi, Yunpeng"
  email: "7301c7b58232362a"
  title: "MS50: Fast Alignment of Images in Sliced 2-Wasserstein Distance"
  abstract: |-
    We present a fast algorithm for aligning images using optimal transport.
    Our method is based on the sliced Wasserstein distance and computes the 1-D Wasserstein
    distance between radial line projections of the input images. Using this approach,
    we develop an algorithm that can align two $L\times L$ images in $\mathcal{O}(L^2
    \log L)$ operations. We show that our method is robust to rotations, translations
    and deformations in the images.
- poster: 139883
  presenter: "Shi, Ji"
  email: "3da7392950277218"
  title: "MS90: Efficient Clustering on Riemannian Manifolds Using Fr\\'echet Embeddings"
  abstract: "Symmetric Positive Definite (SPD) matrices, in particular correlation matrices, appear in various applications from applied mathematics and engineering, most notably in neuro-imaging applications such as Diffusion Tensor Imaging and Functional Magnetic Resonance Imaging, where they are employed to model the strength of neural connections between different brain sites. Mathematically, the set of SPD matrices is not a vector subspace of the Euclidean space under standard matrix addition and scalar multiplication, however it possesses a smooth manifold structure that can be endowed with a Riemannian metric. Hence the proper measure of similarity between SPD matrices is not an Euclidean distance but a Riemannian distance that can capture the intrinsic geometrical structure of the underlying space. Unfortunately, computation of distances in the Riemannian manifold of SPD matrices is rather expensive with the result that it is exceedingly expensive to compute k-means of SPD matrices directly in the Riemannian setting. Here we present and demonstrate a novel approach to efficiently cluster data on the space of SPD matrices taking advantage of a specially designed Fréchet embedding."
- poster: 140583
  presenter: "Shi, Bobby"
  email: "fec4f46d7f31f48a"
  title: "Concentration Inequalities for Sums of Markov Dependent Random Matrices"
  abstract: |-
    We give Hoeffding and Bernstein-type concentration inequalities for the
    largest eigenvalue of sums of random matrices arising from a Markov chain. We
    consider time-dependent matrix-valued functions on a general state space, generalizing
    previous that had only considered Hoeffding-type inequalities, and only for time-independent
    functions on a finite state space. In particular, we study a kind of noncommutative
    moment generating function, give tight bounds on it, and use a method of Garg
    et al. to turn this into tail bounds. Our proof proceeds spectrally, bounding
    the norm of a certain perturbed operator. In the process we make an interesting
    connection to dynamical systems and Banach space theory to prove a crucial result
    on the limiting behavior of our moment generating function that may be of independent
    interest.
- poster: 139576
  presenter: "Shin, Yeonjong"
  email: "9aa29ef8afe1dd27"
  title: "MS57: Tlasdi: Thermodynamics-Informed Latent Space Dynamics Identification"
  abstract: |-
    We present a thermodynamics-informed latent space dynamics  identification
    (tLaSDI) method for learning an intrinsic invariant  manifold that complies with
    the first and second principles of  thermodynamics from data. The approach utilizes
    an autoencoder to  provide a nonlinear latent representation and the reconstruction
    of  the high-dimensional data and the corresponding latent space dynamics are
    captured using GENERIC formalism-informed neural networks (GFINNs). The GFINNs
    are designed to exactly satisfy the degeneracy condition of the GENERIC formalism,
    ensuring that the latent space dynamics adhere to fundamental thermodynamics principles.
    We develop a novel loss formulation based on a rigorous error estimate, which
    significantly improves the generalization performance. Numerical examples are
    presented to demonstrate the performance of tLaSDI, which exhibits robust generalization
    ability, even in extrapolation. In addition, an intriguing correlation is empirically
    observed between the entropy production rates in the latent space and the behaviors
    of the full-state solution.
- poster: 139087
  presenter: "Shustin, Boris"
  email: "dc9a110ab68881ef"
  title: "Structure Regularization for X-Ray Spectroscopy"
  abstract: |-
    X-ray sources continue to improve and new low-emittance sources, promise
    gains in coherent flux of up to two orders of magnitude which is potentially transformative
    for high-resolution. A basic form of compressed sensing has been deployed as a
    proof-of-concept on a single beamline at Diamond (Townsend et al., 2022), exploiting
    the fact that as there are only a small number of actual chemical states in a
    system a low-rank description can be formed, and matrix completion methods can
    be employed to allow us to fill in missing gaps in the measurement from between
    a fifth and sixth of dose otherwise needed. In this talk, we will extend this
    initial proof of concept using the low-rank structure to include additional structure
    that the chemical states are often spatially structured in a known representation.
    In addition, a low-rank plus sparse model is proposed to allow robustness to outliers.
- poster: 140220
  presenter: "Shvaiko, Sofiia"
  email: "dc6a6ccb76565752"
  title: "MS49: Noise-Tolerant Randomized Kaczmarz Variant"
  abstract: |-
    Solving large-scale linear systems is a problem that commonly arises in
    medical imaging, signal processing, and a variety of optimization subroutines.
    However, the measurement data often becomes corrupted with noise. Therefore, developing
    solvers that are robust to such inconsistencies in the data is of great interest.
    The case of symmetric noise is frequently considered, and multiple methods have
    been shown to be robust in that scenario. In contrast, our work focuses on systems
    with dense irregular noise. We propose a Randomized Kaczmarz variant that takes
    advantage of the switching step schedule to ensure convergence in the presence
    of asymmetric noise. For the minisymposium "Efficient and robust optimization
    techniques for structured data learning" (80608)
- poster: 140038
  presenter: "Si, Phillip"
  email: "ea9554791a585d11"
  title: "MS53: Scalable Data Assimilation in Latent Space Using Variational Autoencoder"
  abstract: |-
    Data assimilation in high dimensions with sparse observations can prove
    computationally challenging. By leveraging variational autoencoders and LSTMs
    to train latent dynamics, we can combine it with standard data assimilation approaches
    in the latent space to achieve fast, scalable results.
- poster: 137793
  presenter: "Sidhu, Gagandeep Kaur"
  email: "a838817474cda341"
  title: |-
    A Chaotic Niching Sparrow Search Algorithm for Solving Clustering Optimization
    Problems
  abstract: "Sparrow Search Algorithm (SSA) is one of the most recent optimization algorithms which is known for its good optimal ability along with fast convergence, Although, the SSA has a lot of merits, it is still facing numerous drawbacks namely falling into the local optima, steady convergence, etc. Therefore, a chaotic version of the Sparrow Search Algorithm (SSA) using the niching technique is proposed with different chaotic mechanisms such as bernoulli map, logistic map, chebyshev map, circle map, cubic map, iterative chaotic map, piecewise map, signer map, sinusoidal map, and tent map to address the previously mentioned drawbacks of SSA. Our proposed algorithm is named a Chaotic Niching Sparrow Search Algorithm (CNSSA) where the niching technique is used for updating the position of followers and scouters in SSA. The suggested CNSSA 's performance is assessed using a few benchmark test functions that were obtained from the 2014 Congress on Evolutionary Computation (CEC’14). The outcomes showed that the CNSSA solves benchmark test functions more effectively and with better results. Furthermore, clustering problems are also solved using the CNSSA. When compared to other algorithms namely the original SSA, GWO, PSO, TLBO, JAYA, ACO, BA, and GSA then it is discovered that the proposed CNSSA produces superior clustering results."
- poster: 139142
  presenter: "Silva Perez, Kevin"
  email: "81fe4b44932a39d"
  title: "Difussion over Ramified Domains: Solvability and Fine Regularity"
  abstract: |-
    We consider a domain $\Omega\subseteq\mathbb{R\!}^{\,2}$ with branched
    fractal boundary $\Gamma^{\infty}$ and parameter $\tau\in[1/2,\tau^{\ast}]$ introduced
    by Achdou and Tchou, for $\tau^{\ast}\simeq 0.593465$, which acts as an idealization
    of the bronchial trees in the lungs systems. For each $\tau\in[1/2,\tau^{\ast}]$,
    the corresponding region $\Omega$ is a non-Lipschitz domain, which attains its
    roughest structure at the critical value $\tau=\tau^{\ast}$ in such way that in
    this endpoint parameter the region $\Omega$ fails to be an extension domain, and
    its ramified boundary $\Gamma^{\infty}$ is not post-critically finite. Then, we
    investigate a model equation related to the diffusion of oxygen through the bronchial
    trees by considering the realization of a generalized diffusion equation $$\frac{\partial
    u}{\partial t}-\mathcal{A} u+\mathcal{B}u\,=\,f(t,x)\,\,\,\,\,\,\,\,\textrm{in}\,\,\,(0,\infty)\times\Omega$$
    with inhomogeneous mixed-type boundary conditions $$\displaystyle\frac{\partial
    u}{\partial\nu_{_{\mathcal{A}}}}+\beta u\,=\,g(x,t)\,\,\,\,\,\,\,\textrm{on}\,\,(0,\infty)\times\Gamma^{\infty},\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,u=0\,\,\,\,\,\,\,\textrm{in}\,\,\,(0,\infty)\times(\partial\Omega\setminus\Gamma^{\infty}),$$
    and $u(x,0)=u_0\in C(\overline{\Omega})$. These domains are projections of bronchial
    trees in 2D. Calculating their measure involves an optimization problem that requires
    computational calculations and geometric concepts.
- poster: 140143
  presenter: "Simpson, Gideon"
  email: "ff39f427a5775b15"
  title: "MS21: Optimal Allocation in Splitting Methods for Variance Reduction"
  abstract: |-
    Splitting and killing schemes (often referred to as just “splitting") are among the oldest methods for enhanced Monte Carlo sampling. In splitting and killing, we simulate many realizations of a Markov chain, and we introduce interactions between the Markov chains that encourage the sampling of targeted events of interest. This poster reviews recent and ongoing work on a particular method, Weighted Ensemble, and reflects work in collaboration with D. Aristoff, J.Copperman, R.J. Webber, and D.M. Zuckerman.
- poster: 140088
  presenter: "Simsek, Berfin"
  email: "2dbe9fcb33de8a1f"
  title: "MS89: Learning Associative Memories with Gradient Descent"
  abstract: "This work focuses on the training dynamics of one associative memory module storing outer products of token embeddings.  We reduce this problem to the study of a system of particles, which interact according to properties of the data distribution and correlations between embeddings.  Through theory and experiments, we provide several insights.  In overparameterized regimes, we obtain logarithmic growth of the “classification margins.'  Yet, we show that imbalance in token frequencies and memory interferences due to correlated embeddings lead to oscillatory transitory regimes.  The oscillations are more pronounced with large step sizes, which can create benign loss spikes, although these learning rates speed up the dynamics and accelerate the asymptotic convergence.  In underparameterized regimes, we illustrate how the cross-entropy loss can lead to suboptimal memorization schemes.   Finally, we assess the validity of our findings on small Transformer models. Associated minisymposium: MS0 Mathematical Principles in Foundation Models"
- poster: 140092
  presenter: "Sing Long, Carlos"
  email: "9e7f808d58e76464"
  title: |-
    MS52: Characterizing Reducibility by Solving High-Dimensional Partial Differential
    Equations with Neural Networks
  abstract: |-
    Dynamical systems are widely used in science and engineering. However,
    these systems often involve thousands of interacting components and exhibit complex
    dynamics. This either makes their simulation over large time intervals challenging,
    or their use as a part of a complex computational model, such as in control and
    optimization, computationally demanding. In these settings, model reduction is
    desirable. However, it is not always clear whether this is possible: the system
    may not be reducible or may be reducible only to a degree, the selected reduced
    models may not extrapolate beyond the time interval used for training, or they
    may be too sensitive to initial conditions. In this work, we introduce a partial
    differential equation whose solution, which we call the latent flow, encodes all
    possible reduced models along with their dependence on the initial conditions
    to the system. As this PDE is high-dimensional, we use deep neural networks to
    approximate its solution. We validate our results for low-dimensional systems
    using standard finite-element methods, and we show numerical results for high-dimensional
    chemical reaction networks and some evolution equations.
- poster: 139655
  presenter: "Singh, Himanshu"
  email: "ac57b7970427c557"
  title: |-
    Data-Driven Modelling with Limited Data Via Laplacian Kernelized Dynamic
    Mode Decomposition and Gaussian Random Vectors
  abstract: |-
    Prevalent methods of reduced order modelling, primarily Dynamic Mode Decomposition
    (DMD) encounters pitfalls when limited data is acquired. Hence, it's been a key
    challenge faced by numerous practitioners in the wide variety of field such as
    fluid dynamics, epidemiology, neuroscience and to name a few. In the interest
    of overcoming this very challenge, we provide an exciting algorithm of executing
    Kernelized Extended DMD (KeDMD) which employs the Gaussian random vectors to recover
    the dominant Koopman modes for the standard fluid flow across cylinder experiment.
    The theoretical perspective of this algorithm raises two critical issues. These
    issues are observable replacement by Gaussian probability density function and
    data vector replacement by Gaussian random vectors. In this talk, we study the
    KeDMD algorithm powered by Gaussian Random Vectors coupled with the Laplacian
    Kernel Function which is a common kernel choice in machine learning architectures
    of speech and text recognition. We also settle the much-needed theory of the aforementioned
    critical issues. Further, we study the reproducing kernel Hilbert space (RKHS)
    of the Laplacian Kernel and its interface on the Koopman operator. This deep mathematical
    analysis here demonstrates that the Koopman operator is closable on the RKHS of
    the Laplacian Kernel Function unlike that of Gaussian Radial Basis Function Kernel
    which leverages in making it a unique and novel choice of kernel function for
    such experimental situation.
- poster: 140034
  presenter: "Singh, Rahul"
  email: "a956f1fd597b7c19"
  title: "MS29: Slepians on Directed Signed Graphs"
  abstract: |-
    Graph slepian is an important concept for analyzing graph signals on a
    subgraph of the given graph. Graph slepians are the signals that are maximally
    concentrated within a subset of nodes while at the same time are bandlimited in
    the graph frequency domain. However, the existing slepian designs on graphs are
    restricted to only undirected and unsigned graphs with non-negative edges. Negative
    edges frequently serve as a valuable tool for representing dissimilarity relations.
    For example, in social networks, users may share or oppose political perspectives,
    trust or distrust each other's recommendations, or have positive or negative feelings
    toward one another. These dissimilarities can be effectively depicted using signed
    graphs, where edges can take both positive and negative values. In this work,
    we extend the graph slepians to (directed) signed graphs. Our proposed slepian
    construction is based on recently introduced signed magnetic Laplacian. We further
    study the proposed graph slepian properties for balanced and unbalanced signed
    graphs.
- poster: 139769
  presenter: "Singh, Subham"
  email: "2dfceafdc6db3a27"
  title: |-
    MS47: Understanding the Influence of Genre-Specific Music Using Network
    Analysis and Machine Learning Algorithms
  abstract: "This study analyzes a network of musical influence using machine learning and network analysis techniques. A directed network model is used to represent the influence relations between artists as nodes and edges. Network properties and centrality measures are analyzed to identify influential patterns. In addition, influence within and outside the genre is quantified using in-genre and out-genre weights. Regression analysis is performed to determine the impact of musical attributes on influence. We find that speechiness, acousticness, and valence are the top features of the most influential artists. We also introduce the IRDI, an algorithm that provides an innovative approach to quantify an artist’s influence by capturing the degree of dominance among their followers. This approach underscores influential artists who drive the evolution of music, setting trends and significantly inspiring a new generation of artists. The independent cascade model is further employed to open up the temporal dynamics of influence propagation across the entire musical network, highlighting how initial seeds of influence can contagiously spread through the network. This multidisciplinary approach provides a nuanced understanding of musical influence that refines existing methods and sheds light on influential trends and dynamics."
- poster: 140401
  presenter: "Singh, Abhijeet"
  email: "dfa5dc6e6a45820b"
  title: |-
    Use of Data Science in the Field of Energy and Seismic Data Evaluation (Smoothing
    of Data and Automated Fault Detection Using Ml)
  abstract: |-
    Our proposal focuses on the novel integration of machine learning (ML)
    techniques in energy systems, specifically addressing the interpretation of complex
    energy datasets and optimization of energy management strategies. While ML applications
    in energy have been explored in the past, our session aims to offer unique insights
    by delving into advanced ML algorithms and their practical implementation in real-world
    scenarios. While there may be existing tutorials or sessions covering ML in energy
    systems, our proposal distinguishes itself through its emphasis on cutting-edge
    techniques, such as deep learning and reinforcement learning, and their application
    in addressing current challenges in energy data interpretation and management.
    We do not have specific tutorials or materials available online at this time.
    However, our team is actively engaged in ongoing research and development in the
    field of ML for energy systems, and we are committed to delivering a session that
    offers fresh perspectives and actionable insights for attendees. If desired, we
    can provide supplementary materials or references to relevant research papers,
    but we believe that the live presentation format will allow for a dynamic discussion
    and exchange of ideas that cannot be fully captured in pre-recorded videos or
    GitHub repositories.
- poster: 139614
  presenter: "Singh, Aniket"
  email: "bfbcc7ef3b39b256"
  title: |-
    MS74: The Nuances of Large-Language-Model-Agent Performance in Simple English
    Auction
  abstract: |-
    This research investigates the use of Large Language Models (LLMs) in
    ascending English auctions with hidden independent valuations, focusing on dominant
    strategy adherence through various prompting techniques and model versions. While
    GPT-4, utilized with intricate prompts, demonstrated consistent adherence to bidding
    strategically, findings accentuated that both model selection and prompt type
    critically influence LLM agent performance. Older LLMs or the use of simplistic
    prompts led to suboptimal auction behavior. The study highlights practical constraints
    in LLM-auction experiments, notably model limitations, costs, and researcher bias,
    asserting the need for careful model and prompt selection in experimental design
- poster: 139663
  presenter: "Slote, Kevin"
  email: "865ca0b88d25bb03"
  title: |-
    The Twitter Effect: How Anti-Regulation Organizations Drive Firearm Acquisitions
    in the United States
  abstract: |-
    This study seeks to address the causal role in the Weiner-Granger sense
    of social media and media as drivers of firearm aquisition in the United States.
    Firearm injuries are a leading cause of death in the United States, where a greater
    number of people die by firearms than in motor vehicle crashes. Although firearms
    present a significant public health risk, Americans continue to purchase them
    in large amounts. Three drivers of firearm acquisition are commonly cited in literature,
    namely fear of violent crime, fear of mass shootings, and panic-buying. These
    drivers have been shown to have causal links on a weekly scale. In addition, the
    activity of relevant interest groups on social media, which capitalize on emotions
    such as fear, may impact impulsive firearm acquisition in the short time scales
    of days. We examine the combined role of all factors in driving firearm purchases
    has never been tested in the causal framework, PCMCI.
- poster: 140043
  presenter: "Smith, Chandler"
  email: "91b750ce38c60db8"
  title: "MS75: A Riemannian Distance Geometry Algorithm with Adaptive Sampling"
  abstract: |-
    The Euclidean distance geometry (EDG) problem is an important computational
    task that appears in many applications, ranging from molecular conformation in
    computational chemistry to dimensionality reduction in machine learning and sensor
    network localization. Using the partial pairwise distance information of a point
    set as an input, EDG aims to obtain the configuration of the point set. When partial
    distances are all that are available, previous work shows that a Riemannian matrix
    completion approach adapted to the specifics of the EDG problem provides local
    convergence guarantees along with strong numerical results. However, this approach
    relies on uniform sampling of the distance matrix, which may not always be practical
    in certain applications. In this work, we seek to extend this method beyond the
    uniform sampling models and provide  adaptive sampling schemes that improve the
    robustness of this algorithm.
- poster: 139766
  presenter: "Solomon, Jack"
  email: "ce082cc43ab85dbe"
  title: "MS90: Sparsity Promoting Eeg Source Localization with Time and Spatial Regularization"
  abstract: |-
    Source localization is crucial for interpreting electroencephalograms
    and requires solving large and extremely ill-posed inverse problems.  As standard
    approaches are intractable for such large dynamic datasets at scale, we propose
    a source localization procedure utilizing sparsity promoting spatial and temporal
    regularization. We propose using computationally efficient variable projected
    augmented Lagrangian methods for its solution. In a simulation study, we confirm
    that our approach can successfully locate regions of brain activity for inverse
    problems with 11,500 data points but 3,719,100 unknowns.
- poster: 140102
  presenter: "Sonthalia, Rishi"
  email: "9d35a6c5753333ba"
  title: |-
    Under-Parameterized Double Descent for Ridge Regularized Least Squares Denoising
    of Data on a Line
  abstract: |-
    The relationship between the number of training data points, the number
    of parameters, and the generalization capabilities has been widely studied. Previous
    work has shown that double descent can occur in the over-parameterized regime,
    and believe that the standard bias-variance trade-off holds in the under-parameterized
    regime. These works provide multiple reasons for the existence of the peak. We
    present a simple example, ridge regularized least squares denoising with data
    on a line embedded in high-dimension space, that provably exhibits double descent
    in the under-parameterized regime and does not seem to occur for reasons provided
    in prior work. By deriving an asymptotically accurate formula for the generalization
    error, we observe sample-wise and parameter-wise double descent with the peak
    in the under-parameterized regime rather than at the interpolation point or in
    the over-parameterized regime. The location of the peak depends on the strength
    $\mu$ of the ridge regularizer.
- poster: 139994
  presenter: "Squicciarini, Antonio"
  email: "38dec470c663d009"
  title: |-
    Entropic Information Functionals Combined with Neural Networks Explainability
    to Detect Events in Signals
  abstract: "In this study, we employ a novel methodology combining entropic and informational functionals with neural networks to autonomously detect events in signals. Our feature extraction process involves dividing the signals into non-overlapping windows and employing non-parametric kernel density estimation. This transforms the signals into an ordered sequence of probability density functions, which is the cornerstone of our approach.  By applying Shannon, generalized Rényi and Tsallis entropies, along with complementary measures such as non-parametric Fisher information, our methodology aims to capture the dynamic characteristics of the signals. Moreover, we employ eXplainable AI (XAI) techniques to understand the role of each measure in determining the events' offset.  As a case study, we consider epileptic seizure detection in scalp EEG recordings. Epilepsy is a neurological disorder characterised by recurring, unprovoked seizures. Detection of epilepsy is crucial for accurate diagnosis. Traditional approaches typically rely on describing the frequency of seizure occurrences. Our experiments show a significant dimensionality reduction compared to the raw signals, thus aiding in the accurate classification of epileptic seizures. Additionally, the application of XAI methods shows how the classifier assigns importance to specific measures and channels, offering insights not only into the classifier’s decision-making process but also into the underlying dynamics driving a seizure."
- poster: 140076
  presenter: "Sristi, Ram Dyuthi"
  email: "72db61a7e1c2944a"
  title: "Contextual Feature Selection with Conditional Stochastic Gates"
  abstract: "Feature selection is a crucial tool in machine learning and is widely applied across various scientific disciplines. Traditional supervised methods generally identify a universal set of informative features for the entire population of samples. However, feature relevance often varies with context, where the context itself may not directly affect the outcome variable. Here, we propose a novel end-to-end architecture for prediction with contextual feature selection where the subset of selected features is conditioned on the value of the sample’s context variables. Our new approach, Conditional Stochastic Gates (c-STG), models the importance of features using conditional Bernoulli variables whose parameters are estimated based on contextual variables using a hypernetwork. Our architecture is composed of the hypernetwork to learn the context-dependent gates followed by a prediction model, which can be linear or nonlinear. We further present a theoretical analysis of our model, indicating that it can improve performance and flexibility over population-level methods in complex feature selection settings. Finally, we conduct an extensive benchmark using simulated and real-world datasets across multiple domains, including health and neuroscience, demonstrating that c-STG leads to improved feature selection capabilities while enhancing prediction accuracy and interpretability."
- poster: 139806
  presenter: "Starnes, Andrew"
  email: "94bff02640599cc0"
  title: "MS48: Distributional Smoothing for Machine Learning"
  abstract: |-
    Distribution smoothing is a method for modifying a target function in
    an optimization problem using a set of surrogate functions that are easier to
    optimize. When applied to (stochastic) gradient descent, we substitute the gradient
    of the target function with the gradient from this more favorable class of functions.
    This poster will have an exploration of the intuition and motivation behind distributional
    smoothing. It will also delve into the convergence properties of Smooth Gradient
    Descent, labeled SmoothGD, and its stochastic counterpart, SmoothSGD, demonstrating
    that their convergence rates are no worse than those of their non-smoothed counterparts.
    Finally, it will provide numerical evidence from both machine learning and data
    science problems.
- poster: 140192
  presenter: "Stepaniants, George"
  email: "a44e78c40f67bb90"
  title: |-
    MS71: Discovering Nonlinear Oscillatory and Chaotic Systems from Partial
    Observations
  abstract: "Complex multi-component systems from cells and tissues to biochemical reactors often exhibit oscillatory and chaotic nonlinear dynamics that are essential to their signaling properties and functions. Despite the rapid advancement of sensor and imaging technology, many physical and biological systems can only be partially observed with practitioners in need of model-fitting tools that can account for this missing information. Here we develop an automated inference method that discovers predictive differential equation models from a few noisy partial observations of a system’s state. We illustrate our method on a combination of both simulation and experimental data from a variety of physical, chemical and biological systems showing that in many cases noisy partial observations are sufficient to infer predictive multivariate dynamical systems."
- poster: 139728
  presenter: "Stillman, Michael"
  email: "76139f7908b2c135"
  title: "Learning Selection Strategies in Buchberger's Algorithm"
  abstract: |-
    Studying the set of exact solutions of a system of polynomial equations
    largely depends on a single iterative algorithm, known as Buchberger's algorithm.
    Optimized versions of this algorithm are crucial for many computer algebra systems
    (e.g., Mathematica, Maple, Sage). We introduce a new approach to Buchberger's
    algorithm that uses reinforcement learning agents to perform S-pair selection,
    a key step in the algorithm. We then study how the diffculty of the problem depends
    on the choices of domain and distribution of polynomials, about which little is
    known. Finally, we train a policy model using proximal policy optimization (PPO)
    to learn S-pair selection strategies for random systems of binomial equations.
    In certain domains, the trained model outperforms state-of-the-art selection heuristics
    in total number of polynomial additions performed, which provides a proof-of-concept
    that recent developments in machine learning have the potential to improve performance
    of algorithms in symbolic computation.
- poster: 139921
  presenter: "Stöger, Dominik"
  email: "1c4d8d4f14ecf4b4"
  title: "Breaking the Quadratic Rank Barrier in Non-Convex Matrix Sensing"
  abstract: |-
    Low-rank matrix recovery problems are ubiquitous in many areas of science
    and engineering. Most of the methods that have been studied for these problems
    can be divided into two categories: Convex optimization approaches based on nuclear
    norm minimization, and non-convex approaches that use factorized gradient descent.
    While the latter promises to be computationally much less expensive, basically
    all existing recovery guarantees for factorized gradient descent are much more
    pessimistic with respect to the number of samples required. In particular, they
    require the number of samples to scale quadratically with the rank of the ground
    truth matrix. This is in stark contrast to empirical observations which suggest
    that the non-convex approaches perform as well as the convex ones with respect
    to the sample complexity. In this talk, we resolve this issue and we present the
    first theoretical guarantees to the best of our knowledges for matrix sensing
    that show that factorized gradient descent recovers the ground truth matrix with
    a sample size that is optimal in the number of degrees of freedom. Our proof is
    based on new probabilistic decoupling arguments, which we expect to be of independent
    interest.
- poster: 140283
  presenter: "Stone, Branden"
  email: "d878c8c60efb3144"
  title: "Binary Malware Attribution Using Llm Embeddings and Topological Descriptors"
  abstract: |-
    In malware authorship attribution, there's a scarcity of easily-obtained,
    expressive, stylistically salient features. One has nothing to go off of except
    the binary malware executable; this has traditionally constrained static analysis
    to producing simple statistics on the decompiled binary. Dynamic analysis can
    obtain more descriptive features by running the malware executable in a sandbox,
    but this is both resource-intensive and has obvious risks. In this talk, we introduce
    two new sources of expressive static features which, when combined, achieve classification
    accuracies competitive with leading dynamic analysis models. Our features stem
    from a hypothesis that coding style is found at or around the level of the function.
    To capture features, we leverage Large Language Model (LLM) embeddings, persistent
    homology, and graph neural networks (GNNs) to identify stylistically-salient geometric
    features of the program architecture. We demonstrate that this new paradigm of
    graph learning combined with LLM code-embeddings can easily be extended to encompass
    and enhance existing featurizations.
- poster: 140140
  presenter: "Ström, Emanuel"
  email: "c788db5c322b497c"
  title: "MS80: Phase Recovery with Moment Constraints for Multireference Alignment"
  abstract: "Multireference alignment (MRA) is a computational model used to recover a signal that has undergone random circular translations and subjected to noise. This problem is commonly addressed through two types of methods. Techniques such as template alignment focus on estimating the unknown shifts of observed signals, allowing for the reconstruction of the original signal while accounting for these translations. Alternatively, methods such as expectation–maximization (EM) and the method of moments estimate the signal without explicitly determining the unknown shifts. In our approach, we define a loss function by averaging the distance of a template to the aligned signals. Crucially, the loss function is constrained to templates whose power spectrum matches an empirical estimate. We then use a gradient-based approach to estimate the true signal. Our method offers increased efficiency compared to EM and demonstrates improved accuracy over method of moments applied to the bispectrum."
- poster: 139012
  presenter: "Subercaseaux, Bernardo"
  email: "a38b4346ff3a3398"
  title: |-
    MS4: The Easiest Case Is Already Hard: the Complexity of Understanding Decision
    Trees As Ml Models
  abstract: "Decision trees are traditionally considered one of the most interpretable machine learning models. When looking at decision trees with a couple dozen nodes it is indeed easy to understand what’s going on, and why a specific input instance was classified in a certain way. But what about industry size trees having 2000 nodes? It turns out that several natural questions we would hope to be able to answer about a model’s decision making, like the smallest subset of the input that was actually relevant for a particular decision, are NP-hard over decision trees. Moreover, their probabilistic relaxations turn out to be hard as well, and worse yet, they are hard to approximate. In summary, decisions made by decision trees are not always easy to understand."
- poster: 139746
  presenter: "Sun, Jingmin"
  email: "6b29de2d334ff42a"
  title: |-
    MS31: A Foundation Model for Partial Differential Equations: Multi-Operator
    Generalization and Extrapolation
  abstract: |-
    Foundation models, such as large language models, have demonstrated success
    in addressing various language and image processing tasks. In this work, we introduce
    a multi-modal foundation model for scientific problems, named PROSE-PDE. Our model,
    designed for bi-modality to bi-modality learning, is a multi-operator learning
    approach which can predict future states of spatiotemporal systems while concurrently
    learning the underlying governing equations of the physical system.   Specifically,  we
    focus on multi-operator learning by training distinct one-dimensional time-dependent
    nonlinear constant coefficient partial differential equations, with potential
    applications to many physical applications including physics, geology, and biology.   More
    importantly, we provide three extrapolation studies to demonstrate that PROSE-PDE
    can generalize physical features through the robust training of multiple operators
    and that the proposed model can extrapolate to predict PDE solutions whose models
    or data were unseen during the training. Furthermore, we show through systematic
    numerical experiments that the utilization of the symbolic modality in our model
    effectively resolves the well-posedness problems with training multiple operators
    and thus enhances our model's predictive capabilities.
- poster: 139701
  presenter: "Sun, Zeyu"
  email: "3b6d8b36aa387862"
  title: "Minimum-Risk Recalibration of Classifiers"
  abstract: |-
    Recalibrating probabilistic classifiers is vital for enhancing the reliability
    and accuracy of predictive models. Despite the development of numerous recalibration
    algorithms, there is still a lack of a comprehensive theory that integrates calibration
    and sharpness (which is essential for maintaining predictive power). In this paper,
    we introduce the concept of minimum-risk recalibration within the framework of
    mean-squared-error (MSE) decomposition, offering a principled approach for evaluating
    and recalibrating probabilistic classifiers. Using this framework, we analyze
    the uniform-mass binning (UMB) recalibration method and establish a finite-sample
    risk upper bound of order $\tilde{O}(B/n + 1/B^2)$ where $B$ is the number of
    bins and $n$ is the sample size. By balancing calibration and sharpness, we further
    determine that the optimal number of bins for UMB scales with $n^{1/3}$, resulting
    in a risk bound of approximately $O(n^{-2/3})$. Additionally, we tackle the challenge
    of label shift by proposing a two-stage approach that adjusts the recalibration
    function using limited labeled data from the target domain. Our results show that
    transferring a calibrated classifier requires significantly fewer target samples
    compared to recalibrating from scratch. We validate our theoretical findings through
    numerical simulations, which confirm the tightness of the proposed bounds, the
    optimal number of bins, and the effectiveness of label shift adaptation.
- poster: 139622
  presenter: "Sun, Bonan"
  email: "37fb30fa72d2de79"
  title: "Approximation to Multivariate Functions in the Extended Tensor Train Format"
  abstract: |-
    This talk introduces the extended functional tensor train (EFTT) format
    for compressing and working with multivariate functions on tensor product domains.
    Our compression algorithm combines tensorized Chebyshev interpolation with a low-rank
    approximation algorithm that is entirely based on function evaluations. Compared
    to existing methods based on the functional tensor train format, our approach
    often reduces the required storage, sometimes considerably, while achieving the
    same accuracy. In particular, we reduce the number of function evaluations required
    to achieve a prescribed accuracy by up to over 96% compared to the algorithm from
    [Gorodetsky, Karaman and Marzouk, Comput. Methods Appl. Mech. Eng., 347 (2019)].
- poster: 139651
  presenter: "Suryanarayanan, Shambhavi"
  email: "acc664101ea61889"
  title: |-
    MS13: Block Gauss-Siedel Methods for Solving Tensor Regression under the
    t-Product:
  abstract: |-
    With tensor data becoming more ubiquitous, there is an increasing need
    for effective randomized algorithms to solve tensor regression problems. While
    there is a rich body of work exploring row-based Kaczmarz-type methods to tackle
    this, the literature on column-action methods is limited. In this work, we propose
    extensions of the Gauss-Siedel method and its variants to the tensor regression
    regime under the t-product. In this poster, I will present some theoretical convergence
    guarantees followed by empirical results on synthetic data that highlight the
    effectiveness of these methods in different settings. Further, the application
    of these methods to image deblurring will also be discussed.
- poster: 140163
  presenter: "Suryanarayanan, Shambhavi "
  email: "7a90de421b1fdef4"
  title: |-
    MS67: Kaczmarz Based Iterative Hard Thresholding Techniques for Low-Rank
    Tensor Recovery
  abstract: |-
    Hybrid methods that leverage the Kaczmarz and Iterative Hard Thresholding
    (IHT) algorithms have been shown to be particularly effective in tackling sparse
    vector recovery problems. In this poster, I will discuss our recent work, in which
    we developed Kaczmarz based IHT techniques to recover low-rank tensors from a
    few linear measurements. I will also present some theoretical convergence guarantees
    followed by empirical results that highlight the effectiveness of these methods
    against different classes of measurement operators. Associated poster with minsymposium
    on Structure in Data: Theory, Learning, and Algorithms
- poster: 139814
  presenter: "Suwayyid, Faisal"
  email: "89bea5442949403d"
  title: "MS38: Persistent Path Dirac"
  abstract: |-
    Significant progress has been made in advancing the theoretical underpinnings
    of topological data analysis (TDA) by conceptualizing and refining the Dirac operator,
    a fundamental mathematical construct used in analyzing topological signals and
    molecular representations. However, the current methodologies mostly rely on traditional
    frameworks such as Vietoris-Rips complexes and alpha complexes. This academic
    pursuit aims to introduce a novel methodological approach by integrating Dirac
    operators with path homology within the framework of TDA, with a specific emphasis
    on elucidating molecular structures. Path homology is a theoretical construct
    that extends the domain of graph homology to accommodate directed graphs and hypergraphs.
    Thus, it provides a more comprehensive analytical toolkit conducive to interrogating
    structures endowed with directional attributes. The proposed methodological framework
    involves extracting features from such complex structures, which are examined
    by various exemplary instances. This work seeks to contribute to the ongoing discourse
    within TDA by advancing methodologies that enhance our understanding and analytical
    capabilities concerning intricate molecular architectures. Thus, it aims to foster
    broader applicability and robustness in data-driven investigations within computational
    biology and related disciplines.
- poster: 139768
  presenter: "Sweeney, Mikhail"
  email: "595fdd36f452e53d"
  title: "MS95: Moment Invariant Distribution Learning"
  abstract: |-
    We consider the scenario where data is collected from $M$ sub-populations
    to produce $M$ data batches, but the sample sizes $\{n_i\}_{i=1}^M$ of the batches
    are small, i.e. $n_i\ll M$.  We assume that local environmental factors affect
    only the first and second moments of the sub-populations, so that the $i^{\text{th}}$
    batch consists of $n_i$ independent, identically distributed observations from
    $f_{\sigma_i,\mu_i}(x) = \frac{1}{\sigma_i}f\left(\frac{x-\mu_i}{\sigma_i}\right)$
    for some universal distribution $f$. Our goal is to reliably recover the underlying
    distribution $f$ by aggregating the sparse data. This is a highly relevant problem,
    as it allows for reliable nonparametric estimates of the density $f$ in settings
    where traditional approaches fail, and more broadly leads to more precise comparison
    across sub-populations in settings where little data is available. We approach
    this problem with tools from signal processing for solving multi-reference alignment
    models with dilation, including Fouier moments and unbiasing procedures.
- poster: 139700
  presenter: "Tasissa, Abiy"
  email: "472d0df6a2f96e41"
  title: "MS86: Modified Nystrom Approximation Via Low-Rank Matrix Sensing"
  abstract: |-
    We study a modified version of the Nystrom problem in which the $(1,2)$
    block of the matrix is incomplete. We demonstrate how this modified version is
    equivalent to a low-rank matrix sensing problem.
- poster: 140545
  presenter: "Teneggi, Jacopo"
  email: "d7e6b8463eb1933a"
  title: "MS15: Formal Tests for Semantic Interpretability"
  abstract: |-
    Recent works have extended notions of feature importance to semantic concepts
    that  are inherently interpretable to the users interacting with a black-box predictive
    model. Yet,  precise statistical guarantees, such as false positive rate control,
    are needed to communicate  findings transparently and to avoid unintended consequences
    in real-world scenarios. In this  paper, we formalize the global (i.e., over a
    population) and local (i.e., for a sample) statistical  importance of semantic
    concepts for the predictions of opaque models, by means of conditional  independence,
    which allows for rigorous testing. We use recent ideas of sequential kernelized  testing
    (SKIT) to induce a rank of importance across concepts, and showcase the effectiveness  and
    flexibility of our framework on synthetic datasets as well as on image classification
    tasks  using vision-language models such as CLIP.
- poster: 139520
  presenter: "Terasaki, Gbocho Masato"
  email: "1f027c67b3fe6b4"
  title: |-
    Ecg Classification: Identifying Abnormalies by Leveraging Data for Diagnostic
    Accuracy
  abstract: |-
    The electrocardiogram (ECG) is a valuable tool for diagnosing heart conditions.
    However, the standard 12-lead ECG doesn't provide enough detail for certain clinical
    applications, such as locating the source of an arrhythmia. Current techniques
    require multiple ECG readings and expensive medical imaging to create detailed
    anatomical models. We propose an alternative approach that combines the 12-lead
    ECG with advanced machine learning to first identify different heart conditions,
    with the ultimate goal of predicting the spatiotemporal activation map of the
    human heart from the 12-lead ECG data.
- poster: 139786
  presenter: "Terenin, Alexander"
  email: "a34e351b47f6aef"
  title: "MS64: Lookahead-Free Bayesian Optimization Via Gittins Indices"
  abstract: |-
    Bayesian optimization is a technique for efficiently optimizing unknown
    functions in a black-box manner. To ensure their decisions take into account more
    than just the short-term potential benefit of the current query, it is desirable
    for Bayesian optimization policies to be non-myopic. However, current non-myopic
    approaches rely on lookahead techniques, which are complex and computationally
    intensive. We introduce a new lookahead-free policy for Bayesian optimization,
    using an unexplored connection with the Pandora's box problem, a well-studied
    problem in the economics literature. The Pandora's box problem admits a Bayesian-optimal
    solution based on an acquisition function called the Gittins index. We demonstrate
    empirically that the same acquisition function performs well for Bayesian optimization,
    particularly in higher dimensions. The acquisition function is computationally
    efficient, and it naturally handles problems with heterogeneous query costs. Our
    work constitutes a first step towards integrating ideas from Gittins index theory
    into policies for Bayesian optimization.
- poster: 140235
  presenter: "Terrefortes-Rosado, Michael"
  email: "327e453ae65c9f64"
  title: "ViBEx: A Visualization Tool for Gene Expression Analysis"
  abstract: |-
    Variations in the states in the Gene Regulatory Network may have an impact
    on disease outcomes and drug development. Boolean Networks aid in conceptualizing
    and understanding complex relationships between genes. Threshold computation methods
    are used for the binarization of gene expression and the Boolean representation
    of their Gene Regulatory Network. Because the gene expression binarization may
    vary based on the threshold computation method, the resulting Boolean representation
    of the network may also be different based on the binarization method. Lluberes
    and Seguel proposed a framework for the analysis of gene expression when the resulting
    binarized values differ among threshold methods. We have created a visualization
    tool for this framework. In the proposed interface, the user can upload their
    gene expression dataset and interact with a dashboard to explore the binarization
    of each gene expression. They can compare different threshold methods of binarization.
    Also, by implementing a majority vote framework the tool shows visualization of
    expressed, unexpressed, and undecided states in the gene expression binarization.
    Furthermore, this tool provides statistical analysis of the binarization of gene
    expressions.
- poster: 139169
  presenter: "Tian, Moyi"
  email: "80386ea02f801c8e"
  title: "MS14: Efficient Learning of Models for Dynamic Networks"
  abstract: |-
    Graphs are widely used in social, physical, and biological sciences to
    represent populations of entities interconnected by relations. Exponential Random
    Graph Models (ERGMs) have been a popular statistical model for analyzing network
    attributes and facilitating inference from data. As an extension of ERGMs, Temporal
    Exponential Random Graph Models (TERGMs) further incorporate the Markov property
    to capture the dynamic evolution of network topologies over time. Although this
    family of random graph models provides an efficient description of network distributions
    and has nice properties that can be helpful with empirical analysis, learning
    such models from data has been challenging. Motivated by recent advancements in
    learning of discrete graphical models, we introduce efficient learning algorithms
    for TERGMs which come with favorable sample complexity. This framework can be
    specialized to random temporal hypergraph models and models with tractable sampling
    properties. This poster is associated with the minisymposium entitled Probabilistic
    Methods in Machine Learning and Complex Systems.
- poster: 140174
  presenter: "Tian, Chenxiao"
  email: "42e10ba98b10f60a"
  title: "Exploring Complex Systems with Markov Bridge and Rwr Models"
  abstract: |-
    Understanding the structural and functional complexities of networked
    systems is pivotal for advancing both theoretical and applied network science.
    These complex systems, whether they arise in nature, society, or engineered settings,
    often consist of numerous interacting components that follow nonlinear dynamics.
    This work employs stochastic processes, specifically Markov Bridge processes and
    Random Walk with Restart (RWR) models, to explore and reconstruct the topology
    of networked systems. Our research focuses on real-world transportation networks
    and offers theoretical insights and practical methodologies. Our first application
    utilizes GPS data from San Francisco taxis to construct a Markov Bridge model,
    enabling us to examine the stochastic nature of route choices influenced by vehicle
    occupancy. We introduce a novel entropy measure based on the Markov Bridge, demonstrating
    enhanced sensitivity to changes in network topology compared to traditional measures
    such as spectral entropy. In our second application, we implement an RWR model
    to reconstruct the topology of multi-city, multi-layer multiplex transportation
    networks. This approach allows us to uncover and quantify the varied topological
    and dynamic characteristics inherent in these complex network systems. Through
    these applications, our work contributes to the broader understanding of network
    dynamics, offering valuable insights and tools for the mathematical modeling of
    data-rich, interconnected systems.
- poster: 139845
  presenter: "Tian, Ye"
  email: "df58cb1d7507cfb1"
  title: |-
    MS86: Graph Clustering Techniques with Sparse Solution Methods and Application
    to Medical Image Classification
  abstract: |-
    Given medical image datasets, we are interested in classifying the images
    for potential application in disease diagnosis. There are many neural network
    models and topological machine learning algorithms to do the image classification.
    However, this problem can also be formulated as a graph clustering problem. The
    vertices of a graph are images and the vertices within the same cluster can be
    assumed to share similar features and properties, thus making the applications
    of graph clustering techniques very useful for image classification. We shall
    first use a box spline based wavelet-framelet method to clean the images and help
    building up the adjacency matrix for the given image data. Finding clusters is
    one of major research interests in graph analysis. Recent approach based on sparse
    solutions of linear systems for graph clustering finds clusters more efficiently
    than the traditional spectral clustering method. We thus propose to use the two
    newly developed graph clustering methods based on sparse solution methods for
    linear system for image classification. The performance of our graph clustering
    methods will be shown to be very effective to classify an image. Numerical results
    will be demonstrated in the end of the paper.
- poster: 139658
  presenter: "Tian, Ye"
  email: "345a9626436ebd33"
  title: "Neyman-Pearson Multi-Class Classification Via Cost-Sensitive Learning"
  abstract: |-
    Most existing classification methods aim to minimize the overall misclassification
    error rate. However, in applications such as loan default prediction, different
    types of errors can have different consequences. Two popular paradigms have been
    developed to address this asymmetry issue: the Neyman-Pearson (NP) paradigm and
    the cost-sensitive (CS) paradigm. Previous works on the NP paradigm have predominantly
    focused on the binary case, while the multi-class NP problem presents a significantly
    greater challenge due to its unknown feasibility. In this work, we tackle the
    multi-class NP problem by establishing a connection with the CS problem through
    strong duality and propose two algorithms. In the binary NP problem, the key requirement
    for desirable algorithms is termed NP oracle inequalities. We extend these NP
    oracle inequalities from the binary case to NP oracle properties in the multi-class
    scenario, demonstrating that our two algorithms satisfy NP oracle properties under
    certain conditions. Furthermore, we propose practical algorithms to verify the
    feasibility and strong duality of a multi-class NP problem, which can offer practitioners
    the landscape of a multi-class NP problem with various target error levels. Simulations
    and real data studies confirm the effectiveness of our algorithms. To our knowledge,
    this is the first study to address the multi-class NP problem with theoretical
    guarantees.
- poster: 139649
  presenter: "Tian, Fan"
  email: "bbd2d989394dd6b4"
  title: "MS13: Tensor Completion with BMD Factor Nuclear Norm Minimization"
  abstract: |-
    This work is concerned with the problem of recovering third-order tensor
    data from limited samples. A recently proposed tensor decomposition (BMD) method
    has been shown to efficiently compress third-order spatiotemporal data. Using
    the BMD, we formulate a slicewise nuclear norm penalized algorithm to recover
    a third-order tensor from limited observed samples. We develop an efficient alternating
    direction method of multipliers (ADMM) scheme to solve the resulting minimization
    problem. Experimental results on real data show our method to give reconstruction
    comparable to those of HaLRTC (Liu et al., 2012), a well-known tensor completion
    method, in about the same number of iterations. However, our method has the advantage
    of smaller subproblems and higher parallelizability per iteration.
- poster: 139650
  presenter: "Tokcan, Neriman"
  email: "80e943e0eb32b6ff"
  title: |-
    MS13: Zero-Inflated Poisson Tensor Factorization for Multimodal Genomics
    Data
  abstract: |-
    Tensor factorizations (TF) are crucial for efficiently representing and
    analyzing multidimensional data. However, classic TF methods encounter challenges
    when dealing with zero-inflated count data, where traditional maximum likelihood
    estimation based techniques struggle. Furthermore, the inherent stochastic nature
    of TFs introduces variability across repeated runs, posing difficulties in result
    interpretation and reproducibility. We present Zero Inflated Poisson Tensor Factorization
    (ZIPTF), an unsupervised approach tailored for high-dimensional count data with
    excess zeros. To address stochasticity, we propose Consensus Zero Inflated Poisson
    Tensor Factorization (C-ZIPTF), which combines ZIPTF with a consensus-based meta-analysis
    approach. Our evaluation on synthetic zero-inflated count data and both synthetic
    and real multimodal genomics data illustrates the superiority of ZIPTF over baseline
    matrix and tensor factorization methods in terms of reconstruction accuracy. Additionally,
    our approach consistently identifies known and biologically relevant gene expression
    programs in multimodal genomics data. This work showcases the potential of ZIPTF
    and C-ZIPTF to unlock insights from complex genomics data, paving the way for
    advancements in understanding biological systems.
- poster: 140295
  presenter: "Tong, Shanyin"
  email: "9c0d2d09a0f52dc7"
  title: "MS24: Large Deviation-Informed Sampling for Rare Events in High Dimensions"
  abstract: |-
    Rare and extreme events like hurricanes, energy grid blackouts, dam breaks,
    earthquakes, and pandemics are infrequent but have severe consequences. Because
    estimating the probability of such events can inform strategies that mitigate
    their effects, scientists must develop methods to study the distribution tail
    of these occurrences. However, calculating small probabilities is hard, particularly
    when involving complex dynamics and high-dimensional random variables. In this
    poster, I will discuss our proposed method for the accurate estimation of rare
    event or failure probabilities for expensive-to-evaluate numerical models in high
    dimensions. The proposed approach combines ideas from large deviation theory and
    adaptive importance sampling.  The importance sampler uses a cross-entropy method
    to find an optimal Gaussian biasing distribution, and reuses all samples made
    throughout the process for both, the target probability estimation and for updating
    the biasing distributions. Large deviation theory is used to find a good initial
    biasing distribution through the solution of an optimization problem. Additionally,
    it is used to identify a low-dimensional subspace that is most informative of
    the rare event probability. We compare the method with a state-of-the-art cross-entropy-based
    importance sampling scheme using examples including a tsunami problem.
- poster: 140519
  presenter: "Tran, Hoang"
  email: "b6b8ccdfeda0b360"
  title: "MS58: Score Estimation for Supervised Generative Modeling"
  abstract: |-
    We present a supervised learning framework of generative models based
    on score-based diffusion models. This technique uses a reverse-time diffusion
    process in the form of a stochastic differential equation (SDE) to transport the
    standard Gaussian distribution to a complex target data distribution, thus, the
    goal of generating new data boils down to solving reverse-time SDEs. A key challenge
    of score-based diffusion models is to estimate score functions, which store the
    information of the target distribution and involve high-dimensional integrals.
    In this work, we investigate multiple different techniques for efficient score
    estimation, including Monte Carlo, important sampling and Langevin dynamics. The
    efficiency of the approaches will be demonstrated via benchmark data sampling
    and uncertainty quantification problems.
- poster: 139568
  presenter: "Tran, Jonathan"
  email: "d6fd61ec02ca4875"
  title: |-
    Data-Driven Vehicle Design Optimization Through Aerodynamics Informed Dimensionality
    Reduction
  abstract: |-
    Current methods for optimizing vehicle aerodynamics involve examining
    a high-dimensional parameter space of different geometric configurations through
    costly computational studies and wind tunnel experiments. This naturally calls
    for data-driven approaches to generate lower-cost, efficient surrogate aerodynamic
    models to expedite the vehicle design cycle.  We analyze a dataset of industry
    automobile geometries with their associated aerodynamic performance obtained from
    large-eddy simulations.  A nonlinear autoencoder is utilized to identify a mapping
    between the coarsened vehicle representations and a low-dimensional latent space
    manifold description while a secondary decoder is simultaneously trained to estimate
    the aerodynamic drag from the latent coordinates.  With the current manifold learning
    procedure, we obtain a differential manifold for which the coordinates effectively
    describe our vehicle design space in relation to aerodynamic performance.  We
    perform aerodynamic design optimization by making use of the local gradients of
    drag with respect to the manifold coordinates.   We demonstrate that the aerodynamic
    trends for the decoded geometries produced from the optimization process show
    agreement with a validation LES analysis.  This procedure demonstrates the potential
    for data-driven methods to accelerate the analysis and optimization of design
    parameters for ground, air, and marine vehicles.
- poster: 140261
  presenter: "Tran, Tuyen"
  email: "4a79319116611cf0"
  title: "The Boosted Dc Algorithm for Clustering with Constraints"
  abstract: |-
    This talk aims to investigate the effectiveness of the recently proposed
    Boosted Difference of Convex functions Algorithm (BDCA) when applied to clustering
    and set clustering with constraints. We present the mathematical basis for the
    BDCA and the Difference of Convex functions Algorithm (DCA), along with a penalty
    method based on distance functions. Then we develop algorithms for solving these
    problems with publicly available implementations. We compare old examples and
    provide new experiments to test the algorithms. We find that the BDCA method converges
    in fewer iterations than the corresponding DCA-based method. In addition, BDCA
    yields faster CPU running-times in the tested problems.
- poster: 139082
  presenter: "Treister, Eran "
  email: "5b5e5aa0afab10bb"
  title: |-
    MS18: Multigrid-Augmented Deep Learning Preconditioners for the Helmholtz
    Equation Using Compact Implicit Layers
  abstract: |-
    We present a deep learning-based iterative approach to solve the discrete
    heterogeneous Helmholtz equation for high wavenumbers. Combining classical iterative
    multigrid solvers and convolutional neural networks (CNNs) via preconditioning,
    we obtain a learned neural solver that is faster and scales better than a standard
    multigrid solver. Our approach offers three main contributions over previous neural
    methods of this kind. First, we construct a multilevel U-Net-like encoder-solver
    CNN with an implicit layer on the coarsest grid of the U-Net, where convolution
    kernels are inverted. This alleviates the field of view problem in CNNs and allows
    better scalability. Second, we improve upon the previous CNN preconditioner in
    terms of the number of parameters, computation time, and convergence rates. Third,
    we propose a multiscale training approach that enables the network to scale to
    problems of previously unseen dimensions while still maintaining a reasonable
    training procedure. Our encoder-solver architecture can be used to generalize
    over different slowness models of various difficulties and is efficient at solving
    for many right-hand sides per slowness model. We demonstrate the benefits of our
    novel architecture with numerical experiments on a variety of heterogeneous two-dimensional
    problems at high wavenumbers.
- poster: 140030
  presenter: "Tumurbaatar, Altansuren"
  email: "c218c72c815998f9"
  title: |-
    MS34: Identifying Influential Events for Pressure Injury Development: Applying
    Graph Analysis on Electronic Health Records
  abstract: |-
    We delve into the challenge of identifying influential events that play
    crucial roles in pressure injury development. First, we construct temporal event
    graphs (networks) utilizing pressure injury patient data from MIMIC-III Electronic
    Health Records. As patient events, we include daily average of patient vitals/labs,
    and daily occurrence of interventions from MIMIC-Extract, a preprocessed dataset
    extracted from MIMIC-III. Second, we employ inverse percolation centrality, a
    centrality measure that evaluates the relative importance of patient events within
    a network by considering both their topological connectivity and their involvement
    in pressure injury development dynamics. Third, we utilize a control group of
    non-pressure injury patients to discard common influential events shared by both
    groups. For the control group, we match pressure injury patients with non-pressure
    injury patients using propensity score matching based on patient demographic information.
    Finally, we present our findings regarding pressure injury development, identifying
    influential events and pathways of pressure injury.
- poster: 140082
  presenter: "Turnage, John"
  email: "57044eb1c2a33e0d"
  title: "An Optimal Weighted Least Squares Framework for Operator Learning"
  abstract: |-
    We consider the problem of learning an operator $K:L^2_\nu \to L^2_\eta$
    from a finite set of $M$ observations $g^i=K(f^i)$, where the $f^i$ are independent
    samples drawn from a tailored probability measure $\mu$ on $L^2_\nu$. For a fixed
    probability measure $\rho$ on $L^2_\nu$, the admissible class of operators, $\mathcal
    B_\rho$, is taken to be the Hilbert space of Borel-measurable maps   $A:(L^2_\nu,\mathscr{F},\rho)\to
    (L^2_\eta, \mathcal B (L^2_\eta))$ satisfying $\Vert A\Vert^2_{\mathcal B_\rho}=
    \int_{L^2_\nu}\Vert A(f)\Vert_{L^2_\eta}^2 \rho(df)<\infty,$ and the reconstruction
    error is measured in this norm. Given an $N$ dimensional subspace $V_N \subset
    \mathcal B_\rho$, we establish probabilistic accuracy and stability results for
    general weighted least squares approximations in $V_N$, and we show that there
    exists a sampling measure $\mu$ and weight $w$ for which optimal stability and
    accuracy can be achieved for a given sample size $M$. We provide an explicit optimal
    sampling measure when $\rho$ is Gaussian and $V_N$ comprises both linear and non-linear
    operators. We highlight the effectiveness of this method in several numerical
    experiments: these include learning the PDE solution operator to Darcy flow,  Naiver-Stokes,
    and Burgers equations. Our framework offers similar accuracy to Fourier Neural
    Operators as well as several advantages such as interpretability, $ a priori $
    error estimation with optimal sample efficiency, and simplicity of implementation.
- poster: 139691
  presenter: "Tymochko, Sarah"
  email: "74e0b1ca73757e45"
  title: |-
    MS6: Using Persistent Homology to Analyze Access to Resources with Heterogenous
    Quality
  abstract: |-
    Ideally, all public resources (e.g. parks, grocery stores, hospitals,
    etc.) should be distributed in a way that is fair and equitable to everyone. However,
    this is not always the case. Quantifying how much (or little) access individuals
    have to certain resources is a complex problem. Previous work has shown that tools
    from topological data analysis (TDA) can be useful in determining "holes" in the
    locations of resource locations based on geographic locations and travel times
    [Hickok et al., Persistent homology for resource coverage: a case study of access
    to polling sites, 2023]. Some resources may necessitate incorporation a notion
    of quality. As a case study, we look at public parks, which are heterogeneous
    in many ways. Having access to a park that is hundreds of acres with basketball
    courts, baseball diamonds, and an aquarium is inherently different than having
    access to a small patch of grass with an overgrown tennis court. Here we present
    an exploration of the access to public parks in Chicago using persistent homology,
    a tool from TDA.
- poster: 139992
  presenter: "Uhl, Christian"
  email: "3731cb5643d122d5"
  title: "DyCA and DMD: Differences and Similarities in Theory and Application"
  abstract: |-
    Dynamical Component Analysis (DyCA) is a multivariate/spatio-temporal
    signal decomposition technique that aims to obtain a data-driven model of the
    signal, which is governed by a special set of differential equations. The optimization
    leads to a generalized eigenvalue problem of correlation matrices of the signal
    and time-shifted signals. Therefore, the algorithm is similar to Dynamic Mode
    Decomposition (DMD) with respect to its goal and underlying data-driven approach.
    However, the approach differs from that of DMD. We review the algorithm and its
    application to both simulated and real-world data. New results are presented to
    demonstrate similarities and differences between DyCA and DMD. These are discussed
    with respect to mode selection, amplitude, and signal representation under varying
    measurement setups, including temporal resolution and noise conditions. DyCA outperforms
    DMD under certain conditions and may represent a useful alternative algorithm
    for dimension reduction and data-driven dynamical systems research.
- poster: 140528
  presenter: "Varshney, Lav"
  email: "1f199c27067a705c"
  title: "MS96: Autoequivariant Network Search Via Group Decomposition (Poster)"
  abstract: |-
    Recent works show that group equivariance as an inductive bias improves
    neural network performance for both classification and generation. However, designing
    group-equivariant neural networks is challenging when the group of interest is
    large and is unknown. Moreover, inducing equivariance can significantly reduce
    the number of independent parameters in a network with fixed feature size, affecting
    its overall performance. We address these problems by proving a new group-theoretic
    result in the context of equivariant neural networks that shows that a network
    is equivariant to a large group if and only if it is equivariant to smaller groups
    from which it is constructed. Using this result, we design a novel fast group
    equivariant construction algorithm, and a deep Q-learning-based search algorithm
    in a reduced search space, yielding what we call autoequivariant networks (AENs).
    AENs find the right balance between equivariance and network size when tested
    on new benchmark datasets, G-MNIST and G-Fashion-MNIST, obtained via group transformations
    on MNIST and Fashion-MNIST respectively that we release. Extending these results
    to group convolutional neural networks, where we optimize between equivariances,
    augmentations, and network sizes, we find group equivariance to be the most dominating
    factor in all high-performing GCNNs on several datasets like CIFAR10, SVHN, RotMNIST,
    ASL, EMNIST, and KMNIST.
- poster: 139709
  presenter: "Veneruso, Sara"
  email: "edd7b30995e058c2"
  title: "MS27: Extensions of Consensus-Based Methods"
  abstract: |-
    One way that is becoming increasingly popular in recent times to solve
    minimization problems for non-convex problems are multi-particle metaheuristic
    derivative-free optimization methods. We will focus on the Particle Swarm Optimization
    (PSO) and the Consensus Based Optimization (CBO) methods, whose basic idea is
    to iteratively update a population of particles according to a consensus dynamic
    inspired by social interactions between individuals. We focus on the constrained
    problem in the scenario of the PSO method. We introduce a micro-macro decomposition
    of such algorithm, by writing the distribution function as the sum of an equilibrium
    part and a non-equilibrium part. Subsequently, we address an optimization problem
    where the cost function is the expectation of a random mapping by virtue of two
    approaches based on the CBO algorithm. We approximate the true objective function
    by two suitable samplings and the solve the newly obtained problem with the designated
    metaheuristics. Several numerical experiments show the validity of the proposed
    algorithms.
- poster: 140533
  presenter: "Vepakomma, Praneeth"
  email: "323f7fa558298838"
  title: "MS62: Privacy for Activation Release in Collaborative Training and Inference"
  abstract: |-
    There has been a recent advent of generative AI methods powered by transformers
    and large foundation models. This has been overshadowed by serious concerns about
    privacy-sensitivities of the raw datasets during training, fine-tuning and prompt
    tuning while maintaining computational efficiency in client-server settings. This
    necessitates new mechanisms of training with formal privacy guarantees for releasing
    intermediate activations (embeddings), by clients that hold the sensitive data
    to a computationally more powerful server. The privacy concern for NLP modalities,
    have arisen due to lack of privacy guarantees at the token level, word embedding
    level and sentence embedding level. Current differential privacy methods are applied
    upon extracting the pre-trained embeddings from a standard NLP architecture, while
    treating these embeddings as tabular data. As differential privacy is not immune
    to pre-processing, the privacy guarantees applied at this attack surface do not
    translate to privacy guarantees at the token, word or sentence level. Moreover,
    preventing model identifiability in this setting of inference serving systems
    where a server holds a model zoo of multiple models via latency and accuracy fingerprinting
    attacks is yet another important adjacent problem. This talk provides formal methods
    to cater to such relevant real-world problems in collaborative training and inference.
- poster: 140198
  presenter: "Vergara, Victor"
  email: "82f3ee65d2b9fcbe"
  title: "Restnet: A Residual Multi-Transformation Network Architecture"
  abstract: "Residual networks (ResNets) allowed the use of deeper networks ameliorating the performance degradation problem of deep neural networks. ResNets create a residual connection consisting of a copy of the input data which is added posteriorly in the network after skipping several layers. In addition to branching out a copy and skipping layers, the ResNeXt technique introduces a multi-branch architecture adding the dimension of “cardinality” to artificial network designs. However, cardinality only refer to branching out several network blocks of similar structure. Initially, block similarity allows for block concatenation increasing the efficiency of the ResNeXt. In this work we discuss breaking block similarity by introducing different types of non-linear activation functions for each branch. We call this method ResTNet. Adding an extra branch to ResNet-18 creates a ResTNet18 structure with small complexity increment in between ResNet18 and ResNet34. Classification loss improves drastically just by introducing a single extra branch compared to corresponding ResNet structure. We compare ResNet and ResTNet by training on satellite imagery from the publicly available Rare Planes Dataset. This dataset consists of ground images taken from a Maxar-5 satellite, 18k annotations and 20 different classes. Distribution Statement A: Approved for Public Release. Distribution is Unlimited. Public Affairs Release Approval # AFRL-2024-2314."
- poster: 139911
  presenter: "Vigder, Yeari"
  email: "1868c9e6ca4c655c"
  title: "MS50: Spectral Embedding with Group Invariance: Theory and Applications"
  abstract: "Spectral embedding is a popular kernel-based nonlinear dimensionality reduction method. We consider a data model where each point is drawn from a low-dimensional manifold, M, and a random element from a known group G is applied to it. This introduces nuisance degrees of freedom, affecting the method’s ability to capture the required representation. To address this, we employ a minimum G-invariant kernel that computes affinities between the orbits of the G action, effectively reducing the complexity of the data representation.  We prove that embedding the data points using the minimum kernel is equivalent to working directly in the quotient space M/G of dimension dim(M)-dim(G), thus capturing the intrinsic geometry of the data and leading to improved convergence rates.  we demonstrate our approach on molecular dynamics and cryo-electron microscopy, where managing the effects of transformations is critical."
- poster: 140072
  presenter: "Vijaywargiya, Arjun"
  email: "fd8475e2414c3ac6"
  title: |-
    MS61: Efficient Computation of Mean-Field Control Based Barycenters from
    Reaction-Diffusion Systems
  abstract: |-
    We develop a class of barycenter problems based onmean field control
    problems in three dimensions with associated reactive-diffusion systems of unnormalized
    multi-species densities. This problem is the generalization of the Wasserstein
    barycenter problem for single probability density functions. The primary objective
    is to present a comprehensive framework for efficiently computing the proposed
    variational problem: generalized Benamou-Brenier formulas with multiple input
    density vectors as boundary conditions. Our approach involves the utilization
    of high-order finite element discretizations of the spacetime domain to achieve
    improved accuracy. The discrete optimization problem is then solved using the
    primal-dual hybrid gradient (PDHG) algorithm, a first-order optimization method
    for effectively addressing a wide range of constrained optimization problems.
    The efficacy and robustness of our proposed framework are illustrated through
    several numerical examples in three dimensions, such as the computation of the
    barycenter of multi-density systems consisting of Gaussian distributions and reactive-diffusive
    multi-density systems involving 3D voxel densities. Additional examples highlighting
    computations on 2D embedded surfaces are also provided.
- poster: 139912
  presenter: "Villalobos, German"
  email: "e9647bdf0282dc2b"
  title: |-
    MS85: Neural Networks for Inference in Optimal Control Governed by the FitzHugh-Nagumo
    Model
  abstract: |-
    We investigate the use of neural networks (NN) for the estimation of hidden
    model parameters and uncertainty quantification from noisy observational data
    for inverse parameter estimation problems governed by a system of nonlinear ordinary
    differential equations (ODEs). The underlying ODE is the FitzHugh-Nagumo model.
    The considered problem exhibits significant mathematical and computational challenges
    for classical parameter estimation methods, including strong non-linearities,
    non-convexity, and sharp gradients of the optimization landscape. We explore the
    use of NNs to approximate the reconstruction maps for parameter estimation from
    observational data. The considered data are time series of the spiking membrane
    potential of a biological neuron. We infer parameters controlling the dynamics
    of the model, noise parameters of autocorrelated additive noise and noise modelled
    via stochastic differential equations, as well as the covariance matrix of the
    posterior distribution to expose uncertainties. Our approach is motivated by formulating
    parameter estimation as a Bayesian inverse problem. We report results for different
    NN architectures and study the influence of noise on predication accuracy. We
    also report results for training NNs on dedicated hardware. Our results demonstrate
    the NNs are a promising tool to estimate parameters of the dynamical system, stochastic
    processes, as well as uncertainties as they propagate though our system.
- poster: 140155
  presenter: "Vinod, Pranav"
  email: "85813d3798f250df"
  title: |-
    Training Techniques for Learning Neural ODE Models: Accurate Binary Black
    Hole Dynamical Models for Long-Time Integration and Extrapolation
  abstract: |-
    One important astrophysical application of general relativity is solving
    the binary black hole problem. These equations predict that two black holes orbit
    one another and eventually collide, creating disturbances in spacetime that generate
    gravitational waves (GW). Except in the simplest possible cases, however, the
    ordinary differential equations that describe the motion of the two black holes
    are not known. Previous work showed that neural differential equations (NDE) can
    be used to discover these mechanical models from the GW signal by solving a physics-informed
    constrained optimization within a space of plausible models. This approach, however,
    requires solving potentially expensive ODEs multiple times throughout the training
    procedure. Our modified approach employs a feed-forward neural network (NN) trained
    in two stages. First, the NN is trained directly to approximate the ODEs right-hand
    side without considering any physics of the problem. This pre-trained NN can be
    refined by solving a physics-informed constrained optimization using waveform
    data. Preliminary results indicate successful training across various orbits,
    with errors nearing numerical round-off. This enhanced accuracy enables tackling
    new problem types. For example, we show that the resulting NDE accurately extrapolate
    to long time durations, be used close to the problem's separatrix, and can be
    applied to more complex dynamics such as zoom-whirl orbits where multiple distinct
    timescales appear.
- poster: 140577
  presenter: "Vithana, Sajani"
  email: "9cf31559fa4aba9f"
  title: |-
    MS15: Correlated Privacy Mechanisms for Differentially Private Distributed
    Mean Estimation
  abstract: |-
    Differentially private distributed mean estimation (DP-DME) is a fundamental
    building block in privacy-preserving federated learning, where a central server
    estimates the mean of $d$-dimensional vectors held by $n$ users while ensuring
    $(\epsilon,\delta)$-DP. Local differential privacy (LDP) and distributed DP with
    secure aggregation (SecAgg) are the most common notions of DP used in DP-DME settings
    with an untrusted server. LDP provides strong resilience to dropouts, colluding
    users, and malicious server attacks, but suffers from poor utility. In contrast,
    SecAgg-based DP-DME achieves an $O(n)$ utility gain over LDP in DME, but requires
    increased communication and computation overheads and complex multi-round protocols
    to handle dropouts and malicious attacks. We propose CorDP-DME, a novel DP-DME
    mechanism that spans the gap between DME with LDP and distributed DP, offering
    a favorable balance between utility and resilience to dropouts and collusion.
    CorDP-DME is based on correlated Gaussian noise, ensuring DP without the perfect
    conditional privacy guarantees of SecAgg-based approaches. Our results demonstrate
    that (anti) correlated Gaussian DP mechanisms can significantly improve utility
    in mean estimation tasks compared to LDP -- even in adversarial settings -- while
    maintaining better resilience to dropouts and attacks compared to distributed
    DP.
- poster: 140285
  presenter: "Wan, Lin"
  email: "4480b4e49fd9b2b8"
  title: "MS81: Optimal Transport for Single-Cell Heterogeneous Data Analysis"
  abstract: |-
    Advances in single-cell technologies enable comprehensive studies of heterogeneous
    cell populations that make up tissues, the dynamics of developmental processes,
    and the underlying regulatory mechanisms that control cellular functions. The
    computational integration of single-cell datasets is drawing heavy attention toward
    making advancements in machine learning and data science. Optimal transport (OT)
    is a powerful tool in the analysis of complex data, as it learns an optimal cost-effective
    mapping between data distributions. In this poster, I will report our recent work
    on developing OT-based data analysis methods for single-cell multi-omics integration
    and dynamic inference of time series single-cell data.
- poster: 140153
  presenter: "Wang, Zhichao"
  email: "85baaec137735394"
  title: |-
    Nonlinear Spiked Covariance Matrices and Signal Propagation in Deep Neural
    Networks
  abstract: "Many recent works have studied the eigenvalue spectrum of the Conjugate Kernel  (CK) defined by the nonlinear feature map of a feedforward neural network.  However, existing results only establish weak convergence of the empirical  eigenvalue distribution, and fall short of providing precise quantitative  characterizations of the “spike' eigenvalues and eigenvectors that  often capture the low-dimensional signal structure of the   learning problem. In this work, we characterize these signal eigenvalues and  eigenvectors for a nonlinear version of the spiked covariance model,  including the CK as a special case. Using this general result, we give  a quantitative description of how spiked eigenstructure in the input data  propagates through the hidden layers of a neural network with random weights.  As a second application, we study a simple regime of representation learning  where the weight matrix develops a rank-one signal component over training  and characterize the alignment of the target function with the spike  eigenvector of the CK on test data."
- poster: 140196
  presenter: "Wang, Zhichao"
  email: "85baaec137735394"
  title: |-
    Exact Recovery in Semi-Supervised Learning: Analysis of Contextual Stochastic
    Block Model on Gcn and Spectral Method
  abstract: |-
    We delve into the challenge of semi-supervised node classification on
    the Contextual Stochastic Block Model (CSBM) dataset. Here, nodes from the two-cluster
    stochastic block model (SBM) are coupled with feature vectors, which are derived
    from a Gaussian Mixture Model (GMM) that corresponds to their respective node
    labels. With only a subset of the CSBM node labels accessible for training, our
    primary objective becomes the accurate classification of the remaining nodes.
    Venturing into the transductive learning landscape, we, for the first time, pinpoint
    the information-theoretical threshold for the exact recovery of all test nodes
    in CSBM. Concurrently, we design an optimal spectral estimator inspired by Principal
    Component Analysis (PCA) with the training labels and essential data from both
    the adjacency matrix and feature vectors. We also evaluate the efficacy of graph
    ridge regression and Graph Convolutional Networks (GCN) on this synthetic dataset.
    Our findings underscore that, in contrast to graph ridge regression, GCN possesses
    the ability to achieve the information threshold of exact recovery in a manner
    akin to the optimal estimator. This disparity highlights the potential role of
    feature learning in augmenting the proficiency of GCN, especially in the realm
    of semi-supervised learning.
- poster: 139900
  presenter: "Wang, Yating"
  email: "bceda223ecc97498"
  title: "MS42: Optimal Transport with Cost-Free Transformations"
  abstract: |-
    An extension of the optimal transport problem is proposed, which includes
    a family of transformations incurring no transportation cost. Such extension improves
    the co-registration among datasets, where certain transformations, such as rotations,
    displacements and changes of perspective, are a natural component of data acquisition.   The
    extended optimal transport problem pairs two distributions with minimal distortion,
    while identifying the cost-free explainable components of the map.   A data-driven
    formulation is developed, as well as a methodology for its numerical solution.
    This complements gradient descent with a game-theory inspired approach, favoring
    collaborative moves between the cost-free and the regular transformations.  The
    methodology is illustrated through its application to the pairing of both synthetic
    and real images.
- poster: 139915
  presenter: "Wang, Peng"
  email: "75c0f3ac2de038aa"
  title: "MS10: Interpretable Diffusion Models Via Low-Dimensional Data"
  abstract: |-
    Recently, diffusion models have emerged as powerful deep generative models,
    showcasing cutting-edge performance across various applications such as image
    generation, solving inverse problems, and text-to-image synthesis. These models
    generate new data (e.g., images) by transforming random noise inputs through a
    reverse diffusion process. In this work, we study the optimization foundations
    of diffusion models via low-dimensional modeling. Specifically, under the mixture
    of low-rank Gaussians model, we show that the training problem of the diffusion
    model under proper network parameterization is equivalent to the subspace clustering
    problem. Based on this result, we explain the phenomenon of transition from memorization
    to generalization in diffusion models. Moreover, we also use it to guide controllable
    image editions via low-dimensional semantic space.
- poster: 139632
  presenter: "Wang, Christopher"
  email: "7f8b1acb08ae5e75"
  title: "MS1: Operator Learning for Hyperbolic Partial Differential Equations"
  abstract: "We construct the first rigorously justified probabilistic algorithm for recovering the solution operator of a hyperbolic partial differential equation (PDE) in two variables from input-output training pairs. The primary challenge of recovering the solution operator of hyperbolic PDEs is the presence of characteristics, along which the associated Green's function is discontinuous. Therefore, a central component of our algorithm is a rank detection scheme that identifies the approximate location of the characteristics. By combining the randomized singular value decomposition with an adaptive hierarchical partition of the domain, we construct an approximant to the solution operator using $O(\\Psi_\\epsilon^{-1} \\epsilon^{-7} \\log(\\Xi_\\epsilon^{-1}\\epsilon^{-1}))$ input-output pairs with relative error $O(\\Xi_\\epsilon^{-1}\\epsilon)$ in the operator norm as $\\epsilon\\to0$, with high probability. Here, $\\Psi_\\epsilon$ represents the existence of degenerate singular values of the solution operator, and $\\Xi_\\epsilon$ measures the quality of the training data. Our assumptions on the regularity of the coefficients of the hyperbolic PDE are relatively weak given that hyperbolic PDEs do not have the “instantaneous smoothing effect' of elliptic and parabolic PDEs, and our recovery rate improves as the regularity of the coefficients increases."
- poster: 140264
  presenter: "Wang, Yiran"
  email: "b23350aec5725a32"
  title: "A Modified Extreme Learning Machine Based Diffusion Model in Image Denoising"
  abstract: |-
    In this work, we introduce a novel approach to image denoising using an
    Extreme Learning Machine (ELM) based diffusion model. Inspired by the efficacy
    of ELM in approximating partial differential equations (PDEs), we adapt this technique
    to address the problem of image denoising, which can be conceptualized as solving
    a diffusion process. Traditional ELM methods for solving PDEs typically handle
    algebraic equations, which may lead to limited accuracy in image denoising. To
    address this limitation, we propose a modified approach by incorporating a loss
    function comprising measurement data, equation information, and a carefully designed
    regularization term. As a result, our method seamlessly integrates a data-driven
    approach with physical principles. We demonstrate the effectiveness of our proposed
    method through a series of representative examples. Our results indicate that
    the method achieves a favorable balance between computational efficiency and denoising
    accuracy, making it a promising approach for real-world applications.
- poster: 139884
  presenter: "Wang, Xili"
  email: "13615b860808b52a"
  title: "MS73: Aonn-2: A Deep Learning Based Shape Optimization Method"
  abstract: |-
    Shape optimization has been playing an important role in a large variety
    of engineering applications. Existing shape optimization methods are generally
    mesh-dependent and therefore encounter challenges due to mesh deformation. To
    overcome this limitation, we present a new adjoint-oriented neural network method,
    AONN-2, for PDE-constrained shape optimization problems. This method extends the
    capabilities of the original AONN method, which is developed for efficiently solving
    parametric optimal control problems. AONN-2 inherits the direct-adjoint looping
    (DAL) framework for computing the extremum of an objective functional and the
    neural network methods for solving complicated PDEs from AONN. Furthermore, AONN-2
    expands the application scope to shape optimization by taking advantage of the
    shape derivatives to optimize the shape represented by discrete boundary points.
    AONN-2 is a fully mesh-free shape optimization approach, naturally sidestepping
    issues related to mesh deformation, with no needs for maintaining mesh quality
    and additional mesh corrections. A series of experimental results are presented,
    highlighting the flexibility, robustness, and accuracy of AONN-2.
- poster: 140213
  presenter: "Wang, Tianyu"
  email: "dce41ffef6bb01bd"
  title: |-
    MS48: The Anytime Convergence of Stochastic Gradient Descent with Momentum:
    From a Continuous-Time Perspective
  abstract: |-
    We study the stochastic optimization problem from a continuous-time perspective.
    We propose a stochastic first-order algorithm, called Stochastic Gradient Descent
    with Momentum (SGDM), and show that the trajectory of SGDM, despite its stochastic
    nature, converges to a deterministic second-order Ordinary Differential Equation
    (ODE) in $L_2$-norm, as the stepsize goes to zero. The connection between the
    ODE and the algorithm results in delightful patterns in the discrete-time convergence
    analysis. We prove that, for any $\beta$, the sequence ${ x_k }$ governed by running
    SGDM on a smooth convex function $f$ satisfies $ \mathbb{P} \left( f (x_k) - f^*
    \le \frac{C\sqrt{\log k}\log(2/\beta)}{\sqrt{k}}, ; \forall t \in \mathbb{N}\right)\ge
    1-\beta, $ where $C$ is some constant.
- poster: 140009
  presenter: "Wang, Zheyu Oliver"
  email: "cf5f3ff3803fc2e9"
  title: |-
    MS42: Efficient Neural Network Approaches for Conditional Optimal Transport
    with Applications in Bayesian Inference
  abstract: "We present two neural network approaches that approximate the solutions of static and dynamic conditional optimal transport (COT) problems. Both approaches enable conditional sampling and conditional density estimation, which are core tasks in Bayesian inference---particularly in the simulation-based (“likelihood-free') setting. Our methods represent the target conditional distributions as transformations of a tractable reference distribution and, therefore, fall into the framework of measure transport. Conditional optimal transport (COT) comprises a unique, canonical choice within this framework, but finding COT maps can be computationally challenging, even in moderate dimensions. To improve scalability, our numerical algorithms use neural networks to parameterize COT maps and further exploit the structure of the COT problem. Our static approach approximates the map as the gradient of a partially input-convex neural network and uses a novel numerical implementation to increase computational efficiency compared to state-of-the-art alternatives. Our dynamic approach approximates the conditional optimal transport via the flow map of a regularized neural ODE; compared to the static approach, it is slower to train but faster to generate samples. We demonstrate both algorithms numerically, comparing them with competing state-of-the-art approaches, using benchmark datasets and simulation-based Bayesian inverse problems."
- poster: 139906
  presenter: "Wang, Rui"
  email: "51c3060f7ce987f3"
  title: "MS38: Exploring the Power of Persistent Spectral Graphs in Covid-19 Research"
  abstract: |-
    The ongoing COVID-19 pandemic has underscored the critical need for computational
    tools to interpret complex biological data and enhance our understanding of viral
    spread and treatment efficacy. This study presents a pioneering integration of
    computational topology and artificial intelligence (AI), showcasing a novel approach
    to epidemiological research.       Specifically, we studied the capacities of
    persistent spectral graphs (PSGs) for analyzing intricate topological and geometric
    properties of high-dimensional biological data. Our approach begins with the formulation
    of persistent Laplacian matrices (PLMs), constructed through applying a dynamic
    filtration parameter. The harmonic spectra derived from the null spaces of these
    PLMs encapsulate the underlying topological features, while the non-harmonic spectra
    elucidate the geometry of high-dimensional datasets.        During the COVID-19
    pandemic, we integrated PSG, genomics, and deep learning into a Math-AI model
    to predict the binding free energy (BFE) changes caused by mutations in the interaction
    between the virus's Spike protein and the human ACE2 receptor or antibodies. Such
    a Math-AI model has successfully forecasted the predominance of Omicron variants
    one or two months ahead of their presence, offering a glimpse into a future where
    mathematical techniques and biology converge to combat viral threats more effectively.
- poster: 139835
  presenter: "Wang, Pengjun"
  email: "1691695714ed2b8b"
  title: "MS58: A Pseudo-Reversible Normalizing Flow for Stochastic Differential Equation"
  abstract: "We present a pseudo-reversible normalizing flow method for efficiently generating samples of the state of a stochastic differential equation (SDE) with various initial distributions. The primary objective is to construct an accurate and efficient sampler  that can be used as a surrogate model for computationally expensive numerical integration of SDE, such as those employed in particle simulation. After training, the normalizing flow model can directly generate samples of the SDE's final state without simulating trajectories.The main novelty of our normalizing flow model is that it can learn the conditional distribution of the state, i.e., the distribution of the final state conditional on any initial state, such that the model only needs to be trained once and the trained model can be used to handle various initial distributions. Additionally, we propose to use a pseudo-reversible network architecture to define the normalizing flow model, which has sufficient expressive power and training efficiency for a variety of SDEs in science and engineering, e.g., in particle physics. We provide a rigorous convergence analysis of the pseudo-reversible normalizing flow model to the target probability density function in the Kullback–Leibler divergence metric. Numerical experiments are provided to demonstrate the effectiveness of the proposed normalizing flow model."
- poster: 140144
  presenter: "Wang, Yixuan"
  email: "e52446c3e3dea716"
  title: |-
    Machine Learning-Assisted Agent-Based Model Predicts Outcomes of Cancer Immunotherapy
    and Highlights Spatial Complexity of the Tumor Microenvironment
  abstract: |-
    Immune checkpoint inhibitors (ICIs) have changed the landscape of molecular
    therapeutics for cancers. However, ICIs do not work equally well for all patients.
    There has been a growing interest in using computational models to optimize clinical
    responses. Ordinary differential equations (ODEs) have been widely used for mechanistic
    modeling in immunotherapy because they allow rapid simulations of temporal changes
    in the cellular and molecular populations involved. Agent-based models (ABMs)
    have gained popularity because they can model more detailed spatial heterogeneity
    that better reflect the complexity seen in vivo. In the context of anti-PD-1 immune
    checkpoint inhibitors, we compare treatment outcomes simulated from an ODE model
    and an ABM to show the importance of including spatial components in computational
    models of cancer immunotherapy. We consider tumor cells of high and low antigenicity
    and two distinct cytotoxic T lymphocyte (CTL) killing mechanisms, which differ
    based on the antigenicity of tumor cells. Our ABM reveals different spatial distributions
    of tumor and immune cells even when the temporal trajectories look similar. We
    also use machine learning (ML) to predict tumor status in the intermediate and
    long term based on immediate treatment responses in the ABM. Our ML-assisted method
    eliminates the need to simulate the ABM until equilibrium and suggests an effective
    way to reduce computational time of ABMs.
- poster: 140531
  presenter: "Wang, Gaozhan"
  email: "89fcc5aee199f28e"
  title: |-
    MS56: Rate of Convergence of Policy Improvement Algorithm for Exploratory
    Stochastic Control Problems
  abstract: |-
    In this presentation, we study the Policy Improvement Algorithm for reinforcement
    learning for continuous-time entropy-regularized stochastic control problems.
    We prove the uniform convergence both for the iterative value functions and for
    their derivatives. More importantly, in the finite horizon case and in the infinite
    horizon case with a large discount factor, we obtain the exponential rate of convergence,
    which is new in the literature. Our arguments rely on a simple representation
    formula for the derivatives of a linear PDE and are much easier than those in
    the existing works for convergence.
- poster: 140529
  presenter: "Wang, Peihao"
  email: "5c608d88e3d6a270"
  title: |-
    MS96: Signal Processing with Implicit Neural Fields: Editing, Manipulation,
    and Understanding (Poster)
  abstract: |-
    Implicit Neural Representations (INRs) use multi-layer perceptrons to
    encode continuous multi-media data, showing promise in various computer vision
    tasks. However, editing and processing INRs remain challenging since signals are
    represented by neural network parameters. Existing methods manipulate these continuous
    representations by processing their discretized instances, losing INR's compactness
    and continuity. This work explores directly modifying INRs without explicit decoding
    by proposing an implicit neural signal processing network, INSP-Net, using differential
    operators on INRs. The key insight is that spatial gradients of neural networks
    can be computed analytically and are translation-invariant. Additionally, any
    continuous convolution filter can be approximated by a linear combination of high-order
    differential operators. INSP-Net applies signal processing operators as weighted
    compositions of computational graphs corresponding to INRs' high-order derivatives,
    with data-driven learned weighting parameters. Building on INSP-Net, we introduce
    INSP-ConvNet, a Convolutional Neural Network that implicitly operates on INRs.
    Experiments demonstrate the effectiveness of INSP-Net and INSP-ConvNet in low-level
    image and geometry processing tasks (e.g., blurring, deblurring, denoising, inpainting,
    smoothening) and high-level tasks on implicit fields like image classification
- poster: 139711
  presenter: "Wang, Shuaiqi"
  email: "3edbe59b5aed299a"
  title: "MS46: Statistic Maximal Leakage"
  abstract: "We introduce a privacy metric called statistic maximal leakage that quantifies how much a privacy mechanism leaks about a specific secret, relative to the adversary’s prior information about that secret. Statistic maximal leakage is an extension of the well-known maximal leakage. Unlike maximal leakage, it protects a single, known secret. We show that statistic maximal leakage satisfies composition and post-processing properties. Additionally, we show how to efficiently compute it in the special case of deterministic data release mechanisms. We analyze two important mechanisms under statistic maximal leakage: the quantization mechanism and randomized response. We show theoretically and empirically that the quantization mechanism achieves better privacy-utility tradeoffs in the settings we study."
- poster: 139736
  presenter: "Wang, Jian-Xun"
  email: "d0edd606d0b6e36a"
  title: |-
    MS57: CoNFiLD: Conditional Neural Field Latent Diffusion Model Generating
    Spatiotemporal Turbulence
  abstract: |-
    This study introduces the Conditional Neural Field Latent Diffusion (CoNFiLD)
    model, a novel generative learning framework designed for rapid simulation of
    intricate spatiotemporal dynamics in chaotic and turbulent systems within three-dimensional
    irregular domains. Traditional eddy-resolved numerical simulations, despite offering
    detailed flow predictions, encounter significant limitations due to their extensive
    computational demands. In contrast, deep learning-based surrogate models promise
    efficient, data-driven solutions. However, their effectiveness is often compromised
    by a reliance on deterministic frameworks, which fall short in accurately capturing
    the chaotic and stochastic nature of turbulence. The CoNFiLD model addresses these
    challenges by synergistically integrating conditional neural field encoding with
    latent diffusion processes, enabling the memory-efficient and robust probabilistic
    generation of spatiotemporal turbulence under varied conditions. Leveraging Bayesian
    conditional sampling, the model can seamlessly adapt to a diverse range of turbulence
    generation scenarios without the necessity for retraining, covering applications
    from zero-shot full-field flow reconstruction using sparse sensor measurements
    to super-resolution generation and spatiotemporal flow data restoration.
- poster: 139116
  presenter: "Waniorek, Nathan"
  email: "2eb07dc55b31971b"
  title: |-
    MS69: Hierarchical Bayesian Inverse Problems: a High-Dimensional Statistics
    Viewpoint
  abstract: |-
    We analyze hierarchical Bayesian inverse problems using techniques from
    high dimensional statistics. Our analysis leverages a property of hierarchical
    Bayesian regularizers that we call approximate decomposability to obtain non-asymptotic
    bounds on the reconstruction error attained by maximum a posteriori estimators.
    The new theory explains how hierarchical Bayesian models that exploit sparsity,
    group sparsity, and sparse    representations of the unknown parameter can achieve
    accurate reconstructions in high-dimensional settings.
- poster: 140027
  presenter: "Warns, Maria"
  email: "c1971a786826be1d"
  title: |-
    MS71: Using Identifiability Analysis to Evaluate Wastewater Surveillance
    and Public Health Data As Indirect Observables to Inform Predictive Disease Models
  abstract: |-
    Disease modeling facilitates model-based predictions for public health
    purposes using limited and indirect observations of unknown fidelity and noise,
    like cases and hospital admission rates. Wastewater-based surveillance is an increasingly
    available data stream which may improve calibration of disease models. Unlike
    traditional public health measures, wastewater samples reflect the entire population
    in a sewershed community since individuals infected with SARS-CoV-2 shed viral
    RNA in their stool regardless of symptomology. But the utility of these measurements
    to inform models is unknown and depends on both functional characteristics of
    the chosen disease model and quality of measurements. In this study, we evaluate
    whether wastewater surveillance data improves the calibration of parameters in
    compartmental disease models using identifiability analysis. Structural identifiability
    methods determine whether the parameters of a model can be uniquely identified
    locally or globally, given specified observables and assuming that data is perfect,
    using differential algebra approaches. Practical identifiability methods assess
    the influence of data quality on parameter estimation given an actual data set.
    Identifiability analysis informs the trust one gives parameter values, which can
    have important meaning for public health decision-making. We characterize the
    identifiability of disease transmission models under cases for varying data availability
    and quality during a pandemic.
- poster: 139866
  presenter: "Warren, Andrew"
  email: "a81bc989222bf4f6"
  title: "MS26: Principal Curves in Wasserstein Space"
  abstract: |-
    A "principal curve" of a data distribution refers to a class of nonlinear
    generalizations of the first principal component: namely, we seek a continuous
    curve which locally "passes through the middle" of the distribution. Originally
    proposed in the context of statistics by Hastie and Stuetzle, the principal curve
    problem is closely related to the "average distance problem" in the calculus of
    variations. Our work introduces the principal curve problem in the Wasserstein
    space of probability measures. We relate this problem to the problem of finding
    minimizers to a length-penalized average-distance variational problem in this
    space; for said problem, we then prove existence of minimizers, stability with
    respect to the underlying data distribution, and consistency of a discretized
    variational problem. This latter problem enjoys a "coupled Lloyd's algorithm"
    type numerical scheme which can be feasibly implemented via existing Wasserstein
    barycenter solvers. Lastly, we apply this general framework to a concrete problem
    motivated by recent developments in measurement technologies for gene expression
    data. Namely, given a stochastic process and batches of samples from different
    temporal marginals, but without temporal labels on the batches, can we infer the
    temporal ordering of the batches? This is a version of the so-called seriation
    problem. We thus show that principal curves in Wasserstein space can be employed
    as a consistent seriation method for empirical distributions.
- poster: 139732
  presenter: "Watts, Jeremy"
  email: "f43f12ae758832e9"
  title: "MS79: Characterizing Lymphedema Risk Using MRI Topological Data Analysis"
  abstract: "Breast cancer-related lymphedema (BCRL) is the most common post-treatment complication affecting breast cancer survivors, occurring in $20 %$ of survivors. BCRL is a result of lymph fluid backup due to the damage or removal of lymph nodes during breast cancer treatment. This chronic condition can negatively impact patients’ quality of life due to its intense pain and reduction in mobility. While risk factors for developing this condition have been previously identified, these methods remain insufficient for predicting individualized patient risk. To improve these methods, we leverage topological data analysis to identify anatomical variations in the breast tissue leading to increased lymphedema risk using preoperative MRIs in a primary cohort of 364 breast cancer patients. The resulting model seeks to improve lymphedema risk scoring and identify regions of interest to guide clinical decision-making. Our results are benchmarked against both traditional machine learning (incorporating demographic and treatment information) and deep learning models (incorporating MRI data). With improved BCRL prediction, patients at the greatest risk of developing this condition can be identified as candidates for preventative surgeries, such as lymphovenous bypass surgery, which have historically not been cost-effective to implement widely."
- poster: 140534
  presenter: "Webber, Robert"
  email: "ab41bfb683200972"
  title: "MS92: Fast, Effective Data Reduction Through Novelty Sampling"
  abstract: |-
    We present a new algorithm for reducing a large data set to a small number
    of landmark data points. The landmarks are randomly selected, yet they account
    for nearly all the "novelty" in the data. To generate landmarks, we randomly propose
    data points and accept/reject with probabilities depending on the previous selections.
    After the generation step, the landmarks can be used to quickly make predictions
    and find clusters in the data. Landmark-based learning has a memory footprint
    which is independent of the data size, so the approach is suitable for distilling
    large data sets with N >= 10^9 data points.
- poster: 139548
  presenter: "Webber, Robert"
  email: "ab41bfb683200972"
  title: "MS54: Robust Randomized Preconditioning for Kernel Ridge Regression"
  abstract: |-
    Our work introduces two randomized preconditioning techniques for robustly
    solving kernel ridge regression (KRR) problems with a medium to large number of
    data points ($10^4 <= N <= 10^7$). The first method, RPCholesky preconditioning,
    is capable of accurately solving the full-data KRR problem in $O(N^2)$ arithmetic
    operations, assuming sufficiently rapid polynomial decay of the kernel matrix
    eigenvalues. The second method, KRILL preconditioning, offers an accurate solution
    to a restricted version of the KRR problem involving $k < < N$ selected data centers
    at a cost of $O((N + k^2)k \log k)$ operations. The proposed methods efficiently
    solve a broad range of KRR problems and overcome the failure modes of previous
    KRR preconditioners, making them ideal for practical applications.
- poster: 140212
  presenter: "Webster, Clayton"
  email: "2572234eb3ec00ff"
  title: "MS48: Gaussian Smoothing (Stochastic) Gradient Descent"
  abstract: |-
    This talk presents Gaussian smoothing as a powerful tool to enhance gradient
    descent algorithms, particularly in navigating complex and non-convex optimization
    landscapes. Leveraging Gaussian smoothing, we define a nonlocal gradient that
    effectively reduces high-frequency noise and rapid fluctuations while preserving
    essential structural features. The resulting Gaussian smoothing gradient descent
    (GSmoothGD) approach exhibits remarkable efficacy in traversing away from local
    minima, thereby substantially improving overall optimization performance. This
    can be applied to data science and machine learning by Gaussian smoothing stochastic
    gradient descent (GSmoothSGD) as well. We provide theoretical error estimates
    on the convergence rates of GSmoothGD and GSmoothSGD iterates, considering factors
    such as function convexity, smoothness, input dimension, and the Gaussian smoothing
    radius.
- poster: 139716
  presenter: "Wei, Rongzhe"
  email: "a7b44206d630e92b"
  title: |-
    MS46: On the Inherent Privacy Properties of Discrete Denoising Diffusion
    Models
  abstract: |-
    Privacy concerns have led to a surge in the creation of synthetic datasets,
    with diffusion models emerging as a promising avenue. Although prior studies have
    performed empirical evaluations on these models, there has been a gap in providing
    a mathematical characterization of their privacy-preserving capabilities. To address
    this, we present the pioneering theoretical exploration of the privacy preservation
    inherent in discrete diffusion models (DDMs) for discrete dataset generation.
    Focusing on per-instance differential privacy (pDP), our framework elucidates
    the potential privacy leakage for each data point in a given training dataset,
    offering insights into how the privacy loss of each point correlates with the
    dataset's distribution.      Our bounds also show that training with $s$-sized
    data points leads to a surge in privacy leakage from $(\epsilon, \mathcal{O}(\frac{1}{s^2\epsilon}))$-pDP
    to $(\epsilon, \mathcal{O}(\frac{1}{s\epsilon}))$-pDP of the DDM during the transition
    from the pure noise to the synthetic clean data phase, and a faster decay in diffusion
    coefficients amplifies the privacy guarantee. Finally, we empirically verify our
    theoretical findings on both synthetic and real-world datasets.
- poster: 140007
  presenter: "Weidensager, Laura"
  email: "41d0d4e5be368b31"
  title: "Interpretable Function Approximation Using Random Fourier Features and ANOVA-Boosting"
  abstract: |-
    We study the problem of scattered-data approximation, where we have given
    sample points and the corresponding function evaluations. In the random Fourier
    feature approach, we draw frequencies at random and learn coefficients from the
    given data to construct the approximant. We use the classical analysis of variance
    (ANOVA) decomposition for approximating high-dimensional functions of low effective
    dimension. Thereby we give a relation between the Fourier transform of the function
    and the ANOVA terms. In the case for dependent input variables, the ANOVA decomposition
    is generalized with the aim to detect the structure of the function, i.e. to find
    which input variables and variable interactions are important. This information
    is then used to boost random Fourier feature algorithms.   Furthermore, we generalize
    already existing random Fourier feature models to an ANOVA setting, where terms
    of different order can be used. Our algorithms have the advantage of interpretability,
    meaning that the influence of every input variable is known, even for dependent
    input variables. We give theoretical as well as numerical results that our algorithms
    perform well for sensitivity analysis. The ANOVA-boosting step reduces the approximation
    error of existing methods significantly.
- poster: 139795
  presenter: "Wen, Zheyu"
  email: "9c1386f867e1eacf"
  title: |-
    MS90: Biophysics-Based Data Assimilation of Longitudinal Tau and Amyloid-$\beta$
    Pet Scans
  abstract: |-
    Misfolded tau and amyloid-$\beta$ (A$\beta$) are hallmark proteins of
    Alzheimer's Disease (AD). Interpreting and combining this data beyond statistical
    correlations remains a challenge. Biophysical models offer a complementary avenue
    to assimilating such complex data. To this end, we introduce a mathematical model
    that tracks the dynamics of four species (normal and abnormal tau and A$\beta$)
    and uses a graph to approximate their spatial coupling. The graph nodes represent
    gray matter regions of interest (ROI), and the edges represent tractography-based
    connectivity between ROIs. We model interspecies interactions, migration, proliferation,
    and clearance. Our biophysical model has seven unknown scalar parameters plus
    unknown initial conditions for tau and A$\beta$. Using imaging MRI, tau and A$\beta$
    scans, we can calibrate these parameters by solving an inverse problem. We propose
    an inversion algorithm that stably reconstructs the unknown parameters.  We verify
    and test its numerical stability in the presence of noise using synthetic data.
    We discovered that the inversion is more stable when using multiple scans. Finally,
    we apply the overall methodology on 334 subjects from the ADNI dataset and compare
    it to a commonly used tau-only model calibrated by a single PET scan. We report
    the $R^2$ and relative fitting error metrics. The proposed method achieves $R^2=0.82$
    compared to $R^2=0.64$ of the tau-only single-scan reconstruction.
- poster: 139718
  presenter: "Wiesman, Maximilian"
  email: "1f2eb8a4eebb8dd6"
  title: "MS78: Geometry of Polynomial Neural Networks"
  abstract: |-
    We study the expressivity and learning process for polynomial neural
    networks (PNNs) with monomial activation functions. The weights of the network
    parametrize the neuromanifold. In this paper, we study certain neuromanifolds
    using tools from algebraic geometry: we give explicit descriptions as semialgebraic
    sets and characterize their Zariski closures, called neurovarieties. We study
    their dimension and associate an algebraic degree, the learning degree, to the
    neurovariety. The dimension serves as a geometric measure for the expressivity
    of the network, the learning degree is a measure for the complexity of training
    the network and provides upper bounds on the number of learnable functions. These
    theoretical results are accompanied with experiments.
- poster: 140313
  presenter: "Wild, Stefan"
  email: "9c7ff3aa2b0bf7fb"
  title: |-
    MS68: Performance Analysis of Sequential Experimental Design for Calibration
    in Parallel Computing Environments
  abstract: |-
    Calibration of computationally expensive simulations is usually carried
    out with a data-driven emulator. The effectiveness of the calibration process
    can be significantly improved by using a sequential/online construction of such
    an emulator. The expansion of parallel computing environments can lead to further
    calibration efficiency gains by allowing for the evaluation of the simulation
    model at a batch of parameters in parallel in a sequential design. However, understanding
    the performance implications of different sequential approaches in parallel computing
    environments introduces new complexities since the rate of the speed-up is affected
    by many factors, such as the run time of a simulation model and the variability
    in the run time. This work proposes a new way to understand and benchmark the
    performance of different sequential procedures for the calibration of simulation
    models in parallel environments. We provide metrics and a suite of techniques
    for visualizing the numerical experiment results and demonstrate these with a
    novel sequential procedure. The proposed performance model and the sequential
    procedure along with the state-of-art techniques are implemented in the Python
    software package Parallel Uncertainty Quantification (PUQ), which allows users
    to run a simulation model in parallel. PUQ is an open-source software package
    at https://github.com/parallelUQ/PUQ
- poster: 139922
  presenter: "Williams, Jan"
  email: "cb8b502ea58dd219"
  title: |-
    MS85: Reservoir Computing for System Identification and Predictive Control
    with Limited Data
  abstract: |-
    Model predictive control (MPC) is an industry standard control technique.
    MPC functions by iteratively solving an open-loop optimization problem to guide
    a system towards a desired state or trajectory. Consequently, an accurate forward
    model of system dynamics is critical for the efficacy of MPC and much recent work
    has been aimed at the use of neural networks to act as data-driven surrogate models
    to enable MPC. Perhaps the most common network architecture applied to this task
    is the recurrent neural network (RNN) due to its natural interpretation as a dynamical
    system. In this work, we assess the ability of RNN variants to both learn the
    dynamics of exemplar control systems and serve as surrogate models for MPC. We
    find that echo state networks (ESNs) have a variety of benefits over competing
    architectures, namely reductions in computational complexity, longer valid prediction
    times, and reductions in cost of the MPC objective function.
- poster: 140190
  presenter: "Woo, Hyenkyun"
  email: "92e3692c955b1bb5"
  title: "Linear Classification for Imbalanced Dataset Via Skewed Hyperplane Equation"
  abstract: |-
    In cost-sensitive learning, we usually use the $\pi$-weighted loss function
    to learn a hyperplane decision boundary considering class-imbalance ratio (scale-class-imbalance
    ratio, the ratio of effective number of samples, etc) of the training dataset.
    Empirically, $\pi$-weight is set to be in proportion to the inverse of the size
    of each class. However, it is still unclear how $\pi$-weighted loss function is
    related to class-imbalance ratio of the training dataset. This talk shows the
    connection between the class-imbalance ratio and the loss function used in imbalanced
    classification via a skewed hyperplane equation. This equation is created by linearization
    of the gradient satisfying $\epsilon$-optimal condition. Instead of the margin
    of each sample, the statistic of the margin distribution, i.e., mean-margin, is
    considered while we describe the skewed hyperplane equation. Through the skewed
    hyperplane equation, we could somehow understand a mysterious connection between
    the hyperplane decision boundary and the imbalance-oriented loss function based
    on various $51$ binary class dataset.
- poster: 140539
  presenter: "Wu, Yantao"
  email: "5a078ee8dc7e1e0a"
  title: "MS55: Conditional Regression on a Nonlinear Variable Model"
  abstract: |-
    We consider the problem of estimating the intrinsic structure of composite
    functions of the type F(X)= f(G(X)) where G is the closest point projection onto
    some unknown smooth curve, and f is some unknown link function. This model is
    a generalization of the single-index model where G is a linear rank-one matrix.
    We use the Conditional Regression method to extract information of the nonlinear
    curve, and show that we can relax the curve of dimensionality in this problem:
    under some assumptions restricting the complexity of curve, our estimator for
    function F can achieve one-dimensional optimal minimax rate, with an extra approximation
    error dependent on external noise. We also perform numerical tests to verify the
    robustness of our algorithm, in that even if some assumptions are not satisfied,
    we also achieve small estimation errors.
- poster: 139591
  presenter: "Wu, Jingfeng"
  email: "b4677afeb74e2c75"
  title: |-
    MS59: Large Stepsize Gradient Descent for Logistic Loss: Non-Monotonicity
    of the Loss Improves Optimization Efficiency
  abstract: |-
    We consider gradient descent (GD) with a constant stepsize applied to
    logistic regression with linearly separable data, where the constant stepsize
    $\eta$ is so large that the loss initially oscillates. We show that GD exits this
    initial oscillatory phase rapidly --- in $\mathcal{O}(\eta)$ steps, and subsequently
    achieves an $\tilde{\mathcal{O}}(1 / (\eta t) )$ convergence rate after $t$ additional
    steps. Our results imply that, given a budget of $T$ steps, GD can achieve an
    accelerated loss of $\tilde{\mathcal{O}}(1/T^2)$ with an aggressive stepsize $\eta:=
    \Theta( T)$, without any use of momentum or variable stepsize schedulers. Our
    proof technique is versatile and also handles general classification loss functions
    (where exponential tails are needed for the $\tilde{\mathcal{O}}(1/T^2)$ acceleration),
    nonlinear predictors in the neural tangent kernel regime, and online stochastic
    gradient descent (SGD) with a large stepsize, under suitable separability conditions.
- poster: 140120
  presenter: "Wu, Xiangyu"
  email: "5edd66c1a132e4ee"
  title: |-
    MS2: A Latent Space Approach to Inferring Heterogeneous Reciprocity in Directed
    Network Data
  abstract: "Reciprocity, or the stochastic tendency for actors to form mutual relationships, is an essential characteristic of directed network data. Existing latent space approaches to modeling direct networks are severely limited by the assumption that reciprocity is homogeneous across sub-populations in a network. In this work, we introduce a new latent space model for directed networks that can quantify sources of heterogeneous reciprocity due to external covariate information and actor-specific effects. To disentangle the factors influencing individual edge formation and reciprocity, we parameterize the model using the marginal edge probabilities and the odds ratio of forming a mutual relationship. We introduce a fast and scalable stochastic variational inference algorithm based on a structured mean-field variational family to approximate the model’s posterior distribution. Both simulation studies and real-world data examples illustrate that the proposed model effectively uncovers sources of heterogeneous reciprocity in network data."
- poster: 139674
  presenter: "Wu, Ruihan"
  email: "a165cd28bb46a9a7"
  title: "MS46: Better Membership Inference Privacy Measurement Through Discrepancy"
  abstract: |-
    Membership Inference Attacks have emerged as a dominant method for empirically
    measuring privacy leakage from machine learning models. Here, privacy is measured
    by the advantage or gap between a score or a function computed on the training
    and the test data. A major barrier to the practical deployment of these attacks
    is that they do not scale to large well-generalized models -- either the advantage
    is relatively low, or the attack involves training multiple models which is highly
    compute-intensive. In this work, inspired by discrepancy theory, we propose a
    new empirical privacy metric that is an upper bound on the advantage of a family
    of membership inference attacks. We show that this metric does not involve training
    multiple models, can be applied to large Imagenet classification models in-the-wild,
    and has higher advantage than existing metrics on models trained with more recent
    and sophisticated training recipes. Motivated by our empirical results, we also
    propose new membership inference attacks tailored to these training losses.
- poster: 140552
  presenter: "Wu Fung, Samy"
  email: "5d02616afb49f0f4"
  title: |-
    MS37: Mean-Field Control Barrier Functions: A Framework for Real-Time Swarm
    Control (Poster)
  abstract: |-
    Control Barrier Functions (CBFs) are an effective methodology to assure
    safety and performative efficacy in real- time control applications such as power
    systems, resource allocation, autonomous vehicles, robotics, etc. This approach
    ensures safety independently of the high-level tasks that may have been pre-planned
    off-line. For example, CBFs can be used to guarantee that a self-driving car will
    remain in its lane. However, when the number of agents is large, computation of
    CBFs can suffer from the curse of dimensionality in the multi-agent setting. In
    this work, we present Mean-field Control Barrier Functions (MF-CBFs), which extends
    the CBF framework to the mean-field (or swarm control) setting. The core idea
    is to model swarms as probability measures in the state space and build control
    barrier functions in the space of probability measures. Similar to traditional
    CBFs, we derive safety constraints on the (distributed) controls but now relying
    on the differential calculus in the space of probability measures. Our numerical
    experiments show the effectiveness of MF-CBFs applied to swarm tracking and avoidance.
- poster: 139815
  presenter: "Wuerschmidt, Maximilian"
  email: "354d85515028b9ee"
  title: "MS73: A Probabilistic Approach to Shape Derivatives"
  abstract: |-
    We introduce a novel mesh-free and direct method for computing the shape
    derivative in PDE-constrained shape optimization problems. Our approach is based
    on a probabilistic representation of the shape derivative and is applicable for
    second-order elliptic PDEs with Dirichlet boundary conditions and a general class
    of target functions. The probabilistic representation derives from a boundary
    sensitivity result for diffusion processes due to Constantini, Gobet and El Karoui.
    We provide a Taylor test to verify the accuracy of our methodology.
- poster: 140309
  presenter: "Xia, Li"
  email: "a4cbc83c3b0c33e5"
  title: |-
    MS81: CTRhythm: Accurate Atrial Fibrillation Detection From Single-lead
    Rhythm by Convolutional Neural Network and Transformer Integration
  abstract: |-
    Atrial Fibrillation (AF), a common supraventricular arrhythmia that affects
    about 30 million people globally. Electrocardiogram (ECG) analysis serves as a
    common diagnostic approach. Widespread adoption of wearable devices monitoring
    heart rhythm prompted the development of AF detection models for single-lead ECGs.
    Current state-of-the-art methods for AF detection, such as convolutional neural
    network (CNN) and convolutional recurrent neuralnetwork (CRNN) based models, focus
    on capturing local patterns only, despite the fact heart rhythms exhibit rich
    long-range dependencies. To address this limitation, we propose a novel method
    for single-lead ECG rhythm classification, termed CNN-Transformer Rhythm Classifier
    (CTRhythm), which integratesCNN with a Transformer encoder to effectively captureboth
    local and global patterns. CTRhythm achieved an overall F1 score of 0.8303 and
    an area under the Receiv-er Operating Characteristic Curve (AUROC) at 0.952, outperforming
    the baseline deep learning models on the golden standard CINC2017 dataset. In
    two external validation datasets, CTRhythm achieved overall F1scores of 0.929
    and 0.932, respectively, showing its strong generalization capabilities. CTRhythm
    is freely available at https://github.com/labxscut/CTRhythm.
- poster: 140527
  presenter: "Xu, Wuzhe"
  email: "b6fa38d25904ff25"
  title: |-
    MS96: Generative Downscaling of Pde Solvers with Physics-Guided Diffusion
    Models (Poster)
  abstract: |-
    Solving partial differential equations (PDEs) on fine spatio-temporal
    scales for high-fidelity solutions is critical for numerous scientific breakthroughs.
    Yet, this process can be prohibitively expensive, owing to the inherent complexities
    of the problems, including nonlinearity and multiscale phenomena. To speed up
    large-scale computations, a process known as downscaling is employed, which generates
    high-fidelity approximate solutions from their low-fidelity counterparts. In this
    paper, we propose a novel Physics-Guided Diffusion Model (PGDM) for downscaling.
    Our model, initially trained on a dataset comprising low-and-high-fidelity paired
    solutions across coarse and fine scales, generates new high-fidelity approximations
    from any new low-fidelity inputs. These outputs are subsequently refined through
    fine-tuning, aimed at minimizing the physical discrepancies as defined by the
    discretized PDEs at the finer scale. We evaluate and benchmark our model's performance
    against other downscaling baselines in three categories of nonlinear PDEs. Our
    numerical experiments demonstrate that our model not only outperforms the baselines
    but also achieves a computational acceleration exceeding tenfold, while maintaining
    the same level of accuracy as the conventional fine-scale solvers.
- poster: 140320
  presenter: "Xu, Jason"
  email: "f75f3c23c136fb96"
  title: "MS87: Bayesian Constraint Relaxation: Distance and Divergence Penalization"
  abstract: |-
    We consider regularizing the squared distance to set-based constraints
    for several statistical tasks that can be cast as constrained optimization. These
    distance-to-set penalties are more flexible than many existing algebraic and regularization
    penalties, and often avoid drawbacks that arise from alternatives such as shrinkage.
    Moreover, these translate naturally to a flexible class of priors for incorporating
    relaxed constraints within a Bayesian framework. We show how they are amenable
    to gradient-based samplers, and derive a natural extension to (Bregman) divergence-to-set
    priors. We discuss several examples showcasing how this generalization improves
    performance by making use of information geometry of the data generating mechanism.
- poster: 140195
  presenter: "Xu, Yuesheng"
  email: "8e02ab79f33a01d9"
  title: |-
    Distributed Deep Learning Optimization Using Second-Order Methods with Dynamic
    Weight Sharing and Hessian Variance Reduction
  abstract: |-
    In recent years, deep learning has proven to be the most successful tool
    across various domains, such as computer vision and natural language processing,
    and the scalability of these models under growing data demands has become crucial.
    The increasing complexity of deep learning models and the exponential growth of
    data necessitates the distributed systems for training while managing increased
    computational loads. Nevertheless, this distributed approach faces many challenges.
    Among these are hardware limitations, computational overheads, and communication
    management. Our research introduces a method employing second-order optimization
    methods for distributed learning environments with asynchronous updates across
    worker and master nodes. We integrate dynamic weighting schemes that adjust updates
    when a worker node fails, alongside leveraging data overlap techniques to reduce
    variance for Hessian diagonal approximation.
- poster: 140162
  presenter: "Xu, Tianshi"
  email: "f31e4e7689c01759"
  title: "MS94: Hpytorch: Data-Driven Hierarchical Matrix-Based Gaussian Process"
  abstract: |-
    Gaussian processes (GPs) are a powerful tool for many applications, however,
    their performance heavily relies on selecting suitable hyperparameters. In this
    poster, we introduce HPyTorch, a scalable preconditioned GP package that addresses
    the hyperparameter selection problem while harnessing hardware acceleration capabilities
    through OpenMP and CUDA. HPyTorch is designed to exhibit robust performance across
    a wide range of hyperparameters, owing to its utilization of data-driven matrix
    routines that dynamically adjust components in response to the spectrum decay
    of the kernel matrices. Moreover, when employing gradient-based optimization methods,
    our package eliminates the need for auto-differentiation, further enhancing its
    computational efficiency. We evaluate the performance of HPyTorch on several real-world
    datasets, demonstrating that it outperforms state-of-the-art packages in terms
    of both accuracy and efficiency.
- poster: 140087
  presenter: "Xu, Zhenghao"
  email: "139528c47833ff3d"
  title: |-
    MS59: Provable Acceleration of Nesterov's Accelerated Gradient for Asymmetric
    Matrix Factorization and Linear Neural Networks
  abstract: |-
    We study the convergence rate of first-order methods for the asymmetric
    matrix factorization (MF) problem, which is a prototype of a broad class of nonconvex
    optimization problems. Specifically, for a rank-$r$ matrix $\mathbf{A}\in\mathbb{R}^{m\times
    n}$, we show that gradient descent (GD) can find $\mathbf{X}_T\in\mathbb{R}^{m\times
    d}$ and $\mathbf{Y}_T\in\mathbb{R}^{n\times d}$ satisfying $\Vert \mathbf{X}_T\mathbf{Y}_T^\top-\mathbf{A}\Vert
    _\mathrm{F}\leq\epsilon\Vert \mathbf{A}\Vert _\mathrm{F}$ in $T=O(\kappa^2\log\frac{1}{\epsilon})$
    iterations with high probability, where $d\geq r$ and $\kappa$ denotes the condition
    number of $\mathbf{A}$. Furthermore, we show that an $O(\kappa\log\frac{1}{\epsilon})$
    accelerated rate can be attained by Nesterov's accelerated gradient (NAG) method.
    Different from standard balanced initialization, we adopt an unbalanced initialization.
    Our initialization and analysis can be extended to linear neural networks, where
    we show that NAG can attain a linear convergence rate. In particular, our width
    requirement only scales with the rank of the output matrix, while previous results
    achieving the same rate require excessive widths that depend on the condition
    number of the data matrix.
- poster: 139519
  presenter: "Xu, Yiming"
  email: "8ff00c12461c1f77"
  title: "MS39: Hybrid Least Squares"
  abstract: |-
    The efficient computation of conditional expectations is a central task
    in various data science problems across statistics, machine learning, scientific
    computing, and stochastic simulation. A common goal in these areas concerns the
    rapid assessment of conditional expectations across varying conditioning parameters,
    possibly infinitely many. Motivated by problems in computational finance, we introduce
    a surrogate-based method to approximate conditional expectation functions using
    least squares, with a focus on high-dimensional and noisy settings. Our approach
    integrates recent concepts from randomized function approximation with classical
    techniques from optimal experimental design, providing both theoretical guarantees
    and computational efficiency. Additionally, we propose an adaptive random subspace
    tailored for least-square approximation in the setting of stochastic simulation.
    Through an example of estimating the price of a basket option and employing it
    in downstream tasks like model calibration, we demonstrate the effectiveness of
    the proposed method in addressing real challenges in computational finance.
- poster: 139762
  presenter: "Xu, Zihan"
  email: "4723b9b9abfb1d7a"
  title: |-
    Extending Statistics-Informed Neural Network to Multidimensional Stochastic
    Processes
  abstract: |-
    The statistics-informed neural network (SINN) has been proposed as a machine-learning-based
    stochastic trajectory generator. SINN is built on a deterministic recurrent neural
    network that takes an input stream of white noise sequences to generate an ensemble
    of stochastic trajectories that has statistical properties similar to the original
    stochastic dynamics. While the capabilities of SINN have been demonstrated using
    various one-dimensional non-Markovian processes, for this methodology to be a
    promising tool for real applications (e.g., via surrogate modeling), it is required
    to extend SINN to reproduce multidimensional stochastic processes. In this presentation,
    we present our efforts in this regard. First, since many cross terms are to be
    included in the total loss function to capture the complex statistical properties
    of a multidimensional process, we employ a self-adaptive loss-balanced technique
    to effectively balance various loss terms. Second, since the estimation of a multidimensional
    probability density function (PDF) is computationally inefficient, instead of
    adding these to the total loss function, we include a set of one-dimensional PDFs
    of some linear combinations of component processes. Third, we investigate the
    impact of the dimensionality of the input white noise sequence on the performance
    of SINN. Lastly, we perform simulation studies using the Langevin dynamics of
    the Fermi-Pasta-Ulam chain and the kinetic Monte Carlo simulation of surface chemistry.
- poster: 139731
  presenter: "Xu, Anran"
  email: "45978b0e1e37b81b"
  title: "Leveraging Neural Fields for Geophysical Inverse Problems"
  abstract: |-
    The recent surge in test time learning (TTL) has garnered substantial
    attention from researchers, particularly in the context of incorporating machine
    learning algorithms into the inversion process. The deep image prior (DIP) method
    and coordinate-based representations (e.g., neural fields) have shown that neural
    networks (NN), without any prior learning, can produce good inversion results.
    In this work, we will discuss the progress in utilizing neural fields in the geophysical
    inverse problems. Neural fields use neural networks to map a coordinate to the
    corresponding physical property value at that coordinate. We formulate the inverse
    problem in terms of the NN weights, which allows us to take advantage of searching
    over the high-dimensional space. Furthermore, parameterizing the inverse problems
    in a continuous setting naturally introduces smoothing effects. We demonstrate
    the use of neural fields in seismic tomography inversions and direct current resistivity
    inversions. The results show that this TTL approach can eliminate unwanted artifacts
    in the recovered subsurface physical property model caused by the sensitivity
    of the survey and physics. We also find that our results are better than the conventional
    inversion results in some cases in terms of the recovery of the boundaries and
    physical property values of the main targets. Our work illustrates that the inductive
    bias brought by neural fields can be beneficial in geophysical inversion.
- poster: 139621
  presenter: "Xu, Weihang"
  email: "1f5bd97903ebb274"
  title: |-
    MS59: Over-Parameterization Exponentially Slows Down Gradient Descent for
    Learning a Single Neuron
  abstract: |-
    We revisit the problem of learning a single neuron with ReLU activation
    under Gaussian input with square loss.  We particularly focus on the over-parameterization
    setting where the student network has $n\ge 2$ neurons.  We prove the global convergence
    of randomly initialized gradient descent with a $O\left(T^{-3}\right)$ rate.  This
    is the first global convergence result for this problem beyond the exact-parameterization
    setting ($n=1$) in which the gradient descent enjoys an $\exp(-\Omega(T))$ rate.  Perhaps
    surprisingly, we further present an $\Omega\left(T^{-3}\right)$ lower bound for
    randomly initialized gradient flow in the over-parameterization setting.  These
    two bounds jointly give an exact characterization of the convergence rate and
    imply, for the first time, that $ over-parameterization can exponentially slow
    down the convergence rate $.  To prove the global convergence, we use a three-phase
    structure to analyze GD dynamics. Along the way, we prove gradient descent automatically
    balances student neurons, and use this property to deal with the non-smoothness
    of the objective function. To prove the convergence rate lower bound, we construct
    a novel potential function that characterizes the pairwise distances between the
    student neurons.  We show this potential function converges slowly, which implies
    the slow convergence rate of the loss function.
- poster: 139948
  presenter: "Yadav, Umesh"
  email: "4d1eed279ab20760"
  title: |-
    MS74: Decoding Confidence and Accuracy: An Analysis of Llm Performance on
    Leetcode Challenges
  abstract: "This study introduces an evaluation framework that employs stealth assessment techniques to examine the performance of advanced large language models (LLMs) like GPT-4 and Gemini Advanced. The models are tested against a variety of coding problems sourced from LeetCode competitions, with the special element that they are unaware of the specific aspects being assessed—particularly, their strategic resource management and problem-solving tactics. This method ensures a true demonstration of the models' abilities in handling complex coding tasks. The study uses a structured interaction protocol, which methodically presents these challenges to the LLMs, to analyze their decision-making processes in real-time. This method not only showcases the LLMs' proficiency in code generation under varying conditions but also clarifies how they optimize the use of available hints and manage their hypothetical budgets when solutions are not immediately evident. The results from this investigation provide insightful revelations into the autonomous strategies of LLMs, highlighting their flexibility and potential for practical applications without the confounding effect of their awareness of the evaluation."
- poster: 140188
  presenter: "Yan, Runze"
  email: "df514f42cc6bef73"
  title: |-
    MS34: Enabling Pre-Shock State Detection Using Electrogram Signals from
    Implantable Cardioverter-Defibrillators
  abstract: |-
    Early detection of ventricular arrhythmias in patients with implantable
    cardioverter-defibrillators (ICDs) remains a critical challenge in cardiac care.
    Our study introduces an innovative framework integrating metric learning, prototype
    learning, and few-shot learning to enhance the accuracy of predicting ICD shocks.
    We analyzed electrogram (EGM) data from 326 participants, focusing on EGM segments
    recorded before shock events and during normal conditions. Our approach utilizes
    a Siamese neural network equipped with LSTM units, which utilizes both triplet
    and pair losses to refine the learning process. This strategy allows the network
    to generate distinct embeddings that accurately distinguish between pre-shock
    and normal EGM signals. For classification, we adopted k-means clustering to identify
    prototypes representing the two states, enhancing our model's ability to predict
    impending arrhythmias. The evaluation of our framework showed a robust performance,
    achieving an F1 score of 0.87, a sensitivity of 0.97, and a precision of 0.79.
    This work not only advances predictive analytics in the cardiac field but also
    supports improved clinical decision-making in ICD management, potentially enhancing
    patient safety and outcomes.
- poster: 139684
  presenter: "Yang, Kaylee"
  email: "1b43758c04257982"
  title: "MS33: Optimal Score Estimation Via Empirical Bayes"
  abstract: |-
    We study the problem of estimating the score function of an unknown probability
    distribution $\rho^*$ from $n$ independent and identically distributed observations
    in $d$ dimensions. Assuming that $\rho^*$ is subgaussian and has a Lipschitz-continuous
    score function $s^*$, we establish the optimal rate of $\tilde \Theta(n^{-\frac{2}{d+4}})$
    for this estimation problem under the loss function $\Vert \hat s - s^*\Vert ^2_{L^2(\rho^*)}$
    that is commonly used in the score matching literature, highlighting the curse
    of dimensionality where sample complexity for accurate score estimation grows
    exponentially with the dimension $d$. Leveraging key insights in empirical Bayes
    theory as well as a new convergence rate of smoothed empirical distribution in
    Hellinger distance, we show that a regularized score estimator based on a Gaussian
    kernel attains this rate, shown optimal by a matching minimax lower bound. We
    also discuss the implication of our theory on the sample complexity of score-based
    generative models.
- poster: 140023
  presenter: "Yang, Yahong"
  email: "c393749e59011ee0"
  title: |-
    MS60: Homotopy Relaxation Training Algorithms for Infinite-Width Two-Layer
    Relu Neural Networks
  abstract: ""
- poster: 139662
  presenter: "Yang, Nicole"
  email: "6245905d1ff7a7c1"
  title: "MS35: Nonlinear Filtering in Stochastic Dynamical Systems"
  abstract: |-
    We present a nonlinear filtering approach for solving inverse problems
    sequentially in stochastic dynamical systems. By utilizing a Bayesian framework,
    we construct a generative modeling method to identify the state trajectories from
    noisy data in a robust manner.
- poster: 139966
  presenter: "Yang, Tiantian"
  email: "74d4863bd81421ae"
  title: |-
    Interpretable Graph Neural Networks for Disease Classification on High-Dimensional
    Omics Data
  abstract: |-
    Omics data play crucial roles in exploring disease pathways, forecasting
    clinical outcomes, and gaining insights for disease classification. However, the
    significant challenge of dealing with a relatively small number of samples and
    a large number of features complicates the development of predictive models for
    omics data analysis, with inherent sparsity in biological networks and the presence
    of unknown feature interactions adding further complexities. The advent of Graph
    Neural Networks (GNN) helps alleviate these challenges by incorporating known
    functional relationships over a graph. However, many existing GNN models utilize
    graphs either from existing networks or the generated ones alone, which limits
    model effectiveness. To overcome this restriction, we proposed an innovative GNN
    model that integrates information from both externally and internally generated
    feature graphs. We extensively tested the model through simulations and real data
    applications, confirming its superior performance in classification tasks compared
    to existing state-of-the-art baseline models. Furthermore, our GNN model can select
    features with meaningful interpretations in the biomedical context.
- poster: 140181
  presenter: "Yang, Xianjin"
  email: "7adcaf905bacb6bd"
  title: |-
    MS37: Forward-Backward Algorithm for Functions with Locally Lipschitz Gradient:
    Applications to Mean Field Games.
  abstract: |-
    In this paper, we provide a generalization of the forward-backward splitting
    algorithm for minimizing the sum of a proper convex lower semicontinuous function  and
    a differentiable convex function whose gradient satisfies a locally Lipschitztype
    condition. We prove the convergence of our method and derive a linear  convergence
    rate when the differentiable function is locally strongly convex. We  recover
    classical results in the case when the gradient of the differentiable function
    is globally Lipschitz continuous and an already known linear convergence  rate
    when the function is globally strongly convex. We apply the algorithm to  approximate
    equilibria of variational mean field game systems with local couplings. Compared
    with some benchmark algorithms to solve these problems, our  numerical tests show
    similar performances in terms of the number of iterations  but an important gain
    in the required computational time.
- poster: 140169
  presenter: "Yang, Qian"
  email: "931fc1cc81be58ed"
  title: "MS52: Extrapolation in Data-Driven Learning of Complex Reactive Processes"
  abstract: |-
    Reactive processes are complicated nonlinear dynamical systems that are
    difficult to model and interpret. While ab initio molecular dynamics (MD) is a
    key computational method that can be used to interrogate reactive chemistry, many
    challenges remain due to its computational limitations in time and length scales.
    In this work, we first discuss how it is possible to use the wealth of data generated
    by a few expensive MD simulations to statistically learn fast kinetic Monte Carlo
    (KMC) models or evolutionary equations describing complex chemistry. Then, we
    show how learning a family of reaction networks under different conditions such
    as temperature and pressure can lead to models that provide more accurate predictions
    of dynamics in extrapolated regimes. Finally, we demonstrate how our method can
    be used to predict the evolution of dislocation densities in material at different
    shock velocities, using a model trained on only a few MD simulations.
- poster: 139503
  presenter: "Yang, Ruiyi"
  email: "7c982c7aeac66fcf"
  title: "MS64: Gaussian Process Regression under Computational and Epistemic Misspecification"
  abstract: "Gaussian process regression is a classical kernel method for function estimation and data interpolation. In large data applications, computational costs can be reduced using low-rank or sparse approximations of the kernel. This poster investigates the effect of such kernel approximations on the interpolation error. We introduce a unified framework to analyze Gaussian process regression under important classes of computational misspecification: Karhunen-Loève expansions that result in low-rank kernel approximations, multiscale wavelet expansions that induce sparsity in the covariance matrix, and finite element representations that induce sparsity in the precision matrix. Our theory also accounts for epistemic misspecification in the choice of kernel parameters."
- poster: 140540
  presenter: "Yang, Sichen"
  email: "7ecf026eed7a3e60"
  title: |-
    MS55: Nonlinear Model Reduction for Slow-Fast Stochastic Systems Near Unknown
    Invariant Manifolds
  abstract: "We introduce a model reduction technique for high-dimensional stochastic systems having a low-dimensional invariant effective manifold with slow dynamics, and high-dimensional, large fast modes. Given only access to short bursts of simulation, we design an on-the-fly consistent algorithm exploring the effective state space efficiently. This construction enables fast, efficient simulation of the effective dynamics that averages out fast modes, plus estimation of crucial features of such dynamics, including the stationary distribution, identification of metastable states, and residence times and transition rates between them. We also implement our algorithm on three model systems: “pinched sphere”, “butane model”, and “oscillating half-moons”, where in particular, the last example has large nonlinear fast modes. The results demonstrate the accuracy, efficiency, and robustness of our method. This work is published in the Journal of Nonlinear Science (JNS): https://link.springer.com/article/10.1007/s00332-023-09998-8"
- poster: 140536
  presenter: "Yang, Yitong"
  email: "76e3d6fe76cc854a"
  title: |-
    MS53: Improved Reconstruction Speed for 5D Free Running Motion Resolved
    Using a Variable Projection Augmented Lagrangian (vpal) Method
  abstract: |-
    5D free-running cardiac magnetic resonance (CMR) imaging is a free-breathing
    CMR method that produces CT-quality images with high spatial and temporal resolution
    and without radiation. 5D free-running CMR uses compressed sensing-based reconstruction
    to recover the underlying image data from highly under-sampled k-space in the
    cardiac and respiratory dimensions. However, using the state-of-the-art alternating
    direction method of multipliers (ADMM), the iterative reconstruction method still
    requires several hours on a multi-GPU cluster, which is not viable for clinical
    usage. Therefore, a more efficient reconstruction algorithm is needed. We propose
    to use the advanced numerical algorithm to reduce the reconstruction time while
    preserving image quality.  A variable projection augmented Lagrangian (VPAL) method
    for 5D motion-resolved image reconstruction was developed and compared with the
    state-of-the-art alternating direction method of multipliers (ADMM) on 15 5D free-running
    raw data sets. When compared to the ADMM method, VPAL reduced the reconstruction
    time by 60%, preserved image similarity, had equivalent ejection fraction measurements,
    and had superior radiologist ratings. This study shows that using an advanced
    numerical algorithm for highly under-sampled MR reconstruction both reduces computational
    time and results in better image quality for diagnostics, bringing 5D free-running
    imaging is closer to clinical usage.
- poster: 139611
  presenter: "Yang, Chengrun"
  email: "c53938d51f29d8f4"
  title: "MS20: Associated Poster: Large Language Models As Optimizers"
  abstract: |-
    Optimization is ubiquitous. While derivative-based algorithms have been
    powerful tools for various problems, the absence of gradient imposes challenges
    on many real-world applications. In this work, we propose Optimization by PROmpting
    (OPRO), a simple and effective approach to leverage large language models (LLMs)
    as optimizers, where the optimization task is described in natural language. In
    each optimization step, the LLM generates new solutions from the prompt that contains
    previously generated solutions with their values, then the new solutions are evaluated
    and added to the prompt for the next optimization step. We first showcase OPRO
    on linear regression and traveling salesman problems, then move on to our main
    application in prompt optimization, where the goal is to find instructions that
    maximize the task accuracy. With a variety of LLMs, we demonstrate that the best
    prompts optimized by OPRO outperform human-designed prompts by up to 8% on GSM8K,
    and by up to 50% on Big-Bench Hard tasks. Code at https://github.com/google-deepmind/opro.
- poster: 139721
  presenter: "Yao, Rentian"
  email: "e4548bad2bf5374c"
  title: "MS22: Mean-Field Variational Inference Via Wasserstein Gradient Flow"
  abstract: |-
    Variational inference, such as the mean-field (MF) approximation, requires
    certain conjugacy structures for efficient computation. These can impose unnecessary
    restrictions on the viable prior distribution family and further constraints on
    the variational approximation family. In this work, we introduce a general computational
    framework to implement MF variational inference for Bayesian models, with or without
    latent variables, using the Wasserstein gradient flow (WGF), a modern mathematical
    technique for realizing a gradient flow over the space of probability measures.
    Theoretically, we analyze the algorithmic convergence of the proposed approaches,
    providing an explicit expression for the contraction factor. We also strengthen
    existing results on MF variational posterior concentration from a polynomial to
    an exponential contraction, by utilizing the fixed point equation of the time-discretized
    WGF. Computationally, we propose a new constraint-free function approximation
    method using neural networks to numerically realize our algorithm. This method
    is shown to be more precise and efficient than traditional particle approximation
    methods based on Langevin dynamics.
- poster: 140179
  presenter: "Yavuz Ozdemir, Yagmur"
  email: "fb34722720488b38"
  title: "All Models Are Wrong But a Set of Them Is Useful"
  abstract: |-
    In real-world scenarios, prediction performance often relies on a single,
    complex model that sacrifices interpretability. However, a groundbreaking concept
    called the Rashomon set promotes the idea of using multiple equally effective
    models instead. The Sparse Wrapper Algorithm (SWAG) is a cutting-edge approach
    that blends screening and wrapper techniques to create a set of low-dimensional
    models with generalizable predictive capabilities. SWAG operates through a forward
    step process, where users select a modeling mechanism and the algorithm evaluates
    low-dimensional models. It then systematically builds larger models based on the
    best-performing ones from previous steps, resulting in a collection of models
    termed 'SWAG models.' This method's flexibility empowers practitioners to choose
    models that align with their specific needs or domain expertise without compromising
    accuracy. Furthermore, SWAG effectively addresses common data challenges such
    as missing values, outliers, and collinearity. The benefits of SWAG extend to
    decision-makers in various fields like genomics, engineering, and neurology. By
    constructing a network that illuminates attribute interactions, SWAG provides
    a deeper and more insightful perspective for informed decision-making.
- poster: 140308
  presenter: "Ye, Taiyu"
  email: "671fa9d52d0412a6"
  title: |-
    MS81: False Discovery Rate Control in Multiple Regression Models with Unobserved
    Confounders
  abstract: |-
    False discovery rate (FDR) control in large-dimensional cross-sectional
    datasets remains challenging due to confounders, especially batch effects or unobserved
    variables in genome and brain imaging datasets. We utilized the factor model to
    characterize and eliminate these unobserved confounders, thus contributing to
    FDR control. This novel statistical framework involves two steps to eliminate
    the common dependence among variables caused by confounders. Firstly, we estimated
    the Negative Control (NC) set to address false negatives. Secondly, we extracted
    information on unobserved effects from the estimated NC set and constructed pairwise
    asymptotically independent test statistics. We provided theoretical proofs for
    both steps. Additionally, numerical experiments and theoretical derivations demonstrate
    that our method's false discovery proportion (FDP) estimations converge to the
    oracle values that utilize information from the unobserved variables, guaranteeing
    FDR control.
- poster: 139582
  presenter: "Ye, Qihao"
  email: "6a9003c712eac1c"
  title: "MS56: A Model-Based Approach for Continuous-Time Policy Evaluation with Unknown Lévy Process Dynamics"
  abstract: "This research presents a framework for evaluating policies in a continuous-time setting, where the dynamics are unknown and represented by Lévy processes. Initially, we estimate the model using available trajectory data, followed by solving the associated PDE to conduct the policy evaluation. Our approach encompasses not only the conventional Brownian motion but also the non-Gaussian and heavy-tailed Lévy processes. We have developed an algorithm that demonstrates enhanced performance compared to existing techniques tailored for Brownian motion. Furthermore, we provide a theoretical guarantee regarding the error in policy evaluation given the model error. Experimental results involving both light-tailed and heavy-tailed data will be presented. This research provides a first step to continuous-time model-based reinforcement learning, particularly in scenarios characterized by irregular, heavy-tailed dynamics."
- poster: 140233
  presenter: "Yeganegi, Farhang"
  email: "cc6595e26fc96e84"
  title: |-
    Finite Volume Property: Binary Compressed Sensing With Fourier Sampling
    Matrix
  abstract: |-
    One-bit quantization with random dithering has recently found  significant
    utilization potential in statistical signal processing applications due to its
    relatively low power consumption and low implementation cost. In addition to such
    advantages, an attractive feature of one-bit analog-to-digital converters (ADCs)
    is their superior sampling rates as compared to their conventional multi-bit counterparts.
    This characteristic endows one-bit signal processing frameworks with what one
    may refer to as sample abundance. Previous literature has demonstrated that theoretical
    guarantees for one-bit compressed sensing (CS) are attainable under certain statistical
    assumptions concerning the sensing matrix, such as the restricted isometry-type
    assumption. However, such guarantees are absent when considering the Fourier measurement
    matrix, which is more relevant in practical applications. In this paper, for the
    first time in the literature, we provide the theoretical guarantee for one-bit  CS
    in the case of the Fourier measurement matrix by introducing the concept of Finite-volume
    property (FVP). As demonstrated, within the FVP framework, we can establish universal
    convergence for Fourier one-bit CS with a high probability.
- poster: 140227
  presenter: "Yeganegi, Farhang"
  email: "cc6595e26fc96e84"
  title: |-
    OptiBridge: Multi-Scale Multi-Shift Bridging for Conditioning Optimization
    Landscapes
  abstract: |-
    This paper introduces OptiBridge, a novel optimization methodology designed
    to address the challenges of navigating complex optimization landscapes characterized
    by numerous local optima, such as those found in NP-hard problems. By implementing
    a multi-scale, multi-shift strategy, OptiBridge significantly enhances the attraction
    domains of superior local optima while concurrently diminishing the prevalence
    of suboptimal local solutions. This will help with the performance of off-the-shelf
    local optimizers, such as first-order methods. We also introduce BridgeNet, a
    neural network architecture based on OptiBridge that learns the best shifts and
    scales for the OptiBridge objective to help produce an enhanced subset of local
    optima. Through theoretical analysis and illustrative examples, we demonstrate
    the efficacy of OptiBridge in improving the outcomes of local optimizers across
    various problem domains, particularly highlighting its potential in dominant frequency
    estimation and training of lightweight neural networks.
- poster: 140069
  presenter: "Yi, Chugang"
  email: "fd92686a8d7a1e46"
  title: |-
    MS43: Discovery of Governing Equations Using Sparse and Low-Rank Tensor
    Regression
  abstract: |-
    Data-driven methods for identifying governing equations are gaining popularity
    by enabling discovery without first-principle derivation. In this paper, we present
    a novel and efficient tensor regression framework designed to identify unknown
    governing equations from data. Our method excels in explicitly discovering governing
    equations and remains robust and efficient, even when utilizing an extensive dictionary.
    We demonstrate the effectiveness of our approach with numerical examples involving
    one-dimensional PDEs and high-dimensional systems. Additionally, we provide theoretical
    convergence guarantees and estimation error for our algorithm.
- poster: 139604
  presenter: "Yin, Minglang"
  email: "fcecec2d10533d05"
  title: |-
    MS57: Learning the Geometry-Dependent Solution Operators of Partial Differential
    Equations
  abstract: |-
    Solving partial differential equations (PDEs) using numerical methods
    is a ubiquitous task in engineering and medicine. However, the computational costs
    can be prohibitively high when many-query evaluations of PDE solutions on multiple
    geometries are needed. Although a school of studies in artificial intelligence
    (AI) focused on learning PDE operators on a fixed geometry, frameworks that can
    alleviate the computational burdens on multiple geometries are yet to be developed.
    We aim to address the challenge by introducing a generic AI framework, named DIffeomorphic
    Mapping Operator learNing (DIMON), which allows AI to learn geometry-dependent
    solution operators of different types of PDEs on a wide variety of geometries.
    We present several examples to demonstrate the performance of the framework in
    learning both static and time-dependent PDEs on parameterized and non-parameterized
    domains; these include solving the Laplace equations, reaction-diffusion equations,
    and a system of multiscale PDEs that characterizes the electrical propagation
    on thousands of personalized heart digital twins. Accurate and efficient, DIMON
    can reduce the computational costs of solution approximations on multiple geometries
    from hours to seconds with significantly less computational resources, thus ushering
    in fast prediction of PDE solutions with AI on multiple geometries, and advancing
    the application of AI in engineering and precision medicine.
- poster: 139783
  presenter: "Yousefpour, Amin"
  email: "c0c84b257b584755"
  title: |-
    Neural Networks with Kernel-Weighted Corrective Residuals: A Mesh-Independent
    Approach for Inverse Design
  abstract: |-
    Topology optimization is a challenging inverse problem as it is high-dimensional
    and usually constrained by partial differential equations (PDEs) and additional
    inequalities. Recently, Physics-Informed Neural Networks (PINNs) have been employed
    to simplify this process, yet they struggle to meet all design requirements and
    their effectiveness largely depends on the network configuration. To address these
    challenges, we leverage neural networks (NNs) with kernel-weighted Corrective
    Residuals (CoRes) for topology optimization. We have recently developed NN-CoRes
    to integrate the strengths of kernel methods and deep neural networks. Our method
    is inherently mesh-independent, completely avoiding domain discretization. In
    this presentation, we demonstrate how it significantly helps in (1) satisfying
    equality constraints in the design problem, (2) minimizing gray areas that are
    not favorable in real-world applications, and (3) simplifying the inverse design
    by reducing the sensitivity of neural networks to factors such as random initialization,
    architecture type, and choice of optimizer.
- poster: 139598
  presenter: "Yu, Yaodong"
  email: "12dfe95b6975af6f"
  title: "MS46: Differentially Private Representation Learning Via Image Captioning"
  abstract: |-
    Differentially private (DP) machine learning is considered the gold-standard
    solution for training a model from sensitive data while still preserving privacy.
    However, a major barrier to achieving this ideal is its sub-optimal privacy-accuracy
    trade-off, which is particularly visible in DP representation learning. Specifically,
    it has been shown that under modest privacy budgets, most models learn representations
    that are not significantly better than hand-crafted features. In this work, we
    show that effective DP representation learning can be done via image captioning
    and scaling up to internet-scale multimodal datasets. Through a series of engineering
    tricks, we successfully train a DP image captioner (DP-Cap) on a 233M subset of
    LAION-2B from scratch using a reasonable amount of computation, and obtaining
    unprecedented high-quality image features that can be used in a variety of downstream
    vision and vision-language tasks. For example, under a privacy budget of e = 8,
    a linear classifier trained on top of learned DP-Cap features attains 65.8% accuracy
    on ImageNet-1K, considerably improving the previous SOTA of 56.5%. Our work challenges
    the prevailing sentiment that high-utility DP representation learning cannot be
    achieved by training from scratch.
- poster: 139862
  presenter: "Yu, Jiajia"
  email: "909bb69a281dd597"
  title: "MS11: Solving High-Dimensional Mean-Field Games with Fictitious Play"
  abstract: |-
    Mean-field games study the equilibrium of a game involving infinitely
    many players. They are shown to have rich connections between reinforcement learning
    and flow-based generative models. A mean-field game (MFG) system comprises a backward-in-time
    Hamilton-Jacobi-Bellman (HJB) equation from single players' optimal control problems
    and a forward-in-time Fokker-Planck (FP) equation describing the evolution of
    all players. The forward-backward structure makes the computation of MFG difficult,
    and many existing works in solving high-dimensional MFGs are based on its variational
    formulation, which is a subset of MFGs. In this work, we explore the effective
    implementation of fictitious play to solve MFGs. Fictitious play is a concept
    with a rich history in game theory and was first introduced in the context of
    MFGs in [Cardaliaguet and Hadikhanloo, 2017]. The idea involves alternately solving
    the optimal control problem and simulating population evolution. By decoupling
    the system, we are able to handle a larger class of MFG problems.
- poster: 140257
  presenter: "Yu, Yue"
  email: "657156a9cc1d4b5b"
  title: "MS31: Toward Foundation Material Model with Nonlocal Attention Operator"
  abstract: |-
    Unlike traditional machine learning tasks,  physical systems in real-world
    applications feature scarce measurements and changing hidden states, and therefore
    calls for an automated self-adaptive mechanism capable of evolving continuously
    with the current state of system. To this end, we consider the learning of material
    responses as an exemplar complex physical system modeling problem, and investigate
    the self-adaptive capability of attention mechanisms toward developing a foundation
    material model. Specifically, we first show that the attention mechanism is equivalent
    to a nonlocal neural operator with data-dependent kernels, such that  enables
    the automatic updating of the evolving physical properties via the attention matrix.
    We then propose Nonlocal Attention Operator (NAO), which provides a forward model
    in the form of nonlocal constitutive law, ensuring adherence to fundamental physical
    laws. In a wide variety of scientific applications, including constitutive modeling
    of material deformation, stress wave propagation, and digital twin modeling, we
    empirically demonstrate the advantages of NAO over the baseline neural operators
    and state-of-the-art methods in capturing the change of hidden physical states.
- poster: 139633
  presenter: "Yu, Annan"
  email: "9b7a725a6f35f5e6"
  title: "MS1: Robustifying Long-Memory State-Space Models via Hankel Operator Theory"
  abstract: |-
    State-space models (SSMs) that utilize linear, time-invariant (LTI) systems
    are known for their effectiveness in learning long sequences. However, these models
    typically face several challenges: (i) they require specifically designed initializations
    of the system matrices to achieve state-of-the-art performance, (ii) they require
    training of state matrices on a logarithmic scale with very small learning rates
    to prevent instabilities, and (iii) they require the model to have exponentially
    decaying memory in order to ensure an asymptotically stable LTI system. To address
    these issues, we view SSMs through the lens of Hankel operator theory, which provides
    us with a unified theory for the initialization and training of SSMs. Building
    on this theory, we develop a new parameterization scheme, called HOPE, for LTI
    systems that utilizes Markov parameters within Hankel operators. This approach
    allows for random initializations of the LTI systems and helps to improve training
    stability, while also provides the SSMs with non-decaying memory capabilities.
    Our model efficiently implements these innovations by nonuniformly sampling the
    transfer functions of LTI systems, and it requires fewer parameters compared to
    canonical SSMs. When benchmarked against HiPPO-initialized models such as S4 and
    S4D, an SSM parameterized by Hankel operators demonstrates improved performance
    on Long-Range Arena (LRA) tasks.
- poster: 140296
  presenter: "Yuan, Bo"
  email: "7e8bf30e758ad1ec"
  title: "MS76: Improved Dimension Dependence of a Proximal Algorithm for Sampling"
  abstract: |-
    In this proposal, I will introduce a novel sampling algorithm based on
    the proximal sampler framework introduced by Lee et al. (2021). This new approximate
    proximal sampler achieves superior complexity bounds across almost all the settings,
    such as strong log-concavity, log-concavity, functional inequalities, as well
    as those involving semi-smooth or composite potentials. The core of our algorithm
    is an inexact realization of the restricted Gaussian oracle (RGO) based on approximate
    rejection sampling. The state-of-the-art complexity bounds are largely established
    by a new concentration inequality for semi-smooth functions over Gaussian distributions.
    For strongly log-concave distributions, our method has complexity bound $\tilde{\mathcal{O}}(\kappa
    d^{1/2})$ without warm start, better than the minimax bound for MALA.
- poster: 139850
  presenter: "Zanjani Foumani, Zahra"
  email: "4bbcb040e8eba5e5"
  title: "Constrained Cost-Aware Multi-Fidelity Bayesian Optimization"
  abstract: |-
    Bayesian optimization (BO) is a widely used technique for finding materials
    with unprecedented properties. However, relying solely on expensive high-fidelity
    (HF) sources can inflate optimization expenses in complex scenarios. To address
    this challenge and incorporate known and unknown constraints, we introduce a novel
    constrained cost-aware multi-fidelity BO (C2-MFBO) framework which a few novelties.
    Firstly, it uses manifold-embedded Gaussian process (GP) for emulation which handles
    mixed input spaces and models source-dependent noise and global trends. Secondly,
    it leverages a composite acquisition function (AF) that quantifies the information
    value of high- and low-fidelity sources differently and also accommodates source-dependent
    constraints. Lastly, it pioneers an optimization-based stop condition instead
    of immature stopping after a fixed number of iterations. Through analytical and
    real-world examples, we will demonstrate the benefits of our approach which is
    publicly available via the GP+ package in Python.
- poster: 139918
  presenter: "Zhang, Zheng"
  email: "31eb1a96ff209a53"
  title: "MS77: Self Supervised Representation Learning of Text-Attributed Graphs"
  abstract: |-
    The emergence of pre-trained models has revolutionized various AI research
    domains, such as natural language processing and computer vision. They function
    as a foundation model to generalize across unseen domains to enable label-scarce
    tasks such as zero-shot and few-shot predictions. Although recently there has
    been growing interests in developing pre-trained graph models, their generalization
    capabilities, especially in unseen domains, remain limited. We aim to build a
    unified graph representation learning framework designed to enable effective transfer
    across different graph domains in a label-scarcity manner. We first propose a
    graph-to-text transformation technique that represent each node's neighborhood
    structure in a hierarchical document layout. Second, we propose a self-supervised
    learning framework to align the textual representations of these documents with
    aggregated neighborhood information derived from GNNs, integrating the representations
    within the textual domain. To address scalability and efficiency challenges in
    the pre-training stage, particularly when dealing with large neighborhood sizes
    which results in long context size of documents, we propose a Metropolis-Hastings
    random walk sampling method to approximate neighborhood structures. Extensive
    experiments on eight real-world datasets demonstrate that our framework possesses
    strong zero-shot and few-shot prediction capabilities, effectively generalizing
    the pre-trained model to unseen graph datasets.
- poster: 140029
  presenter: "Zhang, Tianhao"
  email: "5b91bb00afd43b5"
  title: |-
    MS60: Provable In-Context Learning of Pdes with Transformers: The Linear
    Elliptic Case
  abstract: ""
- poster: 139571
  presenter: "Zhang, Ruixun"
  email: "a446c0732cba98b5"
  title: "On Consistency of Signatures Using Lasso"
  abstract: "Signature transforms are iterated path integrals of continuous and discrete-time time series data, and their universal nonlinearity linearizes the problem of feature selection. This paper revisits some statistical properties of signature transform under stochastic integrals with a Lasso regression framework, both theoretically and numerically. Our study shows that, for processes and time series that are closer to Brownian motion or random walk with weaker inter-dimensional correlations, the Lasso regression is more consistent for their signatures defined by Itô integrals; for mean reverting processes and time series, their signatures defined by Stratonovich integrals have more consistency in the Lasso regression. We provide applications in option pricing. Our findings highlight the importance of choosing appropriate definitions of signatures and stochastic models in statistical inference and machine learning."
- poster: 140172
  presenter: "Zhang, Yasi"
  email: "28d66be28253e7b6"
  title: |-
    MS32: Solving Linear Inverse Problems with Flows by Adaptive Corrupted Trajectory
    Matching
  abstract: |-
    Flow-based generative models have attracted significant attention for
    their simplicity and superior performance in the fields of images, video, and
    speech. By leveraging the instantaneous changes of variables formula, one can
    compute the probability of an input image under the ODE framework of flow-based
    models. However, it remains unclear how to use this probability to solve inverse
    problems given some observed measurements. A major obstacle preventing the application
    of flow-based models to solve inverse problems is the slow computation of image
    probability, as the process requires multiple ODE propagations, not to mention
    the gradient descent process. In our paper, we propose an adaptive corrupted trajectory
    matching method as an alternative to initial point optimization to expedite the
    solving process. Theoretically, we demonstrate that solving noisy MAPs at different
    timesteps is approximately equivalent to solving the global MAP. Extensive results
    on various inverse problems, including super-resolution, deblurring, and inpainting
    on datasets such as CelebAHQ, LSUN, and CT, demonstrate the excellence of our
    algorithm.
- poster: 139667
  presenter: "Zhang, Yani"
  email: "8e9e39b020340d34"
  title: "Deep Neural Networks Learn Cellular Automaton Rules in Many-Valued Logic"
  abstract: |-
    We develop a theory characterizing the fundamental capability of deep
    neural networks to learn, from evolution traces, the logical rules governing the
    behavior of cellular automata (CA). This is accomplished by first establishing
    a novel connection between CA and Lukasiewicz propositional logic. While binary
    CA have been known for decades to essentially perform operations in Boolean logic,
    no such relationship exists for general CA. We demonstrate that many-valued (MV)
    logic, specifically Lukasiewicz propositional logic, constitutes a suitable language
    for characterizing general CA as logical machines. This is done by interpolating
    CA transition functions to continuous piecewise linear functions, which, by virtue
    of the McNaughton theorem, yield formulae in MV logic characterizing the CA. Recognizing
    that deep rectified linear unit (ReLU) networks realize continuous piecewise linear
    functions, it follows that these formulae are naturally extracted from CA evolution
    traces by deep ReLU networks. A corresponding algorithm together with a software
    implementation is provided. As the algorithm applies to networks with general,
    in particular also real-valued, weights, it can be used to extract logical formulae
    from deep ReLU networks trained on data.
- poster: 139712
  presenter: "Zhang, Zezhong"
  email: "194d6062d47b9369"
  title: |-
    MS23: Exact Likelihood Score Estimation for Bayesian Sampling with Diffusion
    SDE under Gaussian Mixture Assumption
  abstract: |-
    Diffusion models have recently gained popularity as potent generative
    models for Bayesian sampling, which typically involves adding the scaled likelihood
    score in its generative process. However, due to the analytical intractability
    of the exact likelihood score, traditional diffusion models often rely on very
    crude approximations, controlled by some hyperparameters, which can potentially
    lead to biases in the posterior samples. In this work, we derive an exact expression
    for the likelihood score under a Gaussian mixture assumption and propose an algorithm
    for iteratively estimating the likelihood score. Extensive numerical experiments
    demonstrate that the samples generated using the exact likelihood score from our
    method converge to the true posterior distribution.
- poster: 140205
  presenter: "Zhang, Yifan"
  email: "4f89852285ac0a73"
  title: |-
    MS21: Learning Conditionally Independent Nonparametric Mixture Models Through
    Implicit Tensor Decomposition
  abstract: |-
    We present an alternating least squares type numerical optimization scheme
    to estimate conditionally-independent mixture models in Rn, with minimal additional
    distributional assumptions. Following the method of moments, we tackle a coupled
    system of low-rank tensor decomposition problems. The steep costs associated with
    high-dimensional tensors are avoided, through the development of specialized tensor-free
    operations. Numerical experiments illustrate the performance of the algorithm,
    and its applicability to various models and applications. In many cases the results
    exhibit improved reliability over the expectation-maximization algorithm, with
    similar time and storage costs. We also provide some supporting theory, establishing
    identifiability and local linear convergence.
- poster: 139931
  presenter: "Zhang, Benjamin"
  email: "b532ab917086236e"
  title: "MS37: A Mean-Field Games Laboratory for Generative Modeling"
  abstract: |-
    We demonstrate the versatility of mean-field games (MFGs) as a mathematical
    framework for explaining, enhancing, and designing generative models. We establish
    connections between MFGs and major classes of flow and diffusion-based generative
    models by deriving continuous-time normalizing flows, score-based models, and
    Wasserstein gradient flows through different choices of particle dynamics and
    cost functions. Furthermore, we study the mathematical structure and properties
    of each generative model by examining their associated MFG's optimality condition,
    which consist of a set of coupled forward-backward nonlinear partial differential
    equations. The optimality conditions of MFGs also allow us to introduce HJB regularizers
    for enhanced training of a broad class of generative models. We present this framework
    as an MFG laboratory which serves as a platform for revealing new avenues of experimentation
    and invention of generative models.
- poster: 140185
  presenter: "Zhang, Ziheng"
  email: "85d02fd288b4baf3"
  title: |-
    MS5: Geometric Mcmc for Inference of Coseismic Model for 2011 Tohoku-Oki
    Earthquake Using Derivative-Informed Neural Operators
  abstract: |-
    We consider the large-scale nonlinear Bayesian inverse problem (BIP) of
    inferring heterogeneous material properties in coseismic elastic models of earthquakes.
    Due to the difficulties of (i) the high cost of PDE solutions, (ii) the high dimensionality
    of the material properties, and (iii) complex posterior geometry, sampling from
    the posterior distribution using methods such as MCMC has been intractable for
    this problem. In this work, we propose to overcome the difficulties by training
    a derivative-informed neural operator (DINO) surrogate of the parametric PDE model,
    which achieves high accuracy in both approximating the PDE solution map and its
    Fr\'echet derivative at a low and tractable training cost compared to conventional
    operator learning methods. The trained DINO surrogate is employed in a geometric
    MCMC algorithm to provide scalable and efficient approximate posterior sampling
    exploiting accurate surrogate posterior local geometry. We demonstrate this approach's
    efficacy in inferring the earth's heterogeneous material properties (shear modulus)
    below Japan, using earth surface deformation data from the 2011 Tohoku event.
- poster: 139887
  presenter: "Zhang, Zezhong"
  email: "194d6062d47b9369"
  title: |-
    MS58: Improving the Ensemble Score Filter for Sparse and Nonlinear Data
    Assimilation Problem by Compositing State Dynamics to Observation Function
  abstract: |-
    Recently, the Ensemble Score Filter (EnSF) was introduced to tackle high-dimensional
    nonlinear data assimilation(DA) problems. The primary limitation of EnSF is its
    tendency to underestimate the correlation structure within the filtering distribution,
    rendering it less effective for DA problems with sparse observations. This issue
    arises because the updates to unobserved dimensions rely heavily on their correlation
    with observed dimensions. To overcome this, we composite the state dynamics with
    the observation function in the Bayesian sampling step of EnSF. This composition
    creates non-zero gradients and updates to the unobserved dimensions without the
    need for a strong prior correlation structure. Moreover, we employ automatic differentiation
    and likelihood bias correction techniques to enhance the efficiency of Bayesian
    sampling in EnSF.  Extensive numerical experiments demonstrate that our method
    maintains accuracy and effectiveness even under conditions of sparse observation.
- poster: 140298
  presenter: "Zhang, Matthew"
  email: "5b47128563db61a1"
  title: "MS76: Sampling from the Mean-Field Stationary Distribution"
  abstract: |-
    We study the complexity of sampling from the stationary distribution
    of a mean-field SDE, or equivalently, the complexity of minimizing a functional
    over the space of probability measures which includes an interaction term. Our
    main insight is to decouple the two key aspects of this problem: (1) approximation
    of the mean-field SDE via a finite-particle system, via uniform-in-time propagation
    of chaos, and (2) sampling from the finite-particle stationary distribution, via
    standard log-concave samplers. Our approach is conceptually simpler and its flexibility
    allows for incorporating the state-of-the-art for both algorithms and theory.
    This leads to improved guarantees in numerous settings, including better guarantees
    for optimizing certain two-layer neural networks in the mean-field regime.
- poster: 140232
  presenter: "Zhang, Wei"
  email: "4921dadff47f9cf1"
  title: |-
    MS25: Dynamic Pattern Formation Via Distribution Control of Moment Kernelized
    Population Systems
  abstract: |-
    Targeted coordination of large-scale populations of dynamical systems
    for tuning patterns of their measurement data is an emerging and essential task
    in science and engineering. Its application domains span across numerous disciplines,
    ranging from motion planning of robot swarms in robotics and synchronization of
    rhythmic networks in network science to excitation of nuclear spin ensembles in
    quantum science. However, these dynamic pattern formation tasks are challenged
    by the lack of a principled formulation and the large population size of the systems.
    In this work, we propose a distribution control formulation for dynamic pattern
    formation. Specifically, by leveraging the technique of displacement interpolation
    in optimal transport theory, a dynamic pattern formation task is formulated as
    an optimal control problem over the space of probability distributions. We then
    develop the moment kernel transform, which enables the representation of the evolution
    of the measurement data pattern of a population system in terms of a dynamical
    system, referred to as a moment system, defined on a reproducing kernel Hilbert
    space. To control the moment system by using the data, we propose a deep neural
    network architecture, composed of multiple recurrent neural network layers, which
    are trained by using the transfer learning technique. The applicability and performance
    of the developed dynamic pattern formation framework are demonstrated using examples
    from practical applications.
- poster: 140234
  presenter: "Zhang, Shijie"
  email: "ffbb9069082d57c1"
  title: |-
    MS71: Bayesian Inference of Stochastic Dynamical Systems with Inhomogeneous
    Poisson Noise
  abstract: |-
    Discovering governing laws from noisy data is crucial for understanding
    the underlying dynamics. While data-driven algorithms have emerged to learn ordinary
    and partial differential equations, they have primarily been applied to synthetic
    or physical systems that are largely deterministic with limited noise. Learning
    stochastic differential equations (SDEs) from noisy data remains a challenge.
    Here, we present a framework capable of inferring SDEs with inhomogeneous Poisson
    noise directly from experimental time-series data. Our framework combines basis
    function representation and sparse Bayesian inference to discover SDEs that capture
    the experimentally observed ensemble statistics. By applying our framework to
    biological data of cell growth and division, we discover a nonlinear memory in
    cell division across various species. The approach presented here is also directly
    applicable to infer stochastic dynamical models from real-world healthcare and
    geoscience data.
- poster: 139995
  presenter: "Zhang, Tan"
  email: "e30702c563b2c295"
  title: |-
    A Convergent Interacting Particle Method for Computing Kpp Front Speeds in
    Random Flows
  abstract: |-
    This work aims to efficiently compute the spreading speeds of reaction-diffusion-advection
    fronts in divergence-free random flows under the Kolmogorov-Petrovsky-Piskunov
    nonlinearity. We develop a stochastic interacting particle method (IPM) for the
    reduced principal eigenvalue (Lyapunov exponent) problem of an associated linear
    advection-diffusion operator with spatially random coefficients. The Fourier representation
    of the random advection field and the Feynman-Kac formula of the principal eigenvalue
    (Lyapunov exponent) form the foundation of our method, which is implemented as
    a genetic evolution algorithm. The particles undergo advection-diffusion and mutation/selection
    through a fitness function originated in the Feynman-Kac semigroup. We analyze
    the convergence of the algorithm based on operator splitting and present numerical
    results on representative flows such as 2D cellular flow and 3D Arnold-Beltrami-Childress
    (ABC) flow under random perturbations. The 2D examples serve as a consistency
    check with semi-Lagrangian computation. The 3D results demonstrate that IPM, being
    mesh-free and self-adaptive,  is easy to implement and efficient for computing
    front spreading speeds in the advection-dominated regime for high-dimensional
    random flows on unbounded domains where no truncation is needed.
- poster: 140091
  presenter: "Zhang, Maoyu"
  email: "bc4e557f702e0cc0"
  title: "MS2: Preferential Latent Space Models for Networks with Textual Edges"
  abstract: |-
    Many real world networks contain rich textual information in the edges,
    such as email networks where an edge (or interaction) between two nodes (or actors)
    is an email exchange. Other examples include the co-author network and social
    media networks. The useful textual content is often discarded in most network
    analyses, resulting in an incomplete view of the interactions between nodes. In
    this work, we represent the text document between each pair of nodes as a vector
    that counts the appearances of a set of keywords extracted from the corpus, and
    propose a new and flexible preferential latent space network model that can offer
    direct insights on how contents of the textual exchanges modulate the relationships
    between nodes. We establish identifiability conditions for the proposed model
    and tackle model estimation using a highly efficient projected gradient descent
    algorithm. We further derive the non-asymptotic error bound for the estimator
    from each step of the algorithm. The efficacy of our proposed method is demonstrated
    through simulations and an analysis of the Enron email dataset.
- poster: 140541
  presenter: "Zhang, Sichong"
  email: "8d547b0e9dacdeb9"
  title: "MS55: Minimax Rates for Learning Kernels in Ill-Posed Inverse Problems"
  abstract: |-
    Kernels are efficient in representing nonlocal dependencies and are widely
    used to design operators between function spaces. Thus, learning kernels in operators
    from data is of broad interest, presenting a new topic at the intersection of
    statistical learning and inverse problems. A fundamental question in this context
    is determining the minimax rate. However, two major challenges distinguish this
    from classical nonparametric regression and functional data analysis: (i) the
    absence of universal spaces, such as Sobolev spaces, for learning, and (ii) the
    ill-posedness of the inverse problem.  We address these challenges by first introducing
    adaptive weighted Sobolev spaces based on the normal operator in the large sample
    limit. Then, we establish the optimal minimax rate for the mean square error through
    the study of a tamed least squares estimator. A core technical result is a nonasymptotic
    estimate for the left tail probability of the smallest eigenvalue of the random
    normal matrix.
- poster: 140291
  presenter: "Zhang, Li"
  email: "55fdd8b131f4bda4"
  title: |-
    MS91: Topological Analysis Reveals Organizational Routines Recurrence in
    Outpatient Medical Clinics During Covid-19
  abstract: "Organizational routines in outpatient medical clinics are repetitive, recognizable patterns of action that change over time.  To visualize routine dynamics, we use a novel topological data analysis tool called the Temporal Mapper. We use time-stamped digital trace data to model routines as directed graphs that describe the state of a clinic on a particular day. Over the course of the COVID-19 pandemic, clinical routines varied dramatically. We use Temporal Mapper to identify and visualize stable states and recurrence of organizational routines during different phases of the COVID-19 pandemic, from January 2020 to December 2021. We found that some clinics “bounced back” to their pre-pandemic routines, while other clinics did not. These results demonstrate that topological data analysis (TDA) has the potential to enrich our understanding of routine dynamics and other recurrent processes of social organization."
- poster: 139013
  presenter: "Zhao, Jingtong"
  email: "e837be0704f4f4bb"
  title: |-
    MS4: Neural Ordinary Differential Equation Enables Radiogenomic Explainable
    Ai for Identifying Post-Radiosurgery Brain Metastasis Radionecrosis
  abstract: "Purpose:    Stereotactic radiosurgery(SRS) effectively treats brain metastases(BM) but poses a risk of radionecrosis(RN). A key challenge in managing BM patients post-SRS is the absence of non-invasive diagnostic methods to distinguish RN from true recurrence(TR). We aim to employ a novel neural ordinary differential equation(NODE) model to differentiate post-SRS RN from TR in BM patients.  Methods:   We designed a model based on heavy ball NODE(HBNODE), enabling tracking of DNN behavior by solving the HBNODE and observing the stepwise derivative evolution. The trajectory of each sample within the Image-Genomic-Clinical(I-G-C) space then becomes traceable. A decision-making field was reconstructed, and a non-parametric model aggregated the optimal solutions to predict TR/RN outcomes. Post-SRS MR image, genomic, clinical features from 90 BMs were used. Performance were compared against 1)a DNN using MR images, and 2)a combined ‘I+G+C’ features without the HBNODE model.  Results:   The HBNODE model achieved superior performance of ROCAUC=0.88±0.04, sensitivity=0.79±0.02, specificity=0.86±0.01, and accuracy=0.84±0.01, outperforming the image-only DNN(AUC=0.71±0.05, sensitivity=0.66±0.32) and the 'I+G+C' without HBNODE(AUC=0.81±0.02, sensitivity=0.58±0.11).  Conclusion:   The HBNODE model distinctly identifies RN from TR, aiding explainability in XAI frameworks. Its promising performance encourages further clinical exploration and suggests potential applicability in XAI domains."
- poster: 139647
  presenter: "Zhao, Ruoxi"
  email: "522aa9c5fe53c014"
  title: "MS44: Ai Foundation Models for Surgical Tool Localization"
  abstract: "Recent advancements in foundational language and vision models have achieved state-of-the-art accuracy. Yet, there are still gaps in these model’s ability to understand and respond to tasks in specific areas, particularly scientific and biomedical domains.   This study investigates the performance of these models applied to minimally invasive robotic surgery, by validating their ability to localize and identify surgical tools in endoscopic videos. Integrating the language and vision models presents an opportunity to prompt these models in a natural, intuitive way even for those without a computing or data science background. For instance, one can segment all surgical tools within a robotic surgery scene with simple text prompts like “metal” or “tool”. While we experimented with AI-generated descriptions from the CLIP-interrogator, and using those as text prompts for SAM, the overlapping keywords presented challenges due to the models' limited surgical domain expertise.   Understanding the visual content in surgical scenes, having the trained vocabulary to prompt and infer from these models is vital to adoption of more complex techniques like semantic action recognition, or long term activity prediction. Addressing this, we will further explore fine-tuning models on robotic surgery datasets to increase contextual awareness in the surgical domain, such as segmenting particular surgical instruments or organs based on name-based prompts."
- poster: 139763
  presenter: "Zhao, Shifan"
  email: "a5b64cd69c2d592a"
  title: |-
    MS28: Efficient Two-stage Gaussian Process Regression Via Subsampling and
    Automatic Kernel Searching
  abstract: "Gaussian Process Regression (GPR) is extensively utilized in statistics and machine learning to quantify predictive uncertainty. The efficacy of GPR critically depends on the precise specification of the mean, covariance functions, and associated hyperparameters. We firstly explore the impact of mean function misspecification and introduce a two-stage GPR model to mitigate this issue. Then we propose a novel algorithm, underpinned by a rigorous theoretical bound, to identify the most suitable kernel from a set of candidates. With well specified mean and kernel function, another potential misspecificationthe is from inexact training of GPR. Due to the cubic time complexity, hyperparameters of GPR are usually obtained via training GPR on a randomly sampled dataset. We theoretically and empirically demonstrate that parameters estimated from a subsampled dataset serve as effective initialization for exact GPR training, assuming correct model specification.  Finally, we recommend two GPR approaches—exact and scalable based on our framework—tailored to align with available computational resources and specific needs for uncertainty quantification. Our comprehensive evaluation across real-world datasets, including UCI benchmarks, along with a case study in a safety-critical medical application, highlights the robustness and accuracy of our methodologies."
- poster: 140080
  presenter: "Zheng, Haoyang"
  email: "59b96b0829261820"
  title: |-
    MS61: Accelerating Approximate Thompson Sampling with Underdamped Langevin
    Monte Carlo
  abstract: |-
    Approximate Thompson sampling with Langevin Monte Carlo broadens its reach
    from Gaussian posterior sampling to encompass more general smooth posteriors.
    However, it still encounters scalability issues in high-dimensional problems when
    demanding high accuracy. To address this, we propose an approximate Thompson sampling
    strategy, utilizing underdamped Langevin Monte Carlo, where the latter is the
    go-to workhorse for simulations of high-dimensional posteriors. Based on the standard
    smoothness and log-concavity conditions, we study the accelerated posterior concentration
    and sampling using a specific potential function. This design improves the sample
    complexity for realizing logarithmic regrets from $\mathcal{\tilde O}(d)$ to $\mathcal{\tilde
    O}(\sqrt{d})$. The scalability and robustness of our algorithm are also empirically
    validated through synthetic experiments in high-dimensional bandit problems.
- poster: 139593
  presenter: "Zhong, Suhan"
  email: "ba1e2d421a5bebc6"
  title: "Polynomial Lower Approximation for Two-Stage Stochastic Optimization"
  abstract: |-
    We introduce a two-phase approach to find global optimal solutions of
    two-stage stochastic programs with continuous decision variables and nonconvex
    recourse functions. The first phase involves the construction of a polynomial
    lower bound for the recourse function through a linear optimization problem over
    a nonnegative polynomial cone. Given the complex structure of this cone, we employ
    semidefinite relaxations with quadratic modules to facilitate our computations.
    In the second phase, we solve a surrogate first-stage problem by substituting
    the original recourse function with the polynomial lower approximation obtained
    in the first phase. Our method is particularly advantageous for two reasons: it
    not only generates global lower bounds for the nonconvex stochastic program, aiding
    in the verification of global optimality for prospective solutions like stationary
    solutions computed from other methods, but it also simplifies the computation
    of the expected value of the recourse function by using moments of random vectors.
    This makes our overall algorithm particularly suitable for the case where the
    random vector follows a continuous distribution or when dealing with a large number
    of scenarios. Numerical experiments are given to demonstrate the effectiveness
    of our proposed approach.
- poster: 139882
  presenter: "Zhou, Zihe"
  email: "1f76d99a8e22c484"
  title: "Sequential Quadratic Programming for Optimal Transport"
  abstract: |-
    The Monge optimal transport (OT) problem seeks to optimize transportation
    cost from a source probability measure to a target probability measure. The optimization
    is over a space of transport maps and the transportation cost is defined by a
    cost functional of the maps. As recent Interest arises in OT, many works focus
    on tackling the OT problem computationally using finite dimensional approximations.
    In this work, we present the infinite-dimensional formulation of the OT problem
    over a Banach space. We provide explicit expressions of the first and second-order
    variation of the objective functional, and of the function-form constraint with
    respect to the transport map. We propose a Sequential Quadratic Programming (SQP)
    framework for this infinite-dimensional problem. We show that subject to reasonable
    regularity assumptions, our framework satisfies known conditions for local convergence.
    Moreover, we demonstrate that a merit functional with sufficient constraint penalization
    serves as an effective step-size monitor within SQP iterations, leading to global
    convergence towards critical points. To the best of our knowledge, this is the
    first attempt at a globally convergent SQP operator recursion over infinite-dimensional
    spaces.
- poster: 139362
  presenter: "Zhou, Mo"
  email: "a171bbb059892d8a"
  title: |-
    MS56: Solving Time-Continuous Stochastic Optimal Control Problems: Algorithm
    Design and Convergence Analysis of Actor-Critic Flow
  abstract: |-
    We propose an actor-critic framework to solve the time-continuous stochastic
    optimal control problem. A least square temporal difference method is applied
    to compute the value function for the critic. The policy gradient method is implemented
    as policy improvement for the actor. Our key contribution lies in establishing
    the global convergence property of our proposed actor-critic flow, demonstrating
    a linear rate of convergence. Theoretical findings are further validated through
    numerical examples, showing the efficacy of our approach in practical applications.
- poster: 139927
  presenter: "Zhu, Zhihui"
  email: "ea98b812b30561e0"
  title: "MS89: Similarity of Layer-Wise Representations Within Transformers"
  abstract: |-
    Analyzing the similarity of internal representations within and across
    different models has been an important technique for understanding the behavior
    of deep neural networks. Most existing methods for analyzing the similarity between
    representations of high dimensions, such as the widely used centered kernel alignment
    (CKA) and those based on canonical correlation analysis (CCA), rely on statistical
    properties of the representations for a set of data points. In this paper, we
    focus on transformer models and study the similarity of representations between
    the hidden layers of individual transformers. In this context, we show that a
    simple sample-wise similarity metric is sufficient and aligns with complicated
    ones such as the CKA. Our experimental results on common transformers for both
    vision and NLP tasks also reveal that representations across layers are positively
    correlated, albeit the similarity decreases when layers are far apart. We then
    propose a simple training method to enhance the similarity between internal representations,
    with trained models that enjoy the following properties: (1) the last-layer classifier
    can be directly applied right after any hidden layers, yielding layer-wise accuracies
    much higher than those under standard training, (2) the layer-wise accuracies
    monotonically increase and reveal the minimal depth needed for the given task.
- poster: 139777
  presenter: "Zhu, Yuchen"
  email: "5eecdd2c135d68ea"
  title: |-
    MS72: A Mean-Field Analysis of Neural Gradient Descent-Ascent: Applications
    to Functional Conditional Moment Equations
  abstract: |-
    We study minimax optimization problems defined over infinite-dimensional
    function classes. We restrict the functions to the class of overparameterized
    two-layer neural networks and study (i) the convergence of the gradient descent-ascent
    algorithm and (ii) the representation learning of the neural network. We consider
    the minimax optimization problem stemming from estimating a functional equation
    defined by conditional expectations via adversarial estimation, where the objective
    function is quadratic in the functional space. We establish convergence under
    the mean-field regime by considering the continuous-time and infinite-width limit
    of the optimization dynamics. Under this regime, gradient descent-ascent corresponds
    to a Wasserstein gradient flow over the space of probability measures defined
    over the space of neural network parameters. We prove that the Wasserstein gradient
    flow converges globally to a stationary point of the minimax objective at a sublinear
    rate, and additionally finds the solution to the functional equation when the
    regularizer of the minimax objective is strongly convex. In terms of representation
    learning, our results show that the feature representation induced by the neural
    networks is allowed to deviate from the initial one by the magnitude of $O(\alpha^{-1})$,
    measured in terms of the Wasserstein distance. We apply our results to concrete
    examples including policy evaluation, nonparametric instrumental variable regression,
    and asset pricing.
- poster: 139361
  presenter: "Zhu, Yuhua"
  email: "98ae52e299b33846"
  title: |-
    MS56: Phibe: Physics-Informed Bellman Equation for Continuous-Time Reinforcement
    Learning
  abstract: |-
    In this poster, we address the problem of continuous-time reinforcement
    learning in scenarios where the dynamics follow a stochastic differential equation.
    When the underlying dynamics remain unknown, and we have access only to discrete-time
    information, how can we effectively find the optimal policy? We first highlight
    that the commonly used Bellman equation is not always a reliable approximation
    to the true value function. We then introduce PhiBE, a PDE-based Bellman equation
    that offers a more accurate approximation to the true value function, especially
    in scenarios where the underlying dynamics change slowly. Moreover, we extend
    PhiBE to higher orders, providing increasingly accurate approximations. Additionally,
    we present a model-free algorithm to solve PhiBE when only discrete-time trajectory
    data is available. Numerical experiments are provided to validate the theoretical
    guarantees we propose.
